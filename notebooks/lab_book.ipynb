{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "The objectives of this week are to develop and test benchmarks. This will give us a baseline to compare our methods to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Find example pdf / text data.\n",
    "- Setup LangChain Recursive Text Splitter.\n",
    "- Setup fixed length token splitter.\n",
    "- Setup Chroma DB.\n",
    "- Create pipeline of: Text + Chunker -> Chroma Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Text Data\n",
    "To start off simple, I copied a recent news article from BBC about Effective Accelerationist, Grimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coachella: Grimes apologises for technical difficulties\n",
      "\n",
      "Mon 15 Apr\n",
      "\n",
      "BBC NEWS\n",
      "\n",
      "Grimes has apologised for \"major technical difficulties\" during her Coachella DJ set.\n",
      "\n",
      "Fans watched the singer scream in ...\n"
     ]
    }
   ],
   "source": [
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Test the function with the news.txt file\n",
    "news_data = read_txt_file('../data/news.txt')\n",
    "print(news_data[:200]+'...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain Recursive Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the Recursive Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "# Split the news_data using the splitter\n",
    "split_text = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain fixed length splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "\n",
    "texts = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"chuck_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Precision:\n",
    "In the ARAGOG paper they used Tonic Validate for this. If have taken Tonic's prompt so our implementation is identical (except we have the power to use models beyond GPT-3.5).\n",
    "Ref: https://github.com/TonicAI/tonic_validate/blob/main/tonic_validate/utils/llm_calls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_precision_prompt(question, context):\n",
    "    main_message = (\"Considering the following question and context, determine whether the context \"\n",
    "                    \"is relevant for answering the question. If the context is relevant for \"\n",
    "                    \"answering the question, respond with true. If the context is not relevant for \"\n",
    "                    \"answering the question, respond with false. Respond with either true or false \"\n",
    "                    \"and no additional text.\")\n",
    "\n",
    "    main_message += f\"\\nQUESTION: {question}\\n\"\n",
    "    main_message += f\"CONTEXT: {context}\\n\"\n",
    "\n",
    "    return main_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\n",
      "QUESTION: What is the capital of France?\n",
      "CONTEXT: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the function get_retrieval_precision_prompt\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\"\n",
    "\n",
    "print(get_retrieval_precision_prompt(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the realm of code, a method elegant and divine,\\nRecursion weaves a dance in loops, a concept quite refined.\\nAn echo of itself, the function calls its own embrace,\\nEach iteration a reflection, a cycle of poetic grace.\\n\\nLike a mirror reflecting endless images, recursive paths unfold,\\nEach repetition delving deeper, a story yet untold.\\nA self-referential loop, it spirals through the code's domain,\\nEmbracing complexity with a rhythm, a mesmerizing refrain.\\n\\nThrough recursive magic, problems are tackled with finesse,\\nBreaking them into smaller parts, a puzzle to address.\\nIn the dance of function calls, a symphony of logic rings,\\nSolving complex algorithms with recursive wings.\\n\\nSo heed the call of recursion, let it guide your coding quest,\\nIn its looping, self-referential beauty, let your code be blessed.\\nFor in the world of programming, where complexity may reign,\\nRecursion offers elegance, an eternal looping chain.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='*clears throat and speaks in a croaky voice* Hmm, well I am today, young Padawan. The Force, strong in me it flows. Yes, hmmm.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_CHROMA_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='true', type='text')]\n"
     ]
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=get_retrieval_precision_prompt(question, context),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the json file and read it\n",
    "with open('../eval_questions/eval_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data to verify it's been read correctly\n",
    "print(len(data['questions']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts:\n",
    "\n",
    "Currently using:\n",
    "ARAGOGs - Dataset (small, they added all other papers, arXiv papers)\n",
    "ARAGOGs - Questions (107 questions)\n",
    "Tonic Validate - Prompt for Retrieval Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Text to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../papers_for_questions/bert.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the Recursive Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1024,\n",
    "    # chunk_overlap=256,\n",
    ")\n",
    "\n",
    "# Split the news_data using the splitter\n",
    "split_text = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_text[:5]\n",
    "\n",
    "# Count the number of tokens in each page_content\n",
    "import tiktoken\n",
    "\n",
    "# Count the number of tokens in each page_content\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# for page in split_text:\n",
    "#     print(num_tokens_from_string(page.page_content))\n",
    "    # print(page.page_content[:200]+'...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../papers_for_questions/bert.pdf', 'page': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).These approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "# collection = chroma_client.get_or_create_collection(name=\"chuck_1\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [chunk.page_content for chunk in split_text]\n",
    "metadatas = [chunk.metadata for chunk in split_text]\n",
    "ids = [str(i) for i in range(len(split_text))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3', '20', '0', '6', '4']],\n",
       " 'distances': [[0.7218702708002468,\n",
       "   0.7597797400597308,\n",
       "   0.7711573382687963,\n",
       "   0.7771485522310896,\n",
       "   0.8290271472604436]],\n",
       " 'metadatas': [[{'page': 2, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 13, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 0, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 4, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 3, 'source': '../papers_for_questions/bert.pdf'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['BERT BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nQuestion Paragraph Start/End Span \\nBERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nMasked Sentence A Masked Sentence B \\nPre-training Fine-Tuning NSP Mask LM Mask LM \\nUnlabeled Sentence A and B Pair SQuAD \\nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning . Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\\n4We note that in the literature the bidirectional Trans-',\n",
       "   '•Learning rate (Adam) : 5e-5, 3e-5, 2e-5\\n•Number of epochs : 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are ﬁne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ( [SEP] ) and\\nclassiﬁer token ( [CLS] ) which are only in-\\ntroduced at ﬁne-tuning time; BERT learns\\n[SEP] ,[CLS] and sentence A/Bembed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall ﬁne-tuning experiments; BERT chooses a\\ntask-speciﬁc ﬁne-tuning learning rate which\\nperforms the best on the development set.To isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of ﬁne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciﬁc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe ﬁgure,Erepresents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised .\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiﬁ-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment ,contradiction , or\\nneutral with respect to the ﬁrst one.\\nQQP Quora Question Pairs is a binary classiﬁ-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiﬁcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct',\n",
       "   'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019',\n",
       "   '[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input \\nE[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token \\nEmbeddings \\nEA EB EB EB EB EB EA EA EA EA EASegment \\nEmbeddings \\nE0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position \\nEmbeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciﬁc inputs and outputs into BERT and ﬁne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence Aand sentence Bfrom pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and(4) a degenerate text- ∅pair in text classiﬁcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, ﬁne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7We de-\\nscribe the task-speciﬁc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT ﬁne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo ﬁne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the ﬁnal hid-\\nden vectorC∈RHcorresponding to the ﬁrst\\ninput token ( [CLS] ) as the aggregate representa-\\ntion. The only new parameters introduced during\\nﬁne-tuning are classiﬁcation layer weights W∈\\nRK×H, whereKis the number of labels. We com-\\npute a standard classiﬁcation loss with CandW,\\ni.e.,log(softmax( CWT)).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq .',\n",
       "   'Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g.,⟨Question, Answer⟩) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ( [SEP] ). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence Aor sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token asC∈RH,\\nand the ﬁnal hidden vector for the ithinput token\\nasTi∈RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right orright-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nﬁne-tuning, since the [MASK] token does not ap-\\npear during ﬁne-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac-\\ntual[MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthei-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTiwill be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts=[\"What are the two main tasks BERT is pre-trained on?\"], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Issue:\n",
    "\n",
    "Retrieval Precision expects a COMPLETE RAG system and wants to measure the TOTAL number of returned contexts divided by relavent context. \n",
    "\n",
    "The issue with this is we'd ideally just retrieve N contexts always. The metric should not punish if the model returns all relavent context but there isn't much relavent context. Nor should it be rewarded if it only returns one relavent context but there's lots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Current tokens: 19559\n",
      "1 Current tokens: 12387\n",
      "2 Current tokens: 15654\n",
      "3 Current tokens: 21248\n",
      "4 Current tokens: 30842\n",
      "5 Current tokens: 17573\n",
      "6 Current tokens: 9560\n",
      "7 Current tokens: 24615\n",
      "8 Current tokens: 25285\n",
      "9 Current tokens: 15259\n",
      "10 Current tokens: 9842\n",
      "11 Current tokens: 58265\n",
      "12 Current tokens: 12804\n",
      "13 Current tokens: 58632\n",
      "14 Current tokens: 10976\n",
      "15 Current tokens: 14916\n",
      "16 Current tokens: 18802\n",
      "17 Current tokens: 17639\n",
      "18 Current tokens: 10244\n",
      "19 Current tokens: 11171\n",
      "20 Current tokens: 35282\n",
      "21 Current tokens: 30161\n",
      "22 Current tokens: 10952\n",
      "23 Current tokens: 173899\n",
      "24 Current tokens: 22685\n",
      "25 Current tokens: 46057\n",
      "26 Current tokens: 24993\n",
      "27 Current tokens: 25551\n",
      "28 Current tokens: 19021\n",
      "29 Current tokens: 24021\n",
      "30 Current tokens: 20091\n",
      "31 Current tokens: 17079\n",
      "32 Current tokens: 29179\n",
      "33 Current tokens: 21489\n",
      "34 Current tokens: 55100\n",
      "35 Current tokens: 9865\n",
      "36 Current tokens: 6815\n",
      "37 Current tokens: 5797\n",
      "38 Current tokens: 12755\n",
      "39 Current tokens: 10308\n",
      "40 Current tokens: 25661\n",
      "41 Current tokens: 10264\n",
      "42 Current tokens: 17921\n",
      "43 Current tokens: 97020\n",
      "44 Current tokens: 33483\n",
      "45 Current tokens: 12554\n",
      "46 Current tokens: 44544\n",
      "47 Current tokens: 11206\n",
      "48 Current tokens: 12914\n",
      "49 Current tokens: 17923\n",
      "50 Current tokens: 15530\n",
      "51 Current tokens: 23606\n",
      "52 Current tokens: 20202\n",
      "53 Current tokens: 21771\n",
      "54 Current tokens: 11626\n",
      "55 Current tokens: 205882\n",
      "56 Current tokens: 11273\n",
      "57 Current tokens: 25207\n",
      "58 Current tokens: 0\n",
      "59 Current tokens: 30975\n",
      "60 Current tokens: 11902\n",
      "61 Current tokens: 82374\n",
      "62 Current tokens: 40863\n",
      "63 Current tokens: 20994\n",
      "64 Current tokens: 45672\n",
      "65 Current tokens: 8763\n",
      "66 Current tokens: 30854\n",
      "67 Current tokens: 49161\n",
      "68 Current tokens: 37766\n",
      "69 Current tokens: 36633\n",
      "70 Current tokens: 35388\n",
      "71 Current tokens: 15215\n",
      "72 Current tokens: 14645\n",
      "73 Current tokens: 14714\n",
      "74 Current tokens: 31848\n",
      "75 Current tokens: 21528\n",
      "76 Current tokens: 36029\n",
      "77 Current tokens: 12767\n",
      "78 Current tokens: 18595\n",
      "79 Current tokens: 9874\n",
      "80 Current tokens: 16008\n",
      "81 Current tokens: 20360\n",
      "82 Current tokens: 39401\n",
      "83 Current tokens: 13187\n",
      "84 Current tokens: 18761\n",
      "85 Current tokens: 22880\n",
      "86 Current tokens: 16055\n",
      "87 Current tokens: 36818\n",
      "88 Current tokens: 7580\n",
      "89 Current tokens: 13265\n",
      "90 Current tokens: 41673\n",
      "91 Current tokens: 14288\n",
      "92 Current tokens: 50795\n",
      "93 Current tokens: 12738\n",
      "94 Current tokens: 20988\n",
      "95 Current tokens: 23167\n",
      "96 Current tokens: 36312\n",
      "97 Current tokens: 27498\n",
      "98 Current tokens: 16408\n",
      "99 Current tokens: 9686\n",
      "100 Current tokens: 36501\n",
      "101 Current tokens: 22854\n",
      "102 Current tokens: 15346\n",
      "103 Current tokens: 5975\n",
      "104 Current tokens: 19854\n",
      "105 Current tokens: 14591\n",
      "106 Current tokens: 21092\n",
      "107 Current tokens: 28209\n",
      "108 Current tokens: 16562\n",
      "109 Current tokens: 17947\n",
      "110 Current tokens: 11385\n",
      "111 Current tokens: 32191\n",
      "112 Current tokens: 2536\n",
      "113 Current tokens: 5759\n",
      "114 Current tokens: 36676\n",
      "115 Current tokens: 11865\n",
      "116 Current tokens: 30388\n",
      "117 Current tokens: 18665\n",
      "118 Current tokens: 20551\n",
      "119 Current tokens: 27988\n",
      "120 Current tokens: 24393\n",
      "121 Current tokens: 11049\n",
      "122 Current tokens: 11140\n",
      "123 Current tokens: 7205\n",
      "124 Current tokens: 40763\n",
      "125 Current tokens: 118945\n",
      "126 Current tokens: 16519\n",
      "127 Current tokens: 24115\n",
      "128 Current tokens: 18167\n",
      "129 Current tokens: 37570\n",
      "130 Current tokens: 4221\n",
      "131 Current tokens: 8861\n",
      "132 Current tokens: 11823\n",
      "133 Current tokens: 30163\n",
      "134 Current tokens: 64225\n",
      "135 Current tokens: 24013\n",
      "136 Current tokens: 14032\n",
      "137 Current tokens: 24899\n",
      "138 Current tokens: 0\n",
      "139 Current tokens: 8468\n",
      "140 Current tokens: 11306\n",
      "141 Current tokens: 11361\n",
      "142 Current tokens: 31339\n",
      "143 Current tokens: 31394\n",
      "144 Current tokens: 7394\n",
      "145 Current tokens: 5711\n",
      "146 Current tokens: 26640\n",
      "147 Current tokens: 9185\n",
      "148 Current tokens: 18042\n",
      "149 Current tokens: 11959\n",
      "150 Current tokens: 9315\n",
      "151 Current tokens: 46255\n",
      "152 Current tokens: 31166\n",
      "153 Current tokens: 13383\n",
      "154 Current tokens: 10905\n",
      "155 Current tokens: 25827\n",
      "156 Current tokens: 20461\n",
      "157 Current tokens: 9653\n",
      "158 Current tokens: 12013\n",
      "159 Current tokens: 19471\n",
      "160 Current tokens: 24288\n",
      "161 Current tokens: 15731\n",
      "162 Current tokens: 28463\n",
      "163 Current tokens: 19446\n",
      "164 Current tokens: 5204\n",
      "165 Current tokens: 26922\n",
      "166 Current tokens: 16155\n",
      "167 Current tokens: 21507\n",
      "168 Current tokens: 14651\n",
      "169 Current tokens: 52181\n",
      "170 Current tokens: 4997\n",
      "171 Current tokens: 26709\n",
      "172 Current tokens: 18754\n",
      "173 Current tokens: 10332\n",
      "174 Current tokens: 18887\n",
      "175 Current tokens: 17111\n",
      "176 Current tokens: 13681\n",
      "177 Current tokens: 9578\n",
      "178 Current tokens: 9761\n",
      "179 Current tokens: 14957\n",
      "180 Current tokens: 17241\n",
      "181 Current tokens: 17886\n",
      "182 Current tokens: 10056\n",
      "183 Current tokens: 30103\n",
      "184 Current tokens: 16117\n",
      "185 Current tokens: 37346\n",
      "186 Current tokens: 14333\n",
      "187 Current tokens: 9580\n",
      "188 Current tokens: 15702\n",
      "189 Current tokens: 53020\n",
      "190 Current tokens: 10845\n",
      "191 Current tokens: 14781\n",
      "192 Current tokens: 43349\n",
      "193 Current tokens: 16942\n",
      "194 Current tokens: 20406\n",
      "195 Current tokens: 25532\n",
      "196 Current tokens: 10531\n",
      "197 Current tokens: 12886\n",
      "198 Current tokens: 15042\n",
      "199 Current tokens: 13199\n",
      "200 Current tokens: 21720\n",
      "201 Current tokens: 22600\n",
      "202 Current tokens: 22495\n",
      "203 Current tokens: 27107\n",
      "204 Current tokens: 11625\n",
      "205 Current tokens: 9905\n",
      "206 Current tokens: 25987\n",
      "207 Current tokens: 10627\n",
      "208 Current tokens: 14306\n",
      "209 Current tokens: 0\n",
      "210 Current tokens: 11025\n",
      "211 Current tokens: 4768\n",
      "212 Current tokens: 31787\n",
      "213 Current tokens: 15936\n",
      "214 Current tokens: 15751\n",
      "215 Current tokens: 7373\n",
      "216 Current tokens: 0\n",
      "217 Current tokens: 10422\n",
      "218 Current tokens: 17622\n",
      "219 Current tokens: 17788\n",
      "220 Current tokens: 27068\n",
      "221 Current tokens: 26407\n",
      "222 Current tokens: 35508\n",
      "223 Current tokens: 36666\n",
      "224 Current tokens: 61251\n",
      "225 Current tokens: 97258\n",
      "226 Current tokens: 21838\n",
      "227 Current tokens: 23623\n",
      "228 Current tokens: 29806\n",
      "229 Current tokens: 7292\n",
      "230 Current tokens: 31581\n",
      "231 Current tokens: 43434\n",
      "232 Current tokens: 33885\n",
      "233 Current tokens: 16090\n",
      "234 Current tokens: 59016\n",
      "235 Current tokens: 22574\n",
      "236 Current tokens: 15510\n",
      "237 Current tokens: 36973\n",
      "238 Current tokens: 20642\n",
      "239 Current tokens: 25200\n",
      "240 Current tokens: 13372\n",
      "241 Current tokens: 18497\n",
      "242 Current tokens: 19220\n",
      "243 Current tokens: 20306\n",
      "244 Current tokens: 24581\n",
      "245 Current tokens: 21663\n",
      "246 Current tokens: 39737\n",
      "247 Current tokens: 11574\n",
      "248 Current tokens: 20446\n",
      "249 Current tokens: 53154\n",
      "250 Current tokens: 17907\n",
      "251 Current tokens: 12234\n",
      "252 Current tokens: 13865\n",
      "253 Current tokens: 15579\n",
      "254 Current tokens: 8005\n",
      "255 Current tokens: 15754\n",
      "256 Current tokens: 37309\n",
      "257 Current tokens: 34823\n",
      "258 Current tokens: 23963\n",
      "259 Current tokens: 32256\n",
      "260 Current tokens: 8722\n",
      "261 Current tokens: 17860\n",
      "262 Current tokens: 14453\n",
      "263 Current tokens: 6043\n",
      "264 Current tokens: 20254\n",
      "265 Current tokens: 18671\n",
      "266 Current tokens: 24148\n",
      "267 Current tokens: 4205\n",
      "268 Current tokens: 31489\n",
      "269 Current tokens: 17232\n",
      "270 Current tokens: 10693\n",
      "271 Current tokens: 20993\n",
      "272 Current tokens: 13065\n",
      "273 Current tokens: 26663\n",
      "274 Current tokens: 10490\n",
      "275 Current tokens: 11995\n",
      "276 Current tokens: 19222\n",
      "277 Current tokens: 27483\n",
      "278 Current tokens: 58408\n",
      "279 Current tokens: 6329\n",
      "280 Current tokens: 6075\n",
      "281 Current tokens: 14438\n",
      "282 Current tokens: 66267\n",
      "283 Current tokens: 20527\n",
      "284 Current tokens: 25150\n",
      "285 Current tokens: 12586\n",
      "286 Current tokens: 11538\n",
      "287 Current tokens: 20998\n",
      "288 Current tokens: 58467\n",
      "289 Current tokens: 18310\n",
      "290 Current tokens: 12882\n",
      "291 Current tokens: 17905\n",
      "292 Current tokens: 29235\n",
      "293 Current tokens: 26076\n",
      "294 Current tokens: 19740\n",
      "295 Current tokens: 10713\n",
      "296 Current tokens: 5032\n",
      "297 Current tokens: 20513\n",
      "298 Current tokens: 19572\n",
      "299 Current tokens: 17966\n",
      "300 Current tokens: 12873\n",
      "301 Current tokens: 51757\n",
      "302 Current tokens: 12367\n",
      "303 Current tokens: 12631\n",
      "304 Current tokens: 14357\n",
      "305 Current tokens: 6743\n",
      "306 Current tokens: 8030\n",
      "307 Current tokens: 19234\n",
      "308 Current tokens: 8776\n",
      "309 Current tokens: 17927\n",
      "310 Current tokens: 9253\n",
      "311 Current tokens: 6839\n",
      "312 Current tokens: 9623\n",
      "313 Current tokens: 17984\n",
      "314 Current tokens: 11724\n",
      "315 Current tokens: 5397\n",
      "316 Current tokens: 4899\n",
      "317 Current tokens: 15684\n",
      "318 Current tokens: 10339\n",
      "319 Current tokens: 29964\n",
      "320 Current tokens: 27633\n",
      "321 Current tokens: 14855\n",
      "322 Current tokens: 18973\n",
      "323 Current tokens: 7882\n",
      "324 Current tokens: 9783\n",
      "325 Current tokens: 17455\n",
      "326 Current tokens: 9911\n",
      "327 Current tokens: 38612\n",
      "328 Current tokens: 16583\n",
      "329 Current tokens: 41052\n",
      "330 Current tokens: 20366\n",
      "331 Current tokens: 25752\n",
      "332 Current tokens: 11057\n",
      "333 Current tokens: 12749\n",
      "334 Current tokens: 84829\n",
      "335 Current tokens: 10632\n",
      "336 Current tokens: 20173\n",
      "337 Current tokens: 12747\n",
      "338 Current tokens: 26900\n",
      "339 Current tokens: 10282\n",
      "340 Current tokens: 12527\n",
      "341 Current tokens: 6150\n",
      "342 Current tokens: 15593\n",
      "343 Current tokens: 39079\n",
      "344 Current tokens: 11917\n",
      "345 Current tokens: 10180\n",
      "346 Current tokens: 33256\n",
      "347 Current tokens: 16702\n",
      "348 Current tokens: 20322\n",
      "349 Current tokens: 11747\n",
      "350 Current tokens: 17868\n",
      "351 Current tokens: 31020\n",
      "352 Current tokens: 13191\n",
      "353 Current tokens: 11172\n",
      "354 Current tokens: 15469\n",
      "355 Current tokens: 19457\n",
      "356 Current tokens: 18071\n",
      "357 Current tokens: 21320\n",
      "358 Current tokens: 29212\n",
      "359 Current tokens: 10152\n",
      "360 Current tokens: 6513\n",
      "361 Current tokens: 7147\n",
      "362 Current tokens: 26581\n",
      "363 Current tokens: 39023\n",
      "364 Current tokens: 14562\n",
      "365 Current tokens: 29952\n",
      "366 Current tokens: 22094\n",
      "367 Current tokens: 23671\n",
      "368 Current tokens: 716981\n",
      "369 Current tokens: 11140\n",
      "370 Current tokens: 12374\n",
      "371 Current tokens: 10941\n",
      "372 Current tokens: 18893\n",
      "373 Current tokens: 10754\n",
      "374 Current tokens: 7283\n",
      "375 Current tokens: 22884\n",
      "376 Current tokens: 12471\n",
      "377 Current tokens: 23061\n",
      "378 Current tokens: 11224\n",
      "379 Current tokens: 19973\n",
      "380 Current tokens: 6207\n",
      "381 Current tokens: 12924\n",
      "382 Current tokens: 25958\n",
      "383 Current tokens: 13691\n",
      "384 Current tokens: 16300\n",
      "385 Current tokens: 28190\n",
      "386 Current tokens: 41741\n",
      "387 Current tokens: 34003\n",
      "388 Current tokens: 36028\n",
      "389 Current tokens: 13895\n",
      "390 Current tokens: 22789\n",
      "391 Current tokens: 40899\n",
      "392 Current tokens: 10608\n",
      "393 Current tokens: 22649\n",
      "394 Current tokens: 24549\n",
      "395 Current tokens: 10246\n",
      "396 Current tokens: 12704\n",
      "397 Current tokens: 6330\n",
      "398 Current tokens: 59817\n",
      "399 Current tokens: 26951\n",
      "400 Current tokens: 0\n",
      "401 Current tokens: 24343\n",
      "402 Current tokens: 7008\n",
      "403 Current tokens: 15232\n",
      "404 Current tokens: 49879\n",
      "405 Current tokens: 6912\n",
      "406 Current tokens: 10195\n",
      "407 Current tokens: 14479\n",
      "408 Current tokens: 9615\n",
      "409 Current tokens: 23162\n",
      "410 Current tokens: 20262\n",
      "411 Current tokens: 10194\n",
      "412 Current tokens: 11911\n",
      "413 Current tokens: 62458\n",
      "414 Current tokens: 10378\n",
      "415 Current tokens: 13747\n",
      "416 Current tokens: 37260\n",
      "417 Current tokens: 63868\n",
      "418 Current tokens: 41922\n",
      "419 Current tokens: 8234\n",
      "420 Current tokens: 78414\n",
      "421 Current tokens: 58980\n",
      "422 Current tokens: 12528\n",
      "[58, 138, 209, 216, 400]\n",
      "Total tokens: 10155019\n",
      "Price: $0.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the file\n",
    "with open('../data/train.jsonl', 'r') as file:\n",
    "    # Iterate over each line\n",
    "    # i = 0\n",
    "    total_tokens = 0\n",
    "    fails = []\n",
    "    for index, line in enumerate(file):\n",
    "        # Load the JSON data from each line\n",
    "        data = json.loads(line)\n",
    "        # Now you can use the data as a normal Python dictionary\n",
    "        # print(data)\n",
    "        # dict_keys(['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'content', 'references'])\n",
    "        # print(data.keys())\n",
    "        try:\n",
    "            current_tokens = num_tokens_from_string(data['content'])\n",
    "        except:\n",
    "            fails.append(index)\n",
    "            current_tokens = 0\n",
    "\n",
    "        # print(data['content'][-500:])\n",
    "        \n",
    "        total_tokens += current_tokens\n",
    "        print(f\"{index} Current tokens: {current_tokens}\")\n",
    "\n",
    "        # i += 1\n",
    "        # if i > 5:\n",
    "        #     break\n",
    "    print(fails)\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Price: ${round((total_tokens/1000000) * 0.02, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose Collection.\n",
    "2. Iterate through pdfs. Keep track in case of error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_algorithm():\n",
    "    pass\n",
    "\n",
    "\n",
    "# This class is designed to rapidly create new collections for various chunking methods.\n",
    "# It's done via a class so that it holds state in the case of an error mid-way through the process.\n",
    "class CollectionWriter:\n",
    "    def __init__(self, path=\"../data/chroma_db\"):\n",
    "        self.chroma_client = chromadb.PersistentClient(path=path)\n",
    "        self.openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "        self.collection = None\n",
    "        self.index = 0\n",
    "\n",
    "    def trial_run(self, collection_name, chunking_function):\n",
    "        self.write_to_collection(\"trial_run\", chunking_function, 0, trial_run=True)\n",
    "\n",
    "    def write_to_collection(self, collection_name, chunking_function, start_index=0, trial_run=False):\n",
    "        if not trial_run:\n",
    "            self.collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=self.openai_ef)\n",
    "        \n",
    "        total_tokens = 0\n",
    "\n",
    "        with open('../data/train.jsonl', 'r') as file:\n",
    "            for index, line in enumerate(file):\n",
    "                if index < start_index:\n",
    "                    continue\n",
    "                if index == 368:\n",
    "                    continue\n",
    "                self.index = index\n",
    "                data = json.loads(line)\n",
    "                try:\n",
    "                    documents = chunking_function(data['content'])\n",
    "                except:\n",
    "                    documents = []\n",
    "                metadatas = [{\"id\": data['id'], \"title\": data['title']} for _ in range(len(documents))]\n",
    "                ids = [data['id']+\":\"+str(i) for i in range(len(documents))]\n",
    "                if not trial_run:\n",
    "                    try:\n",
    "                        self.collection.add(\n",
    "                            documents=documents,\n",
    "                            metadatas=metadatas,\n",
    "                            ids=ids\n",
    "                        )\n",
    "                    except:\n",
    "                        print(f\"Failed at index {index}\")\n",
    "                # print(ids[:5])\n",
    "                # print(metadatas[:5])\n",
    "                # print(documents[0])\n",
    "                try:\n",
    "                    num_tokens = sum([num_tokens_from_string(doc) for doc in documents])\n",
    "                except:\n",
    "                    num_tokens = 0\n",
    "                print(f\"{index} Added {len(documents)} documents. Current tokens: {num_tokens}\")\n",
    "                # print(f\"{index} Added {len(documents)} documents.\")\n",
    "                total_tokens += num_tokens\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Price: ${round((total_tokens/1000000) * 0.13, 2)}\")\n",
    "        # print(f\"Price: ${round((total_tokens/1000000) * 0.02, 2)}\")\n",
    "        return self.collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Added 52 documents. Current tokens: 19523\n",
      "1 Added 33 documents. Current tokens: 12363\n",
      "2 Added 42 documents. Current tokens: 15620\n",
      "3 Added 57 documents. Current tokens: 21219\n",
      "4 Added 82 documents. Current tokens: 30797\n",
      "5 Added 47 documents. Current tokens: 17547\n",
      "6 Added 26 documents. Current tokens: 9545\n",
      "7 Added 65 documents. Current tokens: 24580\n",
      "8 Added 67 documents. Current tokens: 25242\n",
      "9 Added 41 documents. Current tokens: 15236\n",
      "10 Added 26 documents. Current tokens: 9832\n",
      "11 Added 154 documents. Current tokens: 58182\n",
      "12 Added 34 documents. Current tokens: 12786\n",
      "13 Added 154 documents. Current tokens: 58543\n",
      "14 Added 29 documents. Current tokens: 10962\n",
      "15 Added 40 documents. Current tokens: 14892\n",
      "16 Added 63 documents. Current tokens: 18731\n",
      "17 Added 47 documents. Current tokens: 17612\n",
      "18 Added 27 documents. Current tokens: 10237\n",
      "19 Added 30 documents. Current tokens: 11152\n",
      "20 Added 93 documents. Current tokens: 35219\n",
      "21 Added 79 documents. Current tokens: 30109\n",
      "22 Added 29 documents. Current tokens: 10935\n",
      "23 Added 488 documents. Current tokens: 173644\n",
      "24 Added 60 documents. Current tokens: 22646\n",
      "25 Added 122 documents. Current tokens: 46002\n",
      "26 Added 66 documents. Current tokens: 24946\n",
      "27 Added 68 documents. Current tokens: 25508\n",
      "28 Added 51 documents. Current tokens: 18990\n",
      "29 Added 63 documents. Current tokens: 23980\n",
      "30 Added 53 documents. Current tokens: 20062\n",
      "31 Added 45 documents. Current tokens: 17046\n",
      "32 Added 77 documents. Current tokens: 29133\n",
      "33 Added 57 documents. Current tokens: 21461\n",
      "34 Added 145 documents. Current tokens: 55006\n",
      "35 Added 27 documents. Current tokens: 9854\n",
      "36 Added 18 documents. Current tokens: 6801\n",
      "37 Added 16 documents. Current tokens: 5788\n",
      "38 Added 34 documents. Current tokens: 12737\n",
      "39 Added 28 documents. Current tokens: 10297\n",
      "40 Added 68 documents. Current tokens: 25618\n",
      "41 Added 27 documents. Current tokens: 10248\n",
      "42 Added 48 documents. Current tokens: 17900\n",
      "43 Added 273 documents. Current tokens: 96843\n",
      "44 Added 98 documents. Current tokens: 33430\n",
      "45 Added 34 documents. Current tokens: 12534\n",
      "46 Added 118 documents. Current tokens: 44476\n",
      "47 Added 30 documents. Current tokens: 11191\n",
      "48 Added 34 documents. Current tokens: 12890\n",
      "49 Added 47 documents. Current tokens: 17893\n",
      "50 Added 42 documents. Current tokens: 15511\n",
      "51 Added 63 documents. Current tokens: 23569\n",
      "52 Added 54 documents. Current tokens: 20175\n",
      "53 Added 58 documents. Current tokens: 21741\n",
      "54 Added 31 documents. Current tokens: 11607\n",
      "55 Added 540 documents. Current tokens: 205560\n",
      "56 Added 30 documents. Current tokens: 11258\n",
      "57 Added 67 documents. Current tokens: 25176\n",
      "58 Added 26 documents. Current tokens: 9885\n",
      "59 Added 82 documents. Current tokens: 30922\n",
      "60 Added 32 documents. Current tokens: 11882\n",
      "61 Added 218 documents. Current tokens: 82225\n",
      "62 Added 108 documents. Current tokens: 40793\n",
      "63 Added 56 documents. Current tokens: 20968\n",
      "64 Added 121 documents. Current tokens: 45574\n",
      "65 Added 23 documents. Current tokens: 8749\n",
      "66 Added 82 documents. Current tokens: 30807\n",
      "67 Added 130 documents. Current tokens: 49077\n",
      "68 Added 103 documents. Current tokens: 37703\n",
      "69 Added 96 documents. Current tokens: 36570\n",
      "70 Added 94 documents. Current tokens: 35340\n",
      "71 Added 41 documents. Current tokens: 15195\n",
      "72 Added 39 documents. Current tokens: 14629\n",
      "73 Added 39 documents. Current tokens: 14693\n",
      "74 Added 84 documents. Current tokens: 31790\n",
      "75 Added 57 documents. Current tokens: 21490\n",
      "76 Added 94 documents. Current tokens: 35960\n",
      "77 Added 34 documents. Current tokens: 12757\n",
      "78 Added 49 documents. Current tokens: 18569\n",
      "79 Added 27 documents. Current tokens: 9862\n",
      "80 Added 42 documents. Current tokens: 15985\n",
      "81 Added 54 documents. Current tokens: 20325\n",
      "82 Added 106 documents. Current tokens: 39344\n",
      "83 Added 35 documents. Current tokens: 13171\n",
      "84 Added 50 documents. Current tokens: 18729\n",
      "85 Added 61 documents. Current tokens: 22841\n",
      "86 Added 43 documents. Current tokens: 16026\n",
      "87 Added 97 documents. Current tokens: 36763\n",
      "88 Added 20 documents. Current tokens: 7571\n",
      "89 Added 42 documents. Current tokens: 13239\n",
      "90 Added 110 documents. Current tokens: 41604\n",
      "91 Added 38 documents. Current tokens: 14264\n",
      "92 Added 134 documents. Current tokens: 50720\n",
      "93 Added 34 documents. Current tokens: 12714\n",
      "94 Added 58 documents. Current tokens: 20949\n",
      "95 Added 61 documents. Current tokens: 23126\n",
      "96 Added 97 documents. Current tokens: 36258\n",
      "97 Added 73 documents. Current tokens: 27439\n",
      "98 Added 43 documents. Current tokens: 16379\n",
      "99 Added 26 documents. Current tokens: 9673\n",
      "100 Added 97 documents. Current tokens: 36442\n",
      "101 Added 60 documents. Current tokens: 22813\n",
      "102 Added 41 documents. Current tokens: 15333\n",
      "103 Added 16 documents. Current tokens: 5962\n",
      "104 Added 53 documents. Current tokens: 19823\n",
      "105 Added 41 documents. Current tokens: 14571\n",
      "106 Added 56 documents. Current tokens: 21066\n",
      "107 Added 74 documents. Current tokens: 28155\n",
      "108 Added 44 documents. Current tokens: 16531\n",
      "109 Added 48 documents. Current tokens: 17914\n",
      "110 Added 31 documents. Current tokens: 11371\n",
      "111 Added 85 documents. Current tokens: 32140\n",
      "112 Added 7 documents. Current tokens: 2531\n",
      "113 Added 16 documents. Current tokens: 5752\n",
      "114 Added 97 documents. Current tokens: 36627\n",
      "115 Added 32 documents. Current tokens: 11852\n",
      "116 Added 80 documents. Current tokens: 30339\n",
      "117 Added 49 documents. Current tokens: 18634\n",
      "118 Added 63 documents. Current tokens: 20535\n",
      "119 Added 74 documents. Current tokens: 27947\n",
      "120 Added 65 documents. Current tokens: 24354\n",
      "121 Added 29 documents. Current tokens: 11031\n",
      "122 Added 30 documents. Current tokens: 11124\n",
      "123 Added 19 documents. Current tokens: 7194\n",
      "124 Added 108 documents. Current tokens: 40689\n",
      "125 Added 316 documents. Current tokens: 118787\n",
      "126 Added 44 documents. Current tokens: 16493\n",
      "127 Added 64 documents. Current tokens: 24074\n",
      "128 Added 48 documents. Current tokens: 18133\n",
      "129 Added 99 documents. Current tokens: 37498\n",
      "130 Added 12 documents. Current tokens: 4212\n",
      "131 Added 24 documents. Current tokens: 8847\n",
      "132 Added 32 documents. Current tokens: 11804\n",
      "133 Added 80 documents. Current tokens: 30124\n",
      "134 Added 169 documents. Current tokens: 64112\n",
      "135 Added 64 documents. Current tokens: 23979\n",
      "136 Added 37 documents. Current tokens: 14011\n",
      "137 Added 67 documents. Current tokens: 24858\n",
      "138 Added 110 documents. Current tokens: 40901\n",
      "139 Added 23 documents. Current tokens: 8455\n",
      "140 Added 30 documents. Current tokens: 11287\n",
      "141 Added 30 documents. Current tokens: 11346\n",
      "142 Added 86 documents. Current tokens: 31285\n",
      "143 Added 84 documents. Current tokens: 31356\n",
      "144 Added 20 documents. Current tokens: 7386\n",
      "145 Added 15 documents. Current tokens: 5701\n",
      "146 Added 70 documents. Current tokens: 26590\n",
      "147 Added 25 documents. Current tokens: 9177\n",
      "148 Added 48 documents. Current tokens: 18011\n",
      "149 Added 32 documents. Current tokens: 11940\n",
      "150 Added 26 documents. Current tokens: 9301\n",
      "151 Added 124 documents. Current tokens: 46141\n",
      "152 Added 82 documents. Current tokens: 31109\n",
      "153 Added 36 documents. Current tokens: 13360\n",
      "154 Added 29 documents. Current tokens: 10888\n",
      "155 Added 69 documents. Current tokens: 25791\n",
      "156 Added 55 documents. Current tokens: 20422\n",
      "157 Added 26 documents. Current tokens: 9633\n",
      "158 Added 32 documents. Current tokens: 11992\n",
      "159 Added 52 documents. Current tokens: 19443\n",
      "160 Added 65 documents. Current tokens: 24251\n",
      "161 Added 42 documents. Current tokens: 15705\n",
      "162 Added 75 documents. Current tokens: 28415\n",
      "163 Added 51 documents. Current tokens: 19419\n",
      "164 Added 14 documents. Current tokens: 5194\n",
      "165 Added 72 documents. Current tokens: 26883\n",
      "166 Added 43 documents. Current tokens: 16131\n",
      "167 Added 57 documents. Current tokens: 21481\n",
      "168 Added 39 documents. Current tokens: 14628\n",
      "169 Added 142 documents. Current tokens: 52052\n",
      "170 Added 13 documents. Current tokens: 4987\n",
      "171 Added 72 documents. Current tokens: 26671\n",
      "172 Added 50 documents. Current tokens: 18727\n",
      "173 Added 27 documents. Current tokens: 10312\n",
      "174 Added 50 documents. Current tokens: 18847\n",
      "175 Added 45 documents. Current tokens: 17085\n",
      "176 Added 37 documents. Current tokens: 13665\n",
      "177 Added 26 documents. Current tokens: 9557\n",
      "178 Added 26 documents. Current tokens: 9742\n",
      "179 Added 40 documents. Current tokens: 14935\n",
      "180 Added 46 documents. Current tokens: 17210\n",
      "181 Added 53 documents. Current tokens: 17857\n",
      "182 Added 27 documents. Current tokens: 10034\n",
      "183 Added 79 documents. Current tokens: 30052\n",
      "184 Added 43 documents. Current tokens: 16085\n",
      "185 Added 98 documents. Current tokens: 37280\n",
      "186 Added 38 documents. Current tokens: 14312\n",
      "187 Added 26 documents. Current tokens: 9569\n",
      "188 Added 42 documents. Current tokens: 15679\n",
      "189 Added 141 documents. Current tokens: 52925\n",
      "190 Added 29 documents. Current tokens: 10828\n",
      "191 Added 39 documents. Current tokens: 14756\n",
      "192 Added 121 documents. Current tokens: 43286\n",
      "193 Added 45 documents. Current tokens: 16916\n",
      "194 Added 58 documents. Current tokens: 20370\n",
      "195 Added 68 documents. Current tokens: 25498\n",
      "196 Added 28 documents. Current tokens: 10512\n",
      "197 Added 34 documents. Current tokens: 12864\n",
      "198 Added 40 documents. Current tokens: 15021\n",
      "199 Added 36 documents. Current tokens: 13178\n",
      "200 Added 69 documents. Current tokens: 21689\n",
      "201 Added 60 documents. Current tokens: 22569\n",
      "202 Added 61 documents. Current tokens: 22464\n",
      "203 Added 72 documents. Current tokens: 27063\n",
      "204 Added 31 documents. Current tokens: 11602\n",
      "205 Added 27 documents. Current tokens: 9888\n",
      "206 Added 68 documents. Current tokens: 25938\n",
      "207 Added 28 documents. Current tokens: 10613\n",
      "208 Added 38 documents. Current tokens: 14285\n",
      "209 Added 65 documents. Current tokens: 24627\n",
      "210 Added 30 documents. Current tokens: 11008\n",
      "211 Added 13 documents. Current tokens: 4764\n",
      "212 Added 84 documents. Current tokens: 31730\n",
      "213 Added 51 documents. Current tokens: 15919\n",
      "214 Added 41 documents. Current tokens: 15726\n",
      "215 Added 20 documents. Current tokens: 7364\n",
      "216 Added 75 documents. Current tokens: 28409\n",
      "217 Added 28 documents. Current tokens: 10403\n",
      "218 Added 46 documents. Current tokens: 17591\n",
      "219 Added 47 documents. Current tokens: 17758\n",
      "220 Added 82 documents. Current tokens: 27029\n",
      "221 Added 70 documents. Current tokens: 26359\n",
      "222 Added 94 documents. Current tokens: 35451\n",
      "223 Added 96 documents. Current tokens: 36587\n",
      "224 Added 161 documents. Current tokens: 61142\n",
      "225 Added 256 documents. Current tokens: 97089\n",
      "226 Added 59 documents. Current tokens: 21814\n",
      "227 Added 62 documents. Current tokens: 23574\n",
      "228 Added 79 documents. Current tokens: 29758\n",
      "229 Added 20 documents. Current tokens: 7281\n",
      "230 Added 84 documents. Current tokens: 31538\n",
      "231 Added 115 documents. Current tokens: 43380\n",
      "232 Added 96 documents. Current tokens: 33824\n",
      "233 Added 42 documents. Current tokens: 16059\n",
      "234 Added 159 documents. Current tokens: 58890\n",
      "235 Added 60 documents. Current tokens: 22533\n",
      "236 Added 41 documents. Current tokens: 15486\n",
      "237 Added 97 documents. Current tokens: 36917\n",
      "238 Added 55 documents. Current tokens: 20611\n",
      "239 Added 67 documents. Current tokens: 25155\n",
      "240 Added 36 documents. Current tokens: 13345\n",
      "241 Added 49 documents. Current tokens: 18465\n",
      "242 Added 51 documents. Current tokens: 19198\n",
      "243 Added 54 documents. Current tokens: 20270\n",
      "244 Added 65 documents. Current tokens: 24541\n",
      "245 Added 57 documents. Current tokens: 21623\n",
      "246 Added 105 documents. Current tokens: 39683\n",
      "247 Added 31 documents. Current tokens: 11555\n",
      "248 Added 54 documents. Current tokens: 20416\n",
      "249 Added 140 documents. Current tokens: 53066\n",
      "250 Added 48 documents. Current tokens: 17879\n",
      "251 Added 33 documents. Current tokens: 12214\n",
      "252 Added 37 documents. Current tokens: 13849\n",
      "253 Added 42 documents. Current tokens: 15550\n",
      "254 Added 22 documents. Current tokens: 7993\n",
      "255 Added 42 documents. Current tokens: 15735\n",
      "256 Added 99 documents. Current tokens: 37245\n",
      "257 Added 91 documents. Current tokens: 34763\n",
      "258 Added 68 documents. Current tokens: 23915\n",
      "259 Added 85 documents. Current tokens: 32191\n",
      "260 Added 24 documents. Current tokens: 8713\n",
      "261 Added 47 documents. Current tokens: 17832\n",
      "262 Added 39 documents. Current tokens: 14426\n",
      "263 Added 16 documents. Current tokens: 6036\n",
      "264 Added 54 documents. Current tokens: 20213\n",
      "265 Added 50 documents. Current tokens: 18652\n",
      "266 Added 63 documents. Current tokens: 24083\n",
      "267 Added 12 documents. Current tokens: 4199\n",
      "268 Added 83 documents. Current tokens: 31432\n",
      "269 Added 46 documents. Current tokens: 17210\n",
      "270 Added 29 documents. Current tokens: 10679\n",
      "271 Added 55 documents. Current tokens: 20956\n",
      "272 Added 35 documents. Current tokens: 13049\n",
      "273 Added 70 documents. Current tokens: 26615\n",
      "274 Added 28 documents. Current tokens: 10473\n",
      "275 Added 32 documents. Current tokens: 11974\n",
      "276 Added 51 documents. Current tokens: 19193\n",
      "277 Added 73 documents. Current tokens: 27439\n",
      "278 Added 153 documents. Current tokens: 58308\n",
      "279 Added 17 documents. Current tokens: 6320\n",
      "280 Added 17 documents. Current tokens: 6066\n",
      "281 Added 39 documents. Current tokens: 14411\n",
      "282 Added 175 documents. Current tokens: 66158\n",
      "283 Added 54 documents. Current tokens: 20486\n",
      "284 Added 67 documents. Current tokens: 25107\n",
      "285 Added 34 documents. Current tokens: 12564\n",
      "286 Added 31 documents. Current tokens: 11524\n",
      "287 Added 58 documents. Current tokens: 20973\n",
      "288 Added 161 documents. Current tokens: 58341\n",
      "289 Added 49 documents. Current tokens: 18283\n",
      "290 Added 34 documents. Current tokens: 12857\n",
      "291 Added 48 documents. Current tokens: 17880\n",
      "292 Added 77 documents. Current tokens: 29185\n",
      "293 Added 69 documents. Current tokens: 26040\n",
      "294 Added 53 documents. Current tokens: 19703\n",
      "295 Added 29 documents. Current tokens: 10696\n",
      "296 Added 14 documents. Current tokens: 5026\n",
      "297 Added 56 documents. Current tokens: 20471\n",
      "298 Added 52 documents. Current tokens: 19538\n",
      "299 Added 48 documents. Current tokens: 17927\n",
      "300 Added 34 documents. Current tokens: 12853\n",
      "301 Added 137 documents. Current tokens: 51683\n",
      "302 Added 33 documents. Current tokens: 12348\n",
      "303 Added 34 documents. Current tokens: 12616\n",
      "304 Added 38 documents. Current tokens: 14339\n",
      "305 Added 18 documents. Current tokens: 6732\n",
      "306 Added 22 documents. Current tokens: 8017\n",
      "307 Added 51 documents. Current tokens: 19200\n",
      "308 Added 24 documents. Current tokens: 8762\n",
      "309 Added 47 documents. Current tokens: 17892\n",
      "310 Added 25 documents. Current tokens: 9237\n",
      "311 Added 19 documents. Current tokens: 6830\n",
      "312 Added 26 documents. Current tokens: 9610\n",
      "313 Added 47 documents. Current tokens: 17955\n",
      "314 Added 31 documents. Current tokens: 11707\n",
      "315 Added 15 documents. Current tokens: 5394\n",
      "316 Added 13 documents. Current tokens: 4894\n",
      "317 Added 42 documents. Current tokens: 15663\n",
      "318 Added 28 documents. Current tokens: 10325\n",
      "319 Added 80 documents. Current tokens: 29919\n",
      "320 Added 74 documents. Current tokens: 27586\n",
      "321 Added 40 documents. Current tokens: 14838\n",
      "322 Added 50 documents. Current tokens: 18940\n",
      "323 Added 21 documents. Current tokens: 7871\n",
      "324 Added 26 documents. Current tokens: 9766\n",
      "325 Added 48 documents. Current tokens: 17429\n",
      "326 Added 27 documents. Current tokens: 9894\n",
      "327 Added 102 documents. Current tokens: 38549\n",
      "328 Added 44 documents. Current tokens: 16551\n",
      "329 Added 108 documents. Current tokens: 40981\n",
      "330 Added 54 documents. Current tokens: 20322\n",
      "331 Added 68 documents. Current tokens: 25706\n",
      "332 Added 29 documents. Current tokens: 11040\n",
      "333 Added 34 documents. Current tokens: 12730\n",
      "334 Added 231 documents. Current tokens: 84692\n",
      "335 Added 28 documents. Current tokens: 10618\n",
      "336 Added 53 documents. Current tokens: 20141\n",
      "337 Added 34 documents. Current tokens: 12729\n",
      "338 Added 71 documents. Current tokens: 26869\n",
      "339 Added 28 documents. Current tokens: 10269\n",
      "340 Added 33 documents. Current tokens: 12509\n",
      "341 Added 17 documents. Current tokens: 6141\n",
      "342 Added 41 documents. Current tokens: 15571\n",
      "343 Added 103 documents. Current tokens: 39005\n",
      "344 Added 32 documents. Current tokens: 11895\n",
      "345 Added 27 documents. Current tokens: 10159\n",
      "346 Added 90 documents. Current tokens: 33162\n",
      "347 Added 45 documents. Current tokens: 16671\n",
      "348 Added 54 documents. Current tokens: 20290\n",
      "349 Added 32 documents. Current tokens: 11734\n",
      "350 Added 48 documents. Current tokens: 17834\n",
      "351 Added 82 documents. Current tokens: 30961\n",
      "352 Added 36 documents. Current tokens: 13164\n",
      "353 Added 30 documents. Current tokens: 11156\n",
      "354 Added 42 documents. Current tokens: 15454\n",
      "355 Added 51 documents. Current tokens: 19419\n",
      "356 Added 49 documents. Current tokens: 18035\n",
      "357 Added 56 documents. Current tokens: 21286\n",
      "358 Added 79 documents. Current tokens: 29170\n",
      "359 Added 27 documents. Current tokens: 10138\n",
      "360 Added 17 documents. Current tokens: 6500\n",
      "361 Added 19 documents. Current tokens: 7138\n",
      "362 Added 71 documents. Current tokens: 26542\n",
      "363 Added 103 documents. Current tokens: 38968\n",
      "364 Added 42 documents. Current tokens: 14535\n",
      "365 Added 79 documents. Current tokens: 29901\n",
      "366 Added 59 documents. Current tokens: 22065\n",
      "367 Added 63 documents. Current tokens: 23638\n",
      "369 Added 30 documents. Current tokens: 11124\n",
      "370 Added 33 documents. Current tokens: 12360\n",
      "371 Added 29 documents. Current tokens: 10928\n",
      "372 Added 50 documents. Current tokens: 18869\n",
      "373 Added 29 documents. Current tokens: 10736\n",
      "374 Added 20 documents. Current tokens: 7274\n",
      "375 Added 61 documents. Current tokens: 22849\n",
      "376 Added 33 documents. Current tokens: 12447\n",
      "377 Added 61 documents. Current tokens: 23015\n",
      "378 Added 30 documents. Current tokens: 11207\n",
      "379 Added 53 documents. Current tokens: 19944\n",
      "380 Added 17 documents. Current tokens: 6198\n",
      "381 Added 35 documents. Current tokens: 12905\n",
      "382 Added 69 documents. Current tokens: 25922\n",
      "383 Added 37 documents. Current tokens: 13672\n",
      "384 Added 43 documents. Current tokens: 16274\n",
      "385 Added 76 documents. Current tokens: 28155\n",
      "386 Added 142 documents. Current tokens: 41671\n",
      "387 Added 89 documents. Current tokens: 33946\n",
      "388 Added 96 documents. Current tokens: 35979\n",
      "389 Added 38 documents. Current tokens: 13875\n",
      "390 Added 60 documents. Current tokens: 22757\n",
      "391 Added 109 documents. Current tokens: 40830\n",
      "392 Added 30 documents. Current tokens: 10588\n",
      "393 Added 60 documents. Current tokens: 22604\n",
      "394 Added 65 documents. Current tokens: 24512\n",
      "395 Added 27 documents. Current tokens: 10228\n",
      "396 Added 34 documents. Current tokens: 12688\n",
      "397 Added 17 documents. Current tokens: 6320\n",
      "398 Added 158 documents. Current tokens: 59713\n",
      "399 Added 72 documents. Current tokens: 26912\n",
      "400 Added 346 documents. Current tokens: 130840\n",
      "401 Added 64 documents. Current tokens: 24296\n",
      "402 Added 19 documents. Current tokens: 6995\n",
      "403 Added 60 documents. Current tokens: 15039\n",
      "404 Added 132 documents. Current tokens: 49808\n",
      "405 Added 19 documents. Current tokens: 6904\n",
      "406 Added 27 documents. Current tokens: 10183\n",
      "407 Added 39 documents. Current tokens: 14462\n",
      "408 Added 26 documents. Current tokens: 9595\n",
      "409 Added 61 documents. Current tokens: 23126\n",
      "410 Added 54 documents. Current tokens: 20224\n",
      "411 Added 27 documents. Current tokens: 10179\n",
      "412 Added 32 documents. Current tokens: 11897\n",
      "413 Added 166 documents. Current tokens: 62377\n",
      "414 Added 28 documents. Current tokens: 10361\n",
      "415 Added 37 documents. Current tokens: 13730\n",
      "416 Added 99 documents. Current tokens: 37203\n",
      "417 Added 171 documents. Current tokens: 63735\n",
      "418 Added 112 documents. Current tokens: 41847\n",
      "419 Added 22 documents. Current tokens: 8219\n",
      "420 Added 207 documents. Current tokens: 78281\n",
      "421 Added 156 documents. Current tokens: 58887\n",
      "422 Added 33 documents. Current tokens: 12506\n",
      "Total tokens: 9657196\n",
      "Price: $1.26\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "import json\n",
    "# Initialize the Recursive Text Splitter\n",
    "#   chunk_size: int = 4000, DEFAULT_CHUNK_SIZE\n",
    "#   chunk_overlap: int = 200, DEFAULT_CHUNK_OVERLAP\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1024,\n",
    "    # chunk_overlap=256,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    "    length_function=num_tokens_from_string\n",
    ")\n",
    "\n",
    "# splitter = TokenTextSplitter(\n",
    "#     encoding_name=\"cl100k_base\",\n",
    "#     chunk_size=400,\n",
    "#     chunk_overlap=0,\n",
    "# )\n",
    "\n",
    "collection_writer = CollectionWriter()\n",
    "\n",
    "def chunking_function(content):\n",
    "    return splitter.split_text(content)\n",
    "\n",
    "# chunked_collection = collection_writer.trial_run(\"chuck_1\", chunking_function)\n",
    "chunked_collection = collection_writer.write_to_collection(\"chuck_4\", chunking_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "split_text = splitter.split_text(\"This is a test sentence. This is another test sentence. This is a third test sentence.\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=chuck_4),\n",
       " Collection(name=chuck_1),\n",
       " Collection(name=chuck_3),\n",
       " Collection(name=chuck_2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()\n",
    "# chroma_client.delete_collection(\"chuck_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20588"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-large\"\n",
    "            )\n",
    "\n",
    "collection = chroma_client.get_collection(\"chuck_8\", embedding_function=openai_ef)\n",
    "\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\"What are the two main tasks BERT is pre-trained on?\"], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and next sentence pre-\\ndiction.\\nMasked Language Model (MLM) A random\\nsample of the tokens in the input sequence is\\nselected and replaced with the special token\\n[MASK]. The MLM objective is a cross-entropy\\nloss on predicting the masked tokens. BERT uni-\\nformly selects 15% of the input tokens for possi-\\nble replacement. Of the selected tokens, 80% are\\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\\ncabulary token.\\nIn the original implementation, random mask-\\ning and replacement is performed once in the be-\\nginning and saved for the duration of training, al-\\nthough in practice, data is duplicated so the mask\\nis not always the same for every training sentence\\n(see Section 4.1).\\nNext Sentence Prediction (NSP) NSP is a bi-\\nnary classiﬁcation loss for predicting whether two\\nsegments follow each other in the original text.\\nPositive examples are created by taking consecu-\\ntive sentences from the text corpus. Negative ex-\\namples are created by pairing segments from dif-\\nferent documents. Positive and negative examples\\nare sampled with equal probability.\\nThe NSP objective was designed to improve\\nperformance on downstream tasks, such as Natural\\nLanguage Inference ( Bowman et al. ,2015 ), which\\nrequire reasoning about the relationships between\\npairs of sentences.\\n2.4 Optimization\\nBERT is optimized with Adam ( Kingma and Ba ,\\n2015 ) using the following parameters: β1= 0.9,\\nβ2= 0.999,ǫ=1e-6 and L2weight de-\\ncay of0.01. The learning rate is warmed up\\nover the ﬁrst 10,000 steps to a peak value of\\n1e-4, and then linearly decayed. BERT trains\\nwith a dropout of 0.1 on all layers and at-\\ntention weights, and a GELU activation func-\\ntion ( Hendrycks and Gimpel ,2016 ). Models are\\npretrained for S=1,000,000 updates, with mini-\\nbatches containing B=256 sequences of maxi-\\nmum length T=512 tokens.\\n2.5 Data\\nBERT is trained on a combination of B OOK COR-\\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\\nwhich totals 16GB of'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../eval_questions/eval_data.json') as f:\n",
    "    eval_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=eval_data['questions'], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning . Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become',\n",
       " ' BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ( [SEP] ). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence Aor sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token asC2RH,\\nand the ﬁnal hidden vector for the ithinput token\\nasTi2RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right orright-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\n',\n",
       " ', as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv',\n",
       " ' a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ( [SEP] ) and\\nclassiﬁer token ( [CLS] ) which are only in-\\ntroduced at ﬁne-tuning time; BERT learns\\n[SEP] ,[CLS] and sentence A/Bembed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall ﬁne-tuning experiments; BERT chooses a\\ntask-speciﬁc ﬁne-tuning learning rate which\\nperforms the best on the development set.To isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of ﬁne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciﬁc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe ﬁgure,Erepresents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special',\n",
       " ', in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019\\nword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems,']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = eval_data['questions'][0]\n",
    "# context = results['documents'][0][0]\n",
    "\n",
    "from openai import OpenAI\n",
    "import backoff\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_time=60)\n",
    "def get_retrieval_precision_indicator(question, context, four=True):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\" if four else \"gpt-3.5-turbo\",\n",
    "        # messages=[\n",
    "        #     {\"role\": \"system\", \"content\": get_retrieval_precision_prompt(question, context)},\n",
    "        #     {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "        # ]\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond using markdown.\"},\n",
    "            {\"role\": \"user\", \"content\": get_retrieval_precision_prompt(question, context)}\n",
    "        ]\n",
    "        )\n",
    "    \n",
    "    return 1 if completion.choices[0].message.content.lower().strip() == 'true' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "retrieval_precision_matrix = np.zeros((len(eval_data['questions']), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0, Context 0: 1.0\n",
      "Question 0, Context 0: 1.0\n",
      "Question 0, Context 0: 1.0\n",
      "Question 0, Context 1: 1.0\n",
      "Question 0, Context 1: 1.0\n",
      "Question 0, Context 1: 1.0\n",
      "Question 0, Context 2: 0.0\n",
      "Question 0, Context 2: 1.0\n",
      "Question 0, Context 2: 1.0\n",
      "Question 0, Context 3: 1.0\n",
      "Question 0, Context 3: 1.0\n",
      "Question 0, Context 3: 0.0\n",
      "Question 0, Context 4: 1.0\n",
      "Question 0, Context 4: 0.0\n",
      "Question 0, Context 4: 0.0\n",
      "Question 1, Context 0: 1.0\n",
      "Question 1, Context 0: 0.0\n",
      "Question 1, Context 0: 1.0\n",
      "Question 1, Context 1: 0.0\n",
      "Question 1, Context 1: 1.0\n",
      "Question 1, Context 1: 0.0\n",
      "Question 1, Context 2: 0.0\n",
      "Question 1, Context 2: 0.0\n",
      "Question 1, Context 2: 0.0\n",
      "Question 1, Context 3: 0.0\n",
      "Question 1, Context 3: 0.0\n",
      "Question 1, Context 3: 1.0\n",
      "Question 1, Context 4: 0.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 2, Context 0: 1.0\n",
      "Question 2, Context 0: 1.0\n",
      "Question 2, Context 0: 1.0\n",
      "Question 2, Context 1: 0.0\n",
      "Question 2, Context 1: 1.0\n",
      "Question 2, Context 1: 1.0\n",
      "Question 2, Context 2: 1.0\n",
      "Question 2, Context 2: 1.0\n",
      "Question 2, Context 2: 1.0\n",
      "Question 2, Context 3: 0.0\n",
      "Question 2, Context 3: 0.0\n",
      "Question 2, Context 3: 0.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 2, Context 4: 0.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 3, Context 0: 1.0\n",
      "Question 3, Context 0: 0.0\n",
      "Question 3, Context 0: 0.0\n",
      "Question 3, Context 1: 1.0\n",
      "Question 3, Context 1: 0.0\n",
      "Question 3, Context 1: 1.0\n",
      "Question 3, Context 2: 1.0\n",
      "Question 3, Context 2: 1.0\n",
      "Question 3, Context 2: 0.0\n",
      "Question 3, Context 3: 1.0\n",
      "Question 3, Context 3: 1.0\n",
      "Question 3, Context 3: 0.0\n",
      "Question 3, Context 4: 0.0\n",
      "Question 3, Context 4: 1.0\n",
      "Question 3, Context 4: 1.0\n",
      "Question 4, Context 0: 0.0\n",
      "Question 4, Context 0: 0.0\n",
      "Question 4, Context 0: 0.0\n",
      "Question 4, Context 1: 1.0\n",
      "Question 4, Context 1: 0.0\n",
      "Question 4, Context 1: 0.0\n",
      "Question 4, Context 2: 0.0\n",
      "Question 4, Context 2: 1.0\n",
      "Question 4, Context 3: 0.0\n",
      "Question 4, Context 3: 0.0\n",
      "Question 4, Context 4: 0.0\n",
      "Question 4, Context 2: 1.0\n",
      "Question 4, Context 4: 1.0\n",
      "Question 4, Context 3: 0.0\n",
      "Question 5, Context 0: 1.0\n",
      "Question 5, Context 0: 1.0\n",
      "Question 4, Context 4: 1.0\n",
      "Question 5, Context 1: 0.0\n",
      "Question 5, Context 1: 1.0\n",
      "Question 5, Context 2: 0.0\n",
      "Question 5, Context 0: 0.0\n",
      "Question 5, Context 2: 0.0\n",
      "Question 5, Context 1: 1.0\n",
      "Question 5, Context 3: 1.0\n",
      "Question 5, Context 3: 0.0\n",
      "Question 5, Context 2: 0.0\n",
      "Question 5, Context 4: 1.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 5, Context 3: 1.0\n",
      "Question 6, Context 0: 1.0\n",
      "Question 6, Context 0: 0.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 6, Context 1: 1.0\n",
      "Question 6, Context 1: 1.0\n",
      "Question 6, Context 0: 1.0\n",
      "Question 6, Context 2: 1.0\n",
      "Question 6, Context 2: 1.0\n",
      "Question 6, Context 1: 1.0\n",
      "Question 6, Context 3: 0.0\n",
      "Question 6, Context 3: 0.0\n",
      "Question 6, Context 2: 1.0\n",
      "Question 6, Context 4: 0.0\n",
      "Question 6, Context 4: 0.0\n",
      "Question 6, Context 3: 0.0\n",
      "Question 7, Context 0: 1.0\n",
      "Question 6, Context 4: 0.0\n",
      "Question 7, Context 0: 1.0\n",
      "Question 7, Context 1: 0.0\n",
      "Question 7, Context 1: 0.0\n",
      "Question 7, Context 0: 0.0\n",
      "Question 7, Context 2: 0.0\n",
      "Question 7, Context 2: 0.0\n",
      "Question 7, Context 3: 0.0\n",
      "Question 7, Context 3: 0.0\n",
      "Question 7, Context 4: 0.0\n",
      "Question 7, Context 4: 1.0\n",
      "Question 8, Context 0: 1.0\n",
      "Question 8, Context 0: 0.0\n",
      "Question 7, Context 1: 0.0\n",
      "Question 8, Context 1: 1.0\n",
      "Question 8, Context 1: 1.0\n",
      "Question 7, Context 2: 0.0\n",
      "Question 8, Context 2: 0.0\n",
      "Question 8, Context 2: 0.0\n",
      "Question 7, Context 3: 0.0\n",
      "Question 8, Context 3: 0.0\n",
      "Question 8, Context 3: 1.0\n",
      "Question 7, Context 4: 0.0\n",
      "Question 8, Context 4: 0.0\n",
      "Question 8, Context 4: 1.0\n",
      "Question 8, Context 0: 0.0\n",
      "Question 9, Context 0: 0.0\n",
      "Question 9, Context 0: 0.0\n",
      "Question 8, Context 1: 0.0\n",
      "Question 8, Context 2: 0.0\n",
      "Question 9, Context 1: 1.0\n",
      "Question 9, Context 1: 1.0\n",
      "Question 8, Context 3: 1.0\n",
      "Question 9, Context 2: 0.0\n",
      "Question 9, Context 2: 0.0\n",
      "Question 9, Context 3: 0.0\n",
      "Question 8, Context 4: 1.0\n",
      "Question 9, Context 3: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 9, Context 0: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 10, Context 0: 0.0\n",
      "Question 9, Context 1: 1.0\n",
      "Question 10, Context 0: 1.0\n",
      "Question 10, Context 1: 1.0\n",
      "Question 9, Context 2: 0.0\n",
      "Question 10, Context 1: 1.0\n",
      "Question 10, Context 2: 1.0\n",
      "Question 9, Context 3: 0.0\n",
      "Question 10, Context 2: 0.0\n",
      "Question 10, Context 3: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 10, Context 3: 0.0\n",
      "Question 10, Context 4: 0.0\n",
      "Question 10, Context 4: 1.0\n",
      "Question 10, Context 0: 1.0\n",
      "Question 11, Context 0: 0.0\n",
      "Question 11, Context 0: 0.0\n",
      "Question 10, Context 1: 1.0\n",
      "Question 11, Context 1: 1.0\n",
      "Question 10, Context 2: 0.0\n",
      "Question 11, Context 1: 1.0\n",
      "Question 11, Context 2: 0.0\n",
      "Question 10, Context 3: 0.0\n",
      "Question 11, Context 2: 0.0\n",
      "Question 11, Context 3: 1.0\n",
      "Question 11, Context 3: 1.0\n",
      "Question 10, Context 4: 0.0\n",
      "Question 11, Context 4: 0.0\n",
      "Question 11, Context 4: 0.0\n",
      "Question 11, Context 0: 0.0\n",
      "Question 12, Context 0: 0.0Question 12, Context 0: 1.0\n",
      "\n",
      "Question 11, Context 1: 1.0\n",
      "Question 12, Context 1: 0.0\n",
      "Question 11, Context 2: 0.0\n",
      "Question 12, Context 1: 0.0\n",
      "Question 12, Context 2: 0.0\n",
      "Question 12, Context 2: 0.0\n",
      "Question 11, Context 3: 0.0\n",
      "Question 12, Context 3: 0.0Question 12, Context 3: 1.0\n",
      "\n",
      "Question 11, Context 4: 0.0\n",
      "Question 12, Context 4: 0.0\n",
      "Question 12, Context 0: 0.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 13, Context 0: 0.0\n",
      "Question 12, Context 1: 0.0\n",
      "Question 13, Context 0: 1.0\n",
      "Question 12, Context 2: 0.0\n",
      "Question 13, Context 1: 0.0\n",
      "Question 13, Context 1: 0.0\n",
      "Question 12, Context 3: 1.0\n",
      "Question 13, Context 2: 0.0\n",
      "Question 13, Context 2: 1.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 13, Context 3: 0.0\n",
      "Question 13, Context 3: 1.0\n",
      "Question 13, Context 0: 1.0\n",
      "Question 13, Context 4: 0.0\n",
      "Question 13, Context 4: 0.0\n",
      "Question 14, Context 0: 1.0\n",
      "Question 14, Context 0: 1.0\n",
      "Question 14, Context 1: 0.0\n",
      "Question 13, Context 1: 0.0\n",
      "Question 14, Context 1: 0.0\n",
      "Question 14, Context 2: 0.0\n",
      "Question 13, Context 2: 1.0\n",
      "Question 14, Context 2: 0.0\n",
      "Question 14, Context 3: 1.0\n",
      "Question 13, Context 3: 1.0\n",
      "Question 14, Context 3: 1.0\n",
      "Question 14, Context 4: 0.0\n",
      "Question 13, Context 4: 0.0\n",
      "Question 15, Context 0: 1.0\n",
      "Question 14, Context 4: 0.0\n",
      "Question 14, Context 0: 1.0\n",
      "Question 15, Context 0: 1.0\n",
      "Question 15, Context 1: 1.0\n",
      "Question 14, Context 1: 0.0\n",
      "Question 15, Context 2: 1.0\n",
      "Question 14, Context 2: 0.0\n",
      "Question 15, Context 1: 1.0\n",
      "Question 15, Context 3: 1.0\n",
      "Question 14, Context 3: 1.0\n",
      "Question 15, Context 2: 1.0\n",
      "Question 15, Context 3: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 14, Context 4: 0.0\n",
      "Question 15, Context 0: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 16, Context 0: 1.0\n",
      "Question 16, Context 0: 0.0\n",
      "Question 16, Context 1: 0.0\n",
      "Question 15, Context 1: 1.0\n",
      "Question 16, Context 2: 1.0\n",
      "Question 16, Context 1: 1.0\n",
      "Question 15, Context 2: 1.0\n",
      "Question 16, Context 3: 1.0\n",
      "Question 15, Context 3: 1.0\n",
      "Question 16, Context 2: 1.0\n",
      "Question 16, Context 3: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 16, Context 0: 1.0\n",
      "Question 17, Context 0: 1.0\n",
      "Question 17, Context 0: 0.0\n",
      "Question 16, Context 1: 1.0\n",
      "Question 17, Context 1: 1.0\n",
      "Question 17, Context 1: 0.0\n",
      "Question 17, Context 2: 1.0\n",
      "Question 16, Context 2: 1.0\n",
      "Question 17, Context 2: 0.0\n",
      "Question 17, Context 3: 1.0\n",
      "Question 16, Context 3: 1.0\n",
      "Question 17, Context 4: 0.0\n",
      "Question 17, Context 3: 1.0\n",
      "Question 16, Context 4: 0.0\n",
      "Question 18, Context 0: 1.0\n",
      "Question 17, Context 0: 1.0\n",
      "Question 17, Context 4: 1.0\n",
      "Question 18, Context 1: 1.0\n",
      "Question 17, Context 1: 1.0\n",
      "Question 18, Context 0: 0.0\n",
      "Question 18, Context 2: 0.0\n",
      "Question 17, Context 2: 1.0\n",
      "Question 18, Context 1: 0.0\n",
      "Question 18, Context 3: 1.0\n",
      "Question 18, Context 2: 0.0\n",
      "Question 17, Context 3: 1.0\n",
      "Question 18, Context 4: 0.0\n",
      "Question 18, Context 3: 0.0\n",
      "Question 17, Context 4: 1.0\n",
      "Question 19, Context 0: 1.0\n",
      "Question 18, Context 4: 1.0\n",
      "Question 18, Context 0: 0.0\n",
      "Question 19, Context 1: 1.0\n",
      "Question 19, Context 0: 0.0\n",
      "Question 18, Context 1: 1.0\n",
      "Question 19, Context 2: 0.0\n",
      "Question 19, Context 1: 0.0\n",
      "Question 18, Context 2: 0.0\n",
      "Question 19, Context 3: 0.0\n",
      "Question 19, Context 2: 0.0\n",
      "Question 18, Context 3: 0.0\n",
      "Question 19, Context 4: 0.0\n",
      "Question 19, Context 3: 0.0\n",
      "Question 18, Context 4: 0.0\n",
      "Question 20, Context 0: 1.0\n",
      "Question 19, Context 4: 1.0\n",
      "Question 19, Context 0: 1.0\n",
      "Question 20, Context 1: 1.0\n",
      "Question 19, Context 1: 0.0\n",
      "Question 20, Context 2: 1.0\n",
      "Question 20, Context 0: 1.0\n",
      "Question 19, Context 2: 0.0\n",
      "Question 20, Context 1: 1.0\n",
      "Question 20, Context 3: 1.0\n",
      "Question 19, Context 3: 0.0\n",
      "Question 20, Context 2: 0.0\n",
      "Question 20, Context 4: 1.0\n",
      "Question 19, Context 4: 0.0\n",
      "Question 20, Context 3: 1.0\n",
      "Question 21, Context 0: 1.0\n",
      "Question 20, Context 0: 1.0\n",
      "Question 20, Context 4: 0.0\n",
      "Question 21, Context 1: 1.0\n",
      "Question 20, Context 1: 1.0\n",
      "Question 21, Context 0: 0.0\n",
      "Question 21, Context 2: 1.0\n",
      "Question 20, Context 2: 1.0\n",
      "Question 21, Context 1: 0.0\n",
      "Question 21, Context 3: 0.0\n",
      "Question 20, Context 3: 1.0\n",
      "Question 21, Context 2: 1.0\n",
      "Question 21, Context 4: 0.0\n",
      "Question 20, Context 4: 1.0\n",
      "Question 21, Context 3: 0.0\n",
      "Question 22, Context 0: 1.0\n",
      "Question 21, Context 0: 1.0\n",
      "Question 21, Context 4: 0.0\n",
      "Question 22, Context 1: 0.0\n",
      "Question 21, Context 1: 0.0\n",
      "Question 22, Context 2: 0.0\n",
      "Question 22, Context 0: 1.0\n",
      "Question 22, Context 1: 0.0\n",
      "Question 21, Context 2: 1.0\n",
      "Question 22, Context 3: 1.0\n",
      "Question 21, Context 3: 1.0\n",
      "Question 22, Context 2: 0.0\n",
      "Question 22, Context 4: 1.0\n",
      "Question 21, Context 4: 0.0\n",
      "Question 22, Context 3: 0.0\n",
      "Question 23, Context 0: 1.0\n",
      "Question 22, Context 4: 0.0\n",
      "Question 23, Context 1: 1.0\n",
      "Question 22, Context 0: 1.0\n",
      "Question 23, Context 0: 1.0\n",
      "Question 23, Context 2: 1.0\n",
      "Question 22, Context 1: 0.0\n",
      "Question 23, Context 1: 1.0\n",
      "Question 23, Context 3: 1.0\n",
      "Question 22, Context 2: 0.0\n",
      "Question 23, Context 2: 1.0\n",
      "Question 22, Context 3: 1.0\n",
      "Question 23, Context 4: 0.0\n",
      "Question 22, Context 4: 0.0\n",
      "Question 24, Context 0: 1.0\n",
      "Question 23, Context 0: 1.0\n",
      "Question 23, Context 3: 1.0\n",
      "Question 24, Context 1: 1.0\n",
      "Question 23, Context 1: 0.0\n",
      "Question 23, Context 4: 0.0\n",
      "Question 23, Context 2: 1.0\n",
      "Question 24, Context 2: 0.0\n",
      "Question 24, Context 0: 1.0\n",
      "Question 23, Context 3: 0.0\n",
      "Question 24, Context 1: 1.0\n",
      "Question 24, Context 3: 0.0\n",
      "Question 23, Context 4: 1.0\n",
      "Question 24, Context 2: 0.0\n",
      "Question 24, Context 4: 0.0\n",
      "Question 24, Context 0: 1.0\n",
      "Question 24, Context 3: 0.0\n",
      "Question 25, Context 0: 1.0\n",
      "Question 24, Context 1: 1.0\n",
      "Question 24, Context 4: 1.0\n",
      "Question 25, Context 1: 0.0\n",
      "Question 25, Context 0: 0.0\n",
      "Question 25, Context 2: 0.0\n",
      "Question 25, Context 1: 1.0\n",
      "Question 25, Context 3: 0.0\n",
      "Question 24, Context 2: 0.0\n",
      "Question 25, Context 4: 1.0\n",
      "Question 25, Context 2: 0.0\n",
      "Question 24, Context 3: 0.0\n",
      "Question 26, Context 0: 1.0\n",
      "Question 25, Context 3: 1.0\n",
      "Question 24, Context 4: 0.0\n",
      "Question 26, Context 1: 0.0\n",
      "Question 25, Context 4: 1.0\n",
      "Question 25, Context 0: 0.0\n",
      "Question 26, Context 0: 0.0\n",
      "Question 25, Context 1: 1.0\n",
      "Question 26, Context 2: 0.0\n",
      "Question 26, Context 1: 1.0\n",
      "Question 25, Context 2: 0.0\n",
      "Question 26, Context 2: 0.0\n",
      "Question 26, Context 3: 0.0\n",
      "Question 25, Context 3: 1.0\n",
      "Question 26, Context 4: 0.0\n",
      "Question 26, Context 3: 0.0\n",
      "Question 25, Context 4: 0.0\n",
      "Question 26, Context 4: 0.0\n",
      "Question 27, Context 0: 0.0\n",
      "Question 26, Context 0: 1.0\n",
      "Question 27, Context 0: 0.0\n",
      "Question 26, Context 1: 1.0\n",
      "Question 27, Context 1: 0.0\n",
      "Question 27, Context 1: 0.0\n",
      "Question 27, Context 2: 0.0\n",
      "Question 26, Context 2: 0.0\n",
      "Question 27, Context 2: 0.0\n",
      "Question 27, Context 3: 1.0\n",
      "Question 26, Context 3: 0.0\n",
      "Question 27, Context 3: 1.0\n",
      "Question 27, Context 4: 0.0\n",
      "Question 26, Context 4: 0.0\n",
      "Question 27, Context 4: 0.0\n",
      "Question 28, Context 0: 0.0\n",
      "Question 27, Context 0: 1.0\n",
      "Question 28, Context 0: 0.0\n",
      "Question 27, Context 1: 0.0\n",
      "Question 28, Context 1: 1.0\n",
      "Question 28, Context 1: 1.0\n",
      "Question 28, Context 2: 0.0\n",
      "Question 27, Context 2: 0.0\n",
      "Question 28, Context 2: 1.0\n",
      "Question 28, Context 3: 1.0\n",
      "Question 27, Context 3: 1.0\n",
      "Question 28, Context 3: 1.0\n",
      "Question 28, Context 4: 1.0\n",
      "Question 27, Context 4: 0.0\n",
      "Question 28, Context 4: 1.0\n",
      "Question 28, Context 0: 0.0\n",
      "Question 29, Context 0: 0.0\n",
      "Question 29, Context 0: 0.0\n",
      "Question 28, Context 1: 1.0\n",
      "Question 29, Context 1: 1.0\n",
      "Question 29, Context 1: 1.0\n",
      "Question 28, Context 2: 1.0\n",
      "Question 29, Context 2: 1.0\n",
      "Question 29, Context 2: 0.0\n",
      "Question 28, Context 3: 0.0\n",
      "Question 29, Context 3: 0.0\n",
      "Question 29, Context 3: 0.0\n",
      "Question 28, Context 4: 0.0\n",
      "Question 29, Context 4: 1.0\n",
      "Question 29, Context 4: 0.0\n",
      "Question 29, Context 0: 1.0\n",
      "Question 30, Context 0: 1.0\n",
      "Question 30, Context 0: 1.0\n",
      "Question 29, Context 1: 1.0\n",
      "Question 30, Context 1: 1.0\n",
      "Question 29, Context 2: 1.0\n",
      "Question 30, Context 1: 0.0\n",
      "Question 30, Context 2: 1.0\n",
      "Question 29, Context 3: 0.0\n",
      "Question 30, Context 2: 0.0\n",
      "Question 30, Context 3: 1.0\n",
      "Question 29, Context 4: 1.0\n",
      "Question 30, Context 4: 0.0\n",
      "Question 30, Context 3: 0.0\n",
      "Question 30, Context 0: 1.0\n",
      "Question 30, Context 4: 0.0\n",
      "Question 31, Context 0: 1.0\n",
      "Question 30, Context 1: 1.0\n",
      "Question 31, Context 0: 0.0\n",
      "Question 31, Context 1: 1.0\n",
      "Question 30, Context 2: 0.0\n",
      "Question 31, Context 1: 1.0\n",
      "Question 31, Context 2: 0.0\n",
      "Question 30, Context 3: 0.0\n",
      "Question 31, Context 2: 0.0\n",
      "Question 31, Context 3: 0.0\n",
      "Question 30, Context 4: 0.0\n",
      "Question 31, Context 4: 0.0\n",
      "Question 31, Context 3: 0.0\n",
      "Question 31, Context 4: 0.0\n",
      "Question 32, Context 0: 0.0\n",
      "Question 31, Context 0: 0.0\n",
      "Question 32, Context 1: 0.0\n",
      "Question 32, Context 0: 1.0\n",
      "Question 31, Context 1: 1.0\n",
      "Question 32, Context 1: 0.0\n",
      "Question 32, Context 2: 1.0\n",
      "Question 32, Context 3: 0.0\n",
      "Question 32, Context 2: 1.0\n",
      "Question 31, Context 2: 0.0\n",
      "Question 32, Context 4: 0.0\n",
      "Question 32, Context 3: 1.0\n",
      "Question 31, Context 3: 0.0\n",
      "Question 32, Context 4: 1.0\n",
      "Question 33, Context 0: 1.0\n",
      "Question 31, Context 4: 1.0\n",
      "Question 33, Context 1: 0.0\n",
      "Question 33, Context 0: 0.0\n",
      "Question 32, Context 0: 1.0\n",
      "Question 33, Context 2: 1.0\n",
      "Question 33, Context 1: 1.0\n",
      "Question 32, Context 1: 0.0\n",
      "Question 33, Context 3: 1.0\n",
      "Question 33, Context 2: 1.0\n",
      "Question 33, Context 4: 0.0\n",
      "Question 33, Context 3: 1.0\n",
      "Question 32, Context 2: 0.0\n",
      "Question 34, Context 0: 0.0\n",
      "Question 33, Context 4: 1.0\n",
      "Question 32, Context 3: 0.0\n",
      "Question 34, Context 1: 0.0\n",
      "Question 34, Context 0: 1.0\n",
      "Question 34, Context 2: 1.0\n",
      "Question 32, Context 4: 1.0\n",
      "Question 34, Context 1: 0.0\n",
      "Question 34, Context 3: 0.0\n",
      "Question 33, Context 0: 1.0\n",
      "Question 34, Context 2: 0.0\n",
      "Question 34, Context 4: 0.0\n",
      "Question 33, Context 1: 0.0\n",
      "Question 34, Context 3: 1.0\n",
      "Question 35, Context 0: 1.0\n",
      "Question 33, Context 2: 1.0\n",
      "Question 34, Context 4: 0.0\n",
      "Question 35, Context 1: 1.0\n",
      "Question 35, Context 0: 0.0\n",
      "Question 33, Context 3: 1.0\n",
      "Question 35, Context 1: 1.0\n",
      "Question 35, Context 2: 0.0\n",
      "Question 33, Context 4: 1.0\n",
      "Question 35, Context 2: 1.0\n",
      "Question 34, Context 0: 1.0\n",
      "Question 35, Context 3: 1.0\n",
      "Question 34, Context 1: 1.0\n",
      "Question 35, Context 3: 0.0\n",
      "Question 34, Context 2: 1.0\n",
      "Question 35, Context 4: 0.0\n",
      "Question 35, Context 4: 0.0\n",
      "Question 34, Context 3: 1.0\n",
      "Question 36, Context 0: 1.0\n",
      "Question 36, Context 0: 0.0\n",
      "Question 34, Context 4: 0.0\n",
      "Question 36, Context 1: 0.0\n",
      "Question 35, Context 0: 0.0\n",
      "Question 36, Context 1: 0.0\n",
      "Question 36, Context 2: 1.0\n",
      "Question 35, Context 1: 1.0\n",
      "Question 36, Context 2: 0.0\n",
      "Question 36, Context 3: 0.0\n",
      "Question 36, Context 3: 1.0\n",
      "Question 36, Context 4: 0.0\n",
      "Question 36, Context 4: 0.0\n",
      "Question 37, Context 0: 1.0\n",
      "Question 37, Context 1: 1.0\n",
      "Question 35, Context 2: 1.0\n",
      "Question 37, Context 2: 1.0\n",
      "Question 35, Context 3: 0.0\n",
      "Question 37, Context 0: 1.0\n",
      "Question 37, Context 3: 1.0\n",
      "Question 35, Context 4: 0.0\n",
      "Question 36, Context 0: 0.0\n",
      "Question 37, Context 4: 1.0\n",
      "Question 37, Context 1: 1.0\n",
      "Question 36, Context 1: 0.0\n",
      "Question 38, Context 0: 1.0\n",
      "Question 37, Context 2: 1.0\n",
      "Question 36, Context 2: 0.0\n",
      "Question 38, Context 1: 1.0\n",
      "Question 37, Context 3: 1.0\n",
      "Question 36, Context 3: 1.0\n",
      "Question 38, Context 2: 1.0\n",
      "Question 36, Context 4: 0.0\n",
      "Question 37, Context 0: 0.0\n",
      "Question 37, Context 1: 1.0\n",
      "Question 37, Context 2: 1.0\n",
      "Question 37, Context 4: 1.0\n",
      "Question 37, Context 3: 0.0\n",
      "Question 38, Context 0: 0.0\n",
      "Question 37, Context 4: 0.0\n",
      "Question 38, Context 1: 1.0\n",
      "Question 38, Context 0: 1.0\n",
      "Question 38, Context 2: 0.0\n",
      "Question 38, Context 1: 0.0\n",
      "Question 38, Context 3: 0.0\n",
      "Question 38, Context 2: 1.0\n",
      "Question 38, Context 4: 0.0\n",
      "Question 38, Context 3: 0.0\n",
      "Question 39, Context 0: 0.0\n",
      "Question 38, Context 4: 0.0\n",
      "Question 39, Context 0: 0.0\n",
      "Question 38, Context 3: 0.0\n",
      "Question 39, Context 1: 1.0\n",
      "Question 39, Context 1: 0.0\n",
      "Question 38, Context 4: 0.0\n",
      "Question 39, Context 2: 0.0\n",
      "Question 39, Context 2: 0.0\n",
      "Question 39, Context 0: 0.0\n",
      "Question 39, Context 3: 0.0\n",
      "Question 39, Context 3: 0.0\n",
      "Question 39, Context 1: 1.0\n",
      "Question 39, Context 4: 0.0\n",
      "Question 39, Context 4: 0.0\n",
      "Question 39, Context 2: 1.0\n",
      "Question 40, Context 0: 0.0\n",
      "Question 40, Context 0: 0.0\n",
      "Question 39, Context 3: 0.0\n",
      "Question 40, Context 1: 0.0\n",
      "Question 40, Context 1: 0.0\n",
      "Question 39, Context 4: 0.0\n",
      "Question 40, Context 2: 0.0\n",
      "Question 40, Context 2: 0.0\n",
      "Question 40, Context 0: 0.0\n",
      "Question 40, Context 3: 1.0\n",
      "Question 40, Context 3: 1.0\n",
      "Question 40, Context 1: 1.0\n",
      "Question 40, Context 4: 1.0\n",
      "Question 40, Context 4: 0.0\n",
      "Question 40, Context 2: 0.0\n",
      "Question 41, Context 0: 1.0\n",
      "Question 41, Context 0: 0.0\n",
      "Question 40, Context 3: 1.0\n",
      "Question 41, Context 1: 0.0\n",
      "Question 41, Context 1: 0.0\n",
      "Question 40, Context 4: 0.0\n",
      "Question 41, Context 2: 1.0\n",
      "Question 41, Context 2: 1.0\n",
      "Question 41, Context 0: 0.0\n",
      "Question 41, Context 3: 1.0\n",
      "Question 41, Context 3: 0.0\n",
      "Question 41, Context 1: 1.0\n",
      "Question 41, Context 4: 0.0\n",
      "Question 41, Context 4: 0.0\n",
      "Question 41, Context 2: 1.0\n",
      "Question 42, Context 0: 0.0\n",
      "Question 42, Context 0: 1.0\n",
      "Question 41, Context 3: 0.0\n",
      "Question 42, Context 1: 1.0\n",
      "Question 42, Context 1: 1.0\n",
      "Question 41, Context 4: 0.0\n",
      "Question 42, Context 2: 0.0\n",
      "Question 42, Context 2: 0.0\n",
      "Question 42, Context 3: 0.0\n",
      "Question 42, Context 0: 1.0\n",
      "Question 42, Context 3: 0.0\n",
      "Question 42, Context 1: 1.0Question 42, Context 4: 1.0\n",
      "\n",
      "Question 42, Context 4: 1.0\n",
      "Question 43, Context 0: 1.0\n",
      "Question 42, Context 2: 0.0\n",
      "Question 43, Context 0: 1.0\n",
      "Question 43, Context 1: 1.0\n",
      "Question 43, Context 1: 1.0\n",
      "Question 42, Context 3: 0.0\n",
      "Question 43, Context 2: 1.0\n",
      "Question 42, Context 4: 0.0\n",
      "Question 43, Context 2: 0.0\n",
      "Question 43, Context 3: 1.0\n",
      "Question 43, Context 3: 0.0\n",
      "Question 43, Context 4: 1.0\n",
      "Question 43, Context 4: 1.0\n",
      "Question 44, Context 0: 0.0\n",
      "Question 44, Context 0: 1.0\n",
      "Question 44, Context 1: 0.0\n",
      "Question 44, Context 1: 0.0\n",
      "Question 44, Context 2: 0.0\n",
      "Question 43, Context 0: 1.0\n",
      "Question 44, Context 2: 1.0\n",
      "Question 44, Context 3: 0.0\n",
      "Question 43, Context 1: 0.0\n",
      "Question 44, Context 3: 0.0\n",
      "Question 43, Context 2: 1.0\n",
      "Question 44, Context 4: 0.0\n",
      "Question 43, Context 3: 1.0\n",
      "Question 44, Context 4: 0.0\n",
      "Question 45, Context 0: 1.0\n",
      "Question 43, Context 4: 1.0\n",
      "Question 45, Context 1: 1.0\n",
      "Question 45, Context 0: 1.0\n",
      "Question 44, Context 0: 0.0\n",
      "Question 45, Context 2: 1.0\n",
      "Question 44, Context 1: 0.0\n",
      "Question 45, Context 3: 0.0\n",
      "Question 44, Context 2: 0.0\n",
      "Question 45, Context 4: 1.0\n",
      "Question 46, Context 0: 1.0\n",
      "Question 44, Context 3: 0.0\n",
      "Question 45, Context 1: 1.0\n",
      "Question 45, Context 2: 0.0\n",
      "Question 46, Context 1: 1.0\n",
      "Question 44, Context 4: 1.0\n",
      "Question 45, Context 3: 1.0\n",
      "Question 45, Context 0: 1.0\n",
      "Question 46, Context 2: 0.0\n",
      "Question 46, Context 3: 1.0\n",
      "Question 45, Context 1: 0.0\n",
      "Question 45, Context 4: 1.0\n",
      "Question 46, Context 4: 0.0\n",
      "Question 45, Context 2: 0.0\n",
      "Question 47, Context 0: 0.0\n",
      "Question 45, Context 3: 0.0\n",
      "Question 47, Context 1: 0.0\n",
      "Question 45, Context 4: 1.0\n",
      "Question 46, Context 0: 0.0\n",
      "Question 46, Context 0: 1.0\n",
      "Question 47, Context 2: 0.0\n",
      "Question 46, Context 1: 1.0\n",
      "Question 47, Context 3: 1.0\n",
      "Question 46, Context 1: 1.0\n",
      "Question 47, Context 4: 0.0\n",
      "Question 46, Context 2: 0.0\n",
      "Question 48, Context 0: 1.0\n",
      "Question 46, Context 3: 1.0\n",
      "Question 48, Context 1: 0.0\n",
      "Question 46, Context 4: 1.0\n",
      "Question 46, Context 2: 0.0\n",
      "Question 48, Context 2: 0.0\n",
      "Question 47, Context 0: 1.0\n",
      "Question 46, Context 3: 1.0\n",
      "Question 48, Context 3: 1.0\n",
      "Question 47, Context 1: 0.0\n",
      "Question 46, Context 4: 0.0\n",
      "Question 48, Context 4: 0.0\n",
      "Question 47, Context 2: 1.0\n",
      "Question 47, Context 0: 1.0\n",
      "Question 49, Context 0: 0.0\n",
      "Question 47, Context 3: 0.0\n",
      "Question 47, Context 1: 0.0\n",
      "Question 49, Context 1: 0.0\n",
      "Question 47, Context 4: 1.0\n",
      "Question 47, Context 2: 0.0\n",
      "Question 49, Context 2: 1.0\n",
      "Question 48, Context 0: 1.0\n",
      "Question 47, Context 3: 0.0\n",
      "Question 49, Context 3: 0.0\n",
      "Question 48, Context 1: 1.0\n",
      "Question 47, Context 4: 1.0\n",
      "Question 49, Context 4: 1.0\n",
      "Question 48, Context 2: 0.0\n",
      "Question 48, Context 0: 0.0\n",
      "Question 50, Context 0: 1.0\n",
      "Question 48, Context 3: 0.0\n",
      "Question 48, Context 1: 0.0\n",
      "Question 50, Context 1: 1.0\n",
      "Question 48, Context 4: 0.0\n",
      "Question 48, Context 2: 0.0\n",
      "Question 49, Context 0: 1.0\n",
      "Question 50, Context 2: 1.0\n",
      "Question 48, Context 3: 1.0\n",
      "Question 50, Context 3: 0.0\n",
      "Question 49, Context 1: 0.0\n",
      "Question 48, Context 4: 0.0\n",
      "Question 50, Context 4: 1.0\n",
      "Question 49, Context 2: 1.0\n",
      "Question 49, Context 0: 0.0\n",
      "Question 51, Context 0: 0.0\n",
      "Question 49, Context 3: 0.0\n",
      "Question 49, Context 1: 0.0\n",
      "Question 51, Context 1: 0.0\n",
      "Question 49, Context 4: 1.0\n",
      "Question 49, Context 2: 1.0\n",
      "Question 51, Context 2: 0.0\n",
      "Question 50, Context 0: 0.0\n",
      "Question 49, Context 3: 0.0\n",
      "Question 51, Context 3: 0.0\n",
      "Question 50, Context 1: 1.0\n",
      "Question 49, Context 4: 1.0\n",
      "Question 51, Context 4: 0.0\n",
      "Question 50, Context 2: 1.0\n",
      "Question 50, Context 0: 1.0\n",
      "Question 52, Context 0: 0.0\n",
      "Question 50, Context 3: 0.0\n",
      "Question 52, Context 1: 1.0\n",
      "Question 50, Context 1: 0.0\n",
      "Question 50, Context 4: 1.0\n",
      "Question 50, Context 2: 1.0Question 52, Context 2: 0.0\n",
      "\n",
      "Question 51, Context 0: 0.0\n",
      "Question 50, Context 3: 0.0\n",
      "Question 52, Context 3: 1.0\n",
      "Question 51, Context 1: 1.0\n",
      "Question 50, Context 4: 1.0\n",
      "Question 52, Context 4: 0.0\n",
      "Question 51, Context 2: 0.0\n",
      "Question 51, Context 0: 0.0\n",
      "Question 53, Context 0: 0.0\n",
      "Question 51, Context 3: 0.0\n",
      "Question 51, Context 1: 1.0\n",
      "Question 53, Context 1: 1.0\n",
      "Question 51, Context 4: 0.0\n",
      "Question 51, Context 2: 0.0\n",
      "Question 53, Context 2: 1.0Question 52, Context 0: 1.0\n",
      "\n",
      "Question 51, Context 3: 0.0\n",
      "Question 51, Context 4: 0.0\n",
      "Question 52, Context 1: 1.0\n",
      "Question 53, Context 3: 1.0\n",
      "Question 52, Context 2: 0.0\n",
      "Question 53, Context 4: 1.0\n",
      "Question 52, Context 3: 0.0\n",
      "Question 54, Context 0: 1.0\n",
      "Question 54, Context 1: 1.0\n",
      "Question 52, Context 4: 0.0\n",
      "Question 53, Context 0: 0.0\n",
      "Question 54, Context 2: 0.0\n",
      "Question 52, Context 0: 1.0\n",
      "Question 53, Context 1: 1.0\n",
      "Question 54, Context 3: 0.0\n",
      "Question 52, Context 1: 1.0\n",
      "Question 54, Context 4: 1.0\n",
      "Question 52, Context 2: 0.0\n",
      "Question 52, Context 3: 1.0\n",
      "Question 53, Context 2: 1.0\n",
      "Question 55, Context 0: 1.0\n",
      "Question 52, Context 4: 0.0\n",
      "Question 53, Context 3: 0.0\n",
      "Question 55, Context 1: 1.0\n",
      "Question 53, Context 0: 0.0Question 53, Context 4: 0.0\n",
      "\n",
      "Question 55, Context 2: 1.0\n",
      "Question 53, Context 1: 1.0\n",
      "Question 54, Context 0: 1.0\n",
      "Question 55, Context 3: 0.0\n",
      "Question 54, Context 1: 1.0\n",
      "Question 53, Context 2: 0.0\n",
      "Question 55, Context 4: 1.0\n",
      "Question 54, Context 2: 0.0\n",
      "Question 53, Context 3: 0.0\n",
      "Question 56, Context 0: 1.0\n",
      "Question 54, Context 3: 1.0\n",
      "Question 53, Context 4: 0.0\n",
      "Question 56, Context 1: 0.0\n",
      "Question 54, Context 4: 1.0\n",
      "Question 56, Context 2: 0.0\n",
      "Question 54, Context 0: 1.0\n",
      "Question 54, Context 1: 1.0\n",
      "Question 55, Context 0: 1.0\n",
      "Question 54, Context 2: 0.0\n",
      "Question 55, Context 1: 1.0\n",
      "Question 56, Context 3: 0.0\n",
      "Question 54, Context 3: 1.0\n",
      "Question 55, Context 2: 1.0\n",
      "Question 56, Context 4: 0.0\n",
      "Question 54, Context 4: 0.0\n",
      "Question 55, Context 3: 0.0\n",
      "Question 55, Context 0: 1.0\n",
      "Question 55, Context 4: 1.0\n",
      "Question 57, Context 0: 1.0\n",
      "Question 55, Context 1: 1.0\n",
      "Question 56, Context 0: 1.0\n",
      "Question 57, Context 1: 0.0\n",
      "Question 55, Context 2: 1.0\n",
      "Question 57, Context 2: 0.0\n",
      "Question 56, Context 1: 0.0\n",
      "Question 55, Context 3: 1.0\n",
      "Question 57, Context 3: 0.0\n",
      "Question 56, Context 2: 1.0\n",
      "Question 55, Context 4: 1.0\n",
      "Question 56, Context 3: 1.0\n",
      "Question 57, Context 4: 1.0\n",
      "Question 56, Context 4: 0.0\n",
      "Question 58, Context 0: 1.0\n",
      "Question 56, Context 0: 1.0\n",
      "Question 57, Context 0: 1.0\n",
      "Question 58, Context 1: 1.0\n",
      "Question 56, Context 1: 0.0\n",
      "Question 57, Context 1: 1.0\n",
      "Question 56, Context 2: 0.0\n",
      "Question 57, Context 2: 1.0\n",
      "Question 56, Context 3: 1.0\n",
      "Question 58, Context 2: 1.0\n",
      "Question 58, Context 3: 1.0\n",
      "Question 57, Context 3: 1.0\n",
      "Question 56, Context 4: 1.0\n",
      "Question 58, Context 4: 0.0\n",
      "Question 57, Context 4: 1.0\n",
      "Question 57, Context 0: 0.0\n",
      "Question 59, Context 0: 1.0\n",
      "Question 58, Context 0: 1.0\n",
      "Question 57, Context 1: 1.0\n",
      "Question 59, Context 1: 1.0\n",
      "Question 58, Context 1: 1.0\n",
      "Question 57, Context 2: 0.0\n",
      "Question 59, Context 2: 1.0\n",
      "Question 58, Context 2: 1.0\n",
      "Question 57, Context 3: 0.0\n",
      "Question 59, Context 3: 1.0\n",
      "Question 58, Context 3: 0.0\n",
      "Question 57, Context 4: 1.0\n",
      "Question 58, Context 4: 1.0\n",
      "Question 58, Context 0: 1.0\n",
      "Question 59, Context 0: 1.0\n",
      "Question 58, Context 1: 1.0\n",
      "Question 59, Context 4: 0.0\n",
      "Question 60, Context 0: 1.0\n",
      "Question 59, Context 1: 0.0\n",
      "Question 58, Context 2: 1.0\n",
      "Question 59, Context 2: 1.0\n",
      "Question 60, Context 1: 1.0\n",
      "Question 58, Context 3: 0.0\n",
      "Question 59, Context 3: 0.0\n",
      "Question 58, Context 4: 0.0\n",
      "Question 60, Context 2: 1.0\n",
      "Question 59, Context 4: 1.0\n",
      "Question 60, Context 3: 0.0\n",
      "Question 59, Context 0: 1.0\n",
      "Question 60, Context 0: 1.0\n",
      "Question 60, Context 4: 1.0\n",
      "Question 59, Context 1: 1.0\n",
      "Question 61, Context 0: 1.0\n",
      "Question 60, Context 1: 0.0\n",
      "Question 59, Context 2: 1.0\n",
      "Question 60, Context 2: 1.0\n",
      "Question 61, Context 1: 0.0\n",
      "Question 59, Context 3: 0.0\n",
      "Question 60, Context 3: 0.0\n",
      "Question 61, Context 2: 1.0\n",
      "Question 59, Context 4: 1.0\n",
      "Question 60, Context 4: 1.0\n",
      "Question 61, Context 3: 0.0\n",
      "Question 60, Context 0: 1.0\n",
      "Question 61, Context 4: 0.0\n",
      "Question 61, Context 0: 1.0\n",
      "Question 61, Context 1: 0.0\n",
      "Question 62, Context 0: 0.0\n",
      "Question 62, Context 1: 1.0\n",
      "Question 60, Context 1: 0.0\n",
      "Question 62, Context 2: 1.0\n",
      "Question 61, Context 2: 0.0\n",
      "Question 62, Context 3: 0.0\n",
      "Question 61, Context 3: 1.0\n",
      "Question 62, Context 4: 1.0\n",
      "Question 61, Context 4: 0.0\n",
      "Question 62, Context 0: 0.0\n",
      "Question 63, Context 0: 1.0\n",
      "Question 60, Context 2: 1.0\n",
      "Question 63, Context 1: 1.0\n",
      "Question 62, Context 1: 1.0\n",
      "Question 60, Context 3: 1.0\n",
      "Question 63, Context 2: 1.0\n",
      "Question 62, Context 2: 0.0\n",
      "Question 62, Context 3: 1.0\n",
      "Question 63, Context 3: 1.0\n",
      "Question 60, Context 4: 1.0\n",
      "Question 61, Context 0: 1.0\n",
      "Question 62, Context 4: 0.0\n",
      "Question 63, Context 4: 1.0\n",
      "Question 63, Context 0: 1.0\n",
      "Question 61, Context 1: 0.0\n",
      "Question 64, Context 0: 1.0\n",
      "Question 64, Context 1: 1.0\n",
      "Question 61, Context 2: 1.0\n",
      "Question 63, Context 1: 1.0\n",
      "Question 64, Context 2: 1.0\n",
      "Question 61, Context 3: 0.0\n",
      "Question 63, Context 2: 1.0\n",
      "Question 61, Context 4: 0.0\n",
      "Question 64, Context 3: 0.0\n",
      "Question 63, Context 3: 0.0\n",
      "Question 62, Context 0: 0.0\n",
      "Question 63, Context 4: 1.0\n",
      "Question 64, Context 4: 1.0\n",
      "Question 65, Context 0: 0.0\n",
      "Question 62, Context 1: 0.0\n",
      "Question 64, Context 0: 1.0\n",
      "Question 64, Context 1: 1.0\n",
      "Question 62, Context 2: 1.0\n",
      "Question 62, Context 3: 1.0\n",
      "Question 62, Context 4: 0.0Question 65, Context 1: 1.0\n",
      "\n",
      "Question 64, Context 2: 1.0\n",
      "Question 65, Context 2: 1.0\n",
      "Question 63, Context 0: 1.0\n",
      "Question 64, Context 3: 1.0\n",
      "Question 65, Context 3: 0.0\n",
      "Question 63, Context 1: 1.0\n",
      "Question 64, Context 4: 0.0\n",
      "Question 65, Context 4: 0.0\n",
      "Question 63, Context 2: 1.0\n",
      "Question 65, Context 0: 1.0\n",
      "Question 63, Context 3: 0.0\n",
      "Question 66, Context 0: 0.0\n",
      "Question 63, Context 4: 1.0\n",
      "Question 65, Context 1: 0.0\n",
      "Question 66, Context 1: 0.0\n",
      "Question 65, Context 2: 1.0\n",
      "Question 64, Context 0: 1.0\n",
      "Question 66, Context 2: 0.0\n",
      "Question 65, Context 3: 0.0\n",
      "Question 64, Context 1: 1.0\n",
      "Question 66, Context 3: 0.0\n",
      "Question 65, Context 4: 1.0\n",
      "Question 64, Context 2: 1.0\n",
      "Question 66, Context 4: 1.0\n",
      "Question 64, Context 3: 0.0\n",
      "Question 66, Context 0: 0.0\n",
      "Question 67, Context 0: 1.0\n",
      "Question 66, Context 1: 1.0\n",
      "Question 64, Context 4: 0.0\n",
      "Question 66, Context 2: 0.0\n",
      "Question 67, Context 1: 1.0\n",
      "Question 65, Context 0: 0.0\n",
      "Question 66, Context 3: 0.0\n",
      "Question 65, Context 1: 1.0\n",
      "Question 66, Context 4: 1.0\n",
      "Question 65, Context 2: 1.0\n",
      "Question 67, Context 0: 1.0\n",
      "Question 65, Context 3: 0.0\n",
      "Question 67, Context 1: 0.0\n",
      "Question 65, Context 4: 1.0\n",
      "Question 67, Context 2: 0.0\n",
      "Question 66, Context 0: 0.0\n",
      "Question 67, Context 3: 0.0\n",
      "Question 66, Context 1: 1.0\n",
      "Question 67, Context 4: 0.0\n",
      "Question 67, Context 2: 0.0\n",
      "Question 68, Context 0: 1.0\n",
      "Question 66, Context 2: 0.0\n",
      "Question 67, Context 3: 1.0\n",
      "Question 66, Context 3: 0.0\n",
      "Question 67, Context 4: 0.0\n",
      "Question 68, Context 0: 1.0\n",
      "Question 66, Context 4: 1.0\n",
      "Question 68, Context 1: 1.0\n",
      "Question 67, Context 0: 1.0\n",
      "Question 68, Context 2: 0.0\n",
      "Question 67, Context 1: 0.0\n",
      "Question 68, Context 3: 1.0\n",
      "Question 68, Context 1: 0.0\n",
      "Question 67, Context 2: 0.0\n",
      "Question 68, Context 4: 0.0\n",
      "Question 67, Context 3: 1.0\n",
      "Question 68, Context 2: 1.0\n",
      "Question 69, Context 0: 0.0\n",
      "Question 67, Context 4: 0.0\n",
      "Question 68, Context 3: 0.0\n",
      "Question 69, Context 1: 1.0\n",
      "Question 68, Context 0: 1.0\n",
      "Question 69, Context 2: 0.0\n",
      "Question 68, Context 1: 0.0\n",
      "Question 68, Context 4: 1.0\n",
      "Question 68, Context 2: 1.0\n",
      "Question 69, Context 3: 0.0\n",
      "Question 68, Context 3: 0.0\n",
      "Question 69, Context 0: 0.0\n",
      "Question 68, Context 4: 0.0\n",
      "Question 69, Context 1: 1.0\n",
      "Question 69, Context 0: 1.0\n",
      "Question 69, Context 2: 0.0\n",
      "Question 69, Context 1: 1.0\n",
      "Question 69, Context 4: 1.0\n",
      "Question 69, Context 3: 1.0\n",
      "Question 70, Context 0: 1.0\n",
      "Question 69, Context 2: 0.0\n",
      "Question 70, Context 1: 1.0\n",
      "Question 69, Context 4: 1.0\n",
      "Question 69, Context 3: 0.0\n",
      "Question 70, Context 2: 1.0\n",
      "Question 70, Context 0: 1.0\n",
      "Question 69, Context 4: 1.0\n",
      "Question 70, Context 1: 1.0\n",
      "Question 70, Context 0: 1.0\n",
      "Question 70, Context 2: 0.0\n",
      "Question 70, Context 1: 0.0\n",
      "Question 70, Context 3: 0.0\n",
      "Question 70, Context 2: 0.0\n",
      "Question 70, Context 4: 0.0\n",
      "Question 70, Context 3: 0.0\n",
      "Question 71, Context 0: 1.0\n",
      "Question 70, Context 4: 0.0\n",
      "Question 71, Context 1: 1.0\n",
      "Question 71, Context 0: 1.0\n",
      "Question 71, Context 2: 0.0\n",
      "Question 71, Context 3: 1.0\n",
      "Question 71, Context 1: 1.0\n",
      "Question 71, Context 4: 0.0\n",
      "Question 71, Context 2: 0.0\n",
      "Question 71, Context 3: 0.0\n",
      "Question 72, Context 0: 1.0\n",
      "Question 71, Context 4: 0.0\n",
      "Question 72, Context 1: 0.0\n",
      "Question 72, Context 0: 1.0\n",
      "Question 72, Context 2: 1.0\n",
      "Question 72, Context 1: 0.0\n",
      "Question 72, Context 3: 0.0\n",
      "Question 72, Context 2: 1.0\n",
      "Question 72, Context 3: 0.0\n",
      "Question 72, Context 4: 0.0\n",
      "Question 73, Context 0: 0.0\n",
      "Question 72, Context 4: 0.0\n",
      "Question 70, Context 3: 1.0\n",
      "Question 70, Context 4: 0.0\n",
      "Question 73, Context 1: 0.0\n",
      "Question 73, Context 0: 1.0\n",
      "Question 71, Context 0: 1.0\n",
      "Question 73, Context 2: 0.0\n",
      "Question 73, Context 1: 0.0\n",
      "Question 73, Context 3: 1.0\n",
      "Question 73, Context 2: 0.0\n",
      "Question 73, Context 4: 1.0\n",
      "Question 73, Context 3: 1.0\n",
      "Question 74, Context 0: 0.0\n",
      "Question 73, Context 4: 0.0\n",
      "Question 74, Context 0: 1.0\n",
      "Question 74, Context 1: 1.0\n",
      "Question 71, Context 1: 1.0\n",
      "Question 74, Context 1: 1.0\n",
      "Question 71, Context 2: 0.0\n",
      "Question 74, Context 2: 1.0\n",
      "Question 71, Context 3: 1.0\n",
      "Question 74, Context 3: 0.0\n",
      "Question 74, Context 2: 0.0\n",
      "Question 71, Context 4: 1.0\n",
      "Question 74, Context 4: 0.0\n",
      "Question 74, Context 3: 1.0\n",
      "Question 75, Context 0: 1.0\n",
      "Question 72, Context 0: 1.0\n",
      "Question 74, Context 4: 0.0\n",
      "Question 75, Context 1: 1.0\n",
      "Question 72, Context 1: 0.0\n",
      "Question 75, Context 2: 1.0\n",
      "Question 75, Context 0: 1.0\n",
      "Question 75, Context 3: 0.0\n",
      "Question 75, Context 1: 1.0\n",
      "Question 75, Context 4: 1.0\n",
      "Question 76, Context 0: 1.0\n",
      "Question 75, Context 2: 1.0\n",
      "Question 72, Context 2: 1.0\n",
      "Question 76, Context 1: 0.0\n",
      "Question 72, Context 3: 0.0\n",
      "Question 76, Context 2: 1.0\n",
      "Question 75, Context 3: 0.0\n",
      "Question 72, Context 4: 0.0\n",
      "Question 76, Context 3: 0.0\n",
      "Question 73, Context 0: 0.0\n",
      "Question 75, Context 4: 1.0\n",
      "Question 73, Context 1: 1.0\n",
      "Question 76, Context 0: 1.0\n",
      "Question 73, Context 2: 0.0\n",
      "Question 76, Context 1: 1.0\n",
      "Question 76, Context 4: 1.0\n",
      "Question 76, Context 2: 1.0\n",
      "Question 73, Context 3: 1.0\n",
      "Question 76, Context 3: 0.0\n",
      "Question 77, Context 0: 1.0\n",
      "Question 77, Context 1: 0.0\n",
      "Question 73, Context 4: 1.0\n",
      "Question 77, Context 2: 1.0\n",
      "Question 74, Context 0: 0.0\n",
      "Question 77, Context 3: 1.0\n",
      "Question 74, Context 1: 1.0\n",
      "Question 77, Context 4: 0.0\n",
      "Question 76, Context 4: 0.0\n",
      "Question 78, Context 0: 1.0\n",
      "Question 77, Context 0: 1.0\n",
      "Question 78, Context 1: 0.0\n",
      "Question 74, Context 2: 1.0\n",
      "Question 78, Context 2: 1.0\n",
      "Question 74, Context 3: 1.0\n",
      "Question 77, Context 1: 0.0\n",
      "Question 77, Context 2: 0.0\n",
      "Question 78, Context 3: 0.0\n",
      "Question 74, Context 4: 1.0\n",
      "Question 77, Context 3: 1.0\n",
      "Question 78, Context 4: 0.0\n",
      "Question 77, Context 4: 1.0\n",
      "Question 79, Context 0: 1.0\n",
      "Question 78, Context 0: 1.0\n",
      "Question 79, Context 1: 1.0\n",
      "Question 75, Context 0: 1.0\n",
      "Question 78, Context 1: 0.0\n",
      "Question 75, Context 1: 1.0\n",
      "Question 78, Context 2: 1.0\n",
      "Question 75, Context 2: 1.0\n",
      "Question 79, Context 2: 1.0\n",
      "Question 75, Context 3: 1.0\n",
      "Question 78, Context 3: 0.0\n",
      "Question 75, Context 4: 1.0\n",
      "Question 79, Context 3: 0.0\n",
      "Question 78, Context 4: 0.0\n",
      "Question 76, Context 0: 1.0\n",
      "Question 79, Context 4: 0.0\n",
      "Question 76, Context 1: 0.0\n",
      "Question 80, Context 0: 0.0\n",
      "Question 76, Context 2: 1.0\n",
      "Question 80, Context 1: 1.0\n",
      "Question 76, Context 3: 0.0\n",
      "Question 80, Context 2: 0.0\n",
      "Question 76, Context 4: 1.0\n",
      "Question 80, Context 3: 1.0\n",
      "Question 79, Context 0: 1.0\n",
      "Question 77, Context 0: 1.0\n",
      "Question 79, Context 1: 1.0\n",
      "Question 77, Context 1: 0.0\n",
      "Question 80, Context 4: 1.0\n",
      "Question 77, Context 2: 0.0\n",
      "Question 81, Context 0: 1.0\n",
      "Question 77, Context 3: 1.0\n",
      "Question 81, Context 1: 1.0\n",
      "Question 77, Context 4: 1.0\n",
      "Question 78, Context 0: 0.0\n",
      "Question 79, Context 2: 1.0\n",
      "Question 79, Context 3: 0.0\n",
      "Question 78, Context 1: 0.0\n",
      "Question 81, Context 2: 0.0\n",
      "Question 79, Context 4: 1.0\n",
      "Question 78, Context 2: 0.0\n",
      "Question 81, Context 3: 0.0\n",
      "Question 80, Context 0: 1.0\n",
      "Question 78, Context 3: 0.0\n",
      "Question 80, Context 1: 1.0\n",
      "Question 78, Context 4: 0.0\n",
      "Question 80, Context 2: 0.0\n",
      "Question 79, Context 0: 1.0\n",
      "Question 80, Context 3: 1.0\n",
      "Question 79, Context 1: 1.0\n",
      "Question 81, Context 4: 0.0\n",
      "Question 79, Context 2: 1.0\n",
      "Question 82, Context 0: 1.0\n",
      "Question 79, Context 3: 1.0\n",
      "Question 80, Context 4: 0.0\n",
      "Question 79, Context 4: 0.0\n",
      "Question 81, Context 0: 1.0\n",
      "Question 81, Context 1: 0.0\n",
      "Question 80, Context 0: 1.0\n",
      "Question 81, Context 2: 1.0\n",
      "Question 80, Context 1: 0.0\n",
      "Question 82, Context 1: 1.0\n",
      "Question 81, Context 3: 0.0\n",
      "Question 82, Context 2: 1.0\n",
      "Question 81, Context 4: 0.0\n",
      "Question 82, Context 3: 1.0\n",
      "Question 82, Context 0: 1.0\n",
      "Question 80, Context 2: 1.0\n",
      "Question 82, Context 1: 1.0\n",
      "Question 82, Context 4: 1.0\n",
      "Question 80, Context 3: 1.0\n",
      "Question 83, Context 0: 0.0\n",
      "Question 80, Context 4: 1.0\n",
      "Question 83, Context 1: 1.0\n",
      "Question 81, Context 0: 1.0\n",
      "Question 83, Context 2: 1.0\n",
      "Question 81, Context 1: 1.0\n",
      "Question 82, Context 2: 1.0\n",
      "Question 81, Context 2: 0.0\n",
      "Question 83, Context 3: 0.0\n",
      "Question 82, Context 3: 1.0\n",
      "Question 83, Context 4: 0.0\n",
      "Question 82, Context 4: 1.0\n",
      "Question 84, Context 0: 0.0\n",
      "Question 83, Context 0: 0.0\n",
      "Question 83, Context 1: 0.0\n",
      "Question 84, Context 1: 1.0\n",
      "Question 84, Context 2: 1.0\n",
      "Question 83, Context 2: 1.0\n",
      "Question 84, Context 3: 1.0\n",
      "Question 83, Context 3: 0.0\n",
      "Question 84, Context 4: 1.0\n",
      "Question 83, Context 4: 0.0\n",
      "Question 84, Context 0: 1.0\n",
      "Question 84, Context 1: 1.0\n",
      "Question 84, Context 2: 1.0\n",
      "Question 84, Context 3: 0.0\n",
      "Question 81, Context 3: 1.0\n",
      "Question 81, Context 4: 0.0\n",
      "Question 84, Context 4: 1.0\n",
      "Question 85, Context 0: 0.0\n",
      "Question 82, Context 0: 1.0\n",
      "Question 85, Context 0: 0.0\n",
      "Question 82, Context 1: 0.0\n",
      "Question 85, Context 1: 0.0\n",
      "Question 85, Context 1: 0.0\n",
      "Question 85, Context 2: 0.0Question 82, Context 2: 1.0\n",
      "\n",
      "Question 85, Context 2: 0.0\n",
      "Question 85, Context 3: 0.0\n",
      "Question 82, Context 3: 1.0\n",
      "Question 85, Context 3: 0.0\n",
      "Question 82, Context 4: 1.0\n",
      "Question 85, Context 4: 0.0\n",
      "Question 85, Context 4: 0.0\n",
      "Question 83, Context 0: 1.0\n",
      "Question 86, Context 0: 1.0\n",
      "Question 83, Context 1: 0.0\n",
      "Question 86, Context 1: 0.0\n",
      "Question 83, Context 2: 0.0\n",
      "Question 86, Context 2: 1.0\n",
      "Question 83, Context 3: 1.0\n",
      "Question 86, Context 0: 0.0\n",
      "Question 86, Context 3: 0.0\n",
      "Question 86, Context 1: 1.0\n",
      "Question 86, Context 4: 0.0\n",
      "Question 83, Context 4: 0.0\n",
      "Question 87, Context 0: 0.0\n",
      "Question 86, Context 2: 1.0\n",
      "Question 84, Context 0: 1.0\n",
      "Question 84, Context 1: 1.0\n",
      "Question 86, Context 3: 1.0\n",
      "Question 87, Context 1: 0.0\n",
      "Question 86, Context 4: 0.0\n",
      "Question 84, Context 2: 1.0\n",
      "Question 87, Context 2: 1.0\n",
      "Question 84, Context 3: 1.0\n",
      "Question 84, Context 4: 0.0\n",
      "Question 87, Context 3: 1.0\n",
      "Question 85, Context 0: 1.0Question 87, Context 4: 0.0\n",
      "\n",
      "Question 88, Context 0: 1.0\n",
      "Question 85, Context 1: 0.0\n",
      "Question 87, Context 0: 0.0\n",
      "Question 88, Context 1: 0.0\n",
      "Question 85, Context 2: 0.0\n",
      "Question 87, Context 1: 0.0\n",
      "Question 88, Context 2: 0.0\n",
      "Question 87, Context 2: 1.0\n",
      "Question 88, Context 3: 1.0\n",
      "Question 87, Context 3: 1.0\n",
      "Question 85, Context 3: 0.0\n",
      "Question 88, Context 4: 1.0\n",
      "Question 87, Context 4: 0.0\n",
      "Question 85, Context 4: 0.0\n",
      "Question 89, Context 0: 1.0\n",
      "Question 88, Context 0: 1.0\n",
      "Question 86, Context 0: 0.0\n",
      "Question 89, Context 1: 1.0\n",
      "Question 86, Context 1: 1.0\n",
      "Question 89, Context 2: 0.0\n",
      "Question 88, Context 1: 0.0\n",
      "Question 89, Context 3: 0.0\n",
      "Question 88, Context 2: 0.0\n",
      "Question 89, Context 4: 1.0\n",
      "Question 88, Context 3: 1.0\n",
      "Question 90, Context 0: 1.0\n",
      "Question 88, Context 4: 0.0\n",
      "Question 90, Context 1: 0.0\n",
      "Question 89, Context 0: 1.0\n",
      "Question 90, Context 2: 1.0\n",
      "Question 86, Context 2: 1.0\n",
      "Question 90, Context 3: 1.0\n",
      "Question 86, Context 3: 1.0\n",
      "Question 90, Context 4: 0.0\n",
      "Question 86, Context 4: 0.0\n",
      "Question 89, Context 1: 1.0\n",
      "Question 87, Context 0: 0.0\n",
      "Question 89, Context 2: 1.0\n",
      "Question 87, Context 1: 1.0\n",
      "Question 91, Context 0: 1.0\n",
      "Question 89, Context 3: 0.0\n",
      "Question 91, Context 1: 1.0\n",
      "Question 91, Context 2: 0.0\n",
      "Question 91, Context 3: 0.0\n",
      "Question 89, Context 4: 1.0\n",
      "Question 91, Context 4: 0.0\n",
      "Question 87, Context 2: 0.0\n",
      "Question 90, Context 0: 1.0\n",
      "Question 92, Context 0: 1.0\n",
      "Question 90, Context 1: 1.0\n",
      "Question 92, Context 1: 1.0\n",
      "Question 90, Context 2: 0.0\n",
      "Question 87, Context 3: 1.0\n",
      "Question 92, Context 2: 0.0\n",
      "Question 90, Context 3: 1.0\n",
      "Question 87, Context 4: 0.0\n",
      "Question 92, Context 3: 1.0\n",
      "Question 90, Context 4: 0.0\n",
      "Question 92, Context 4: 0.0\n",
      "Question 91, Context 0: 1.0\n",
      "Question 93, Context 0: 1.0\n",
      "Question 91, Context 1: 1.0\n",
      "Question 91, Context 2: 1.0\n",
      "Question 93, Context 1: 0.0\n",
      "Question 91, Context 3: 0.0\n",
      "Question 88, Context 0: 0.0\n",
      "Question 91, Context 4: 0.0\n",
      "Question 88, Context 1: 0.0\n",
      "Question 92, Context 0: 1.0\n",
      "Question 88, Context 2: 0.0\n",
      "Question 93, Context 2: 1.0\n",
      "Question 88, Context 3: 0.0\n",
      "Question 92, Context 1: 1.0\n",
      "Question 93, Context 3: 1.0\n",
      "Question 88, Context 4: 0.0\n",
      "Question 92, Context 2: 0.0\n",
      "Question 89, Context 0: 1.0\n",
      "Question 93, Context 4: 1.0\n",
      "Question 89, Context 1: 1.0\n",
      "Question 89, Context 2: 0.0\n",
      "Question 92, Context 3: 0.0\n",
      "Question 89, Context 3: 0.0\n",
      "Question 92, Context 4: 1.0\n",
      "Question 89, Context 4: 1.0\n",
      "Question 93, Context 0: 0.0\n",
      "Question 94, Context 0: 1.0\n",
      "Question 90, Context 0: 1.0\n",
      "Question 93, Context 1: 0.0\n",
      "Question 94, Context 1: 1.0\n",
      "Question 90, Context 1: 1.0\n",
      "Question 94, Context 2: 1.0\n",
      "Question 90, Context 2: 1.0\n",
      "Question 94, Context 3: 0.0\n",
      "Question 90, Context 3: 1.0\n",
      "Question 94, Context 4: 0.0\n",
      "Question 90, Context 4: 0.0\n",
      "Question 95, Context 0: 1.0\n",
      "Question 91, Context 0: 1.0\n",
      "Question 93, Context 2: 1.0\n",
      "Question 91, Context 1: 1.0\n",
      "Question 93, Context 3: 1.0\n",
      "Question 95, Context 1: 1.0\n",
      "Question 91, Context 2: 0.0\n",
      "Question 95, Context 2: 0.0\n",
      "Question 91, Context 3: 1.0\n",
      "Question 93, Context 4: 0.0\n",
      "Question 91, Context 4: 0.0\n",
      "Question 95, Context 3: 0.0\n",
      "Question 95, Context 4: 1.0\n",
      "Question 92, Context 0: 1.0\n",
      "Question 92, Context 1: 1.0\n",
      "Question 96, Context 0: 1.0\n",
      "Question 96, Context 1: 1.0\n",
      "Question 92, Context 2: 0.0\n",
      "Question 94, Context 0: 1.0\n",
      "Question 96, Context 2: 0.0\n",
      "Question 94, Context 1: 0.0\n",
      "Question 96, Context 3: 0.0\n",
      "Question 94, Context 2: 0.0\n",
      "Question 96, Context 4: 1.0\n",
      "Question 94, Context 3: 0.0\n",
      "Question 92, Context 3: 1.0\n",
      "Question 97, Context 0: 1.0\n",
      "Question 92, Context 4: 0.0\n",
      "Question 97, Context 1: 0.0\n",
      "Question 94, Context 4: 0.0\n",
      "Question 97, Context 2: 1.0\n",
      "Question 93, Context 0: 0.0\n",
      "Question 95, Context 0: 1.0\n",
      "Question 93, Context 1: 0.0\n",
      "Question 97, Context 3: 0.0\n",
      "Question 95, Context 1: 1.0\n",
      "Question 97, Context 4: 1.0\n",
      "Question 95, Context 2: 1.0\n",
      "Question 93, Context 2: 1.0\n",
      "Question 95, Context 3: 0.0\n",
      "Question 93, Context 3: 1.0\n",
      "Question 95, Context 4: 1.0\n",
      "Question 96, Context 0: 1.0\n",
      "Question 96, Context 1: 1.0\n",
      "Question 98, Context 0: 1.0\n",
      "Question 96, Context 2: 0.0\n",
      "Question 98, Context 1: 0.0\n",
      "Question 96, Context 3: 0.0\n",
      "Question 98, Context 2: 1.0\n",
      "Question 96, Context 4: 1.0\n",
      "Question 98, Context 3: 0.0\n",
      "Question 97, Context 0: 1.0\n",
      "Question 98, Context 4: 0.0\n",
      "Question 97, Context 1: 0.0\n",
      "Question 99, Context 0: 0.0\n",
      "Question 93, Context 4: 0.0\n",
      "Question 97, Context 2: 1.0\n",
      "Question 99, Context 1: 0.0\n",
      "Question 94, Context 0: 1.0\n",
      "Question 97, Context 3: 0.0\n",
      "Question 99, Context 2: 0.0\n",
      "Question 94, Context 1: 1.0\n",
      "Question 99, Context 3: 1.0\n",
      "Question 94, Context 2: 0.0\n",
      "Question 99, Context 4: 1.0\n",
      "Question 94, Context 3: 0.0\n",
      "Question 97, Context 4: 1.0\n",
      "Question 94, Context 4: 0.0\n",
      "Question 98, Context 0: 1.0\n",
      "Question 95, Context 0: 1.0\n",
      "Question 98, Context 1: 0.0\n",
      "Question 100, Context 0: 1.0\n",
      "Question 98, Context 2: 1.0\n",
      "Question 100, Context 1: 1.0\n",
      "Question 95, Context 1: 1.0\n",
      "Question 98, Context 3: 1.0\n",
      "Question 95, Context 2: 1.0\n",
      "Question 100, Context 2: 1.0\n",
      "Question 95, Context 3: 0.0\n",
      "Question 98, Context 4: 1.0\n",
      "Question 95, Context 4: 0.0\n",
      "Question 99, Context 0: 1.0\n",
      "Question 100, Context 3: 1.0\n",
      "Question 99, Context 1: 0.0\n",
      "Question 100, Context 4: 1.0\n",
      "Question 96, Context 0: 0.0\n",
      "Question 101, Context 0: 0.0\n",
      "Question 96, Context 1: 1.0\n",
      "Question 99, Context 2: 1.0\n",
      "Question 96, Context 2: 1.0\n",
      "Question 99, Context 3: 0.0\n",
      "Question 101, Context 1: 1.0\n",
      "Question 96, Context 3: 0.0\n",
      "Question 101, Context 2: 0.0\n",
      "Question 96, Context 4: 1.0\n",
      "Question 101, Context 3: 1.0\n",
      "Question 97, Context 0: 1.0\n",
      "Question 101, Context 4: 0.0\n",
      "Question 99, Context 4: 1.0\n",
      "Question 100, Context 0: 1.0\n",
      "Question 102, Context 0: 1.0\n",
      "Question 100, Context 1: 1.0\n",
      "Question 102, Context 1: 1.0\n",
      "Question 100, Context 2: 0.0\n",
      "Question 97, Context 1: 1.0\n",
      "Question 102, Context 2: 1.0\n",
      "Question 100, Context 3: 1.0\n",
      "Question 97, Context 2: 1.0\n",
      "Question 102, Context 3: 1.0\n",
      "Question 100, Context 4: 1.0\n",
      "Question 102, Context 4: 1.0\n",
      "Question 97, Context 3: 0.0\n",
      "Question 103, Context 0: 0.0\n",
      "Question 97, Context 4: 0.0\n",
      "Question 103, Context 1: 1.0\n",
      "Question 98, Context 0: 1.0\n",
      "Question 98, Context 1: 0.0\n",
      "Question 103, Context 2: 1.0\n",
      "Question 98, Context 2: 1.0\n",
      "Question 103, Context 3: 1.0\n",
      "Question 98, Context 3: 1.0\n",
      "Question 101, Context 0: 0.0\n",
      "Question 98, Context 4: 1.0\n",
      "Question 101, Context 1: 1.0\n",
      "Question 99, Context 0: 1.0\n",
      "Question 99, Context 1: 0.0\n",
      "Question 101, Context 2: 0.0\n",
      "Question 103, Context 4: 0.0\n",
      "Question 99, Context 2: 1.0\n",
      "Question 101, Context 3: 0.0\n",
      "Question 99, Context 3: 0.0\n",
      "Question 104, Context 0: 1.0\n",
      "Question 101, Context 4: 0.0\n",
      "Question 99, Context 4: 1.0\n",
      "Question 104, Context 1: 0.0\n",
      "Question 102, Context 0: 1.0\n",
      "Question 100, Context 0: 1.0\n",
      "Question 102, Context 1: 1.0\n",
      "Question 104, Context 2: 0.0\n",
      "Question 100, Context 1: 0.0\n",
      "Question 102, Context 2: 0.0\n",
      "Question 100, Context 2: 1.0\n",
      "Question 104, Context 3: 0.0\n",
      "Question 102, Context 3: 1.0\n",
      "Question 102, Context 4: 1.0\n",
      "Question 103, Context 0: 1.0\n",
      "Question 103, Context 1: 1.0\n",
      "Question 103, Context 2: 1.0\n",
      "Question 100, Context 3: 0.0\n",
      "Question 103, Context 3: 0.0\n",
      "Question 104, Context 4: 0.0\n",
      "Question 100, Context 4: 1.0\n",
      "Question 105, Context 0: 0.0\n",
      "Question 103, Context 4: 0.0\n",
      "Question 101, Context 0: 0.0\n",
      "Question 104, Context 0: 1.0\n",
      "Question 105, Context 1: 0.0\n",
      "Question 101, Context 1: 0.0\n",
      "Question 104, Context 1: 0.0\n",
      "Question 105, Context 2: 0.0\n",
      "Question 101, Context 2: 0.0\n",
      "Question 104, Context 2: 1.0\n",
      "Question 101, Context 3: 1.0\n",
      "Question 104, Context 3: 0.0\n",
      "Question 101, Context 4: 0.0\n",
      "Question 104, Context 4: 0.0\n",
      "Question 105, Context 3: 0.0\n",
      "Question 105, Context 0: 1.0\n",
      "Question 105, Context 4: 1.0\n",
      "Question 105, Context 1: 1.0\n",
      "Question 105, Context 2: 1.0\n",
      "Question 102, Context 0: 1.0Question 106, Context 0: 1.0\n",
      "\n",
      "Question 105, Context 3: 0.0\n",
      "Question 106, Context 1: 1.0\n",
      "Question 105, Context 4: 1.0\n",
      "Question 106, Context 0: 1.0\n",
      "Question 106, Context 2: 0.0\n",
      "Question 106, Context 3: 0.0\n",
      "Question 106, Context 1: 1.0\n",
      "Question 106, Context 4: 0.0\n",
      "[0.6355140186915887, 0.602803738317757, 0.5669781931464175, 0.5327102803738317, 0.5102803738317757]\n",
      "Question 102, Context 1: 1.0\n",
      "Question 106, Context 2: 1.0\n",
      "Question 102, Context 2: 0.0\n",
      "Question 106, Context 3: 0.0\n",
      "Question 102, Context 3: 0.0\n",
      "Question 106, Context 4: 0.0\n",
      "[0.719626168224299, 0.6542056074766355, 0.6105919003115265, 0.5560747663551402, 0.5289719626168223]\n",
      "Question 102, Context 4: 1.0\n",
      "Question 103, Context 0: 0.0\n",
      "Question 103, Context 1: 1.0\n",
      "Question 103, Context 2: 1.0\n",
      "Question 103, Context 3: 0.0\n",
      "Question 103, Context 4: 0.0\n",
      "Question 104, Context 0: 1.0\n",
      "Question 104, Context 1: 0.0\n",
      "Question 104, Context 2: 0.0\n",
      "Question 104, Context 3: 0.0\n",
      "Question 104, Context 4: 1.0\n",
      "Question 105, Context 0: 1.0\n",
      "Question 105, Context 1: 1.0\n",
      "Question 105, Context 2: 1.0\n",
      "Question 105, Context 3: 0.0\n",
      "Question 105, Context 4: 1.0\n",
      "Question 106, Context 0: 1.0\n",
      "Question 106, Context 1: 1.0\n",
      "Question 106, Context 2: 0.0\n",
      "Question 106, Context 3: 0.0\n",
      "Question 106, Context 4: 0.0\n",
      "[0.616822429906542, 0.6121495327102804, 0.5638629283489096, 0.5397196261682243, 0.5233644859813082]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "def print_retrieval_scores():\n",
    "    retrieval_precision_matrix = np.zeros((len(eval_data['questions']), 5))\n",
    "    for i, question in enumerate(eval_data['questions']):\n",
    "        for j, context in enumerate(results['documents'][i]):\n",
    "            retrieval_precision_matrix[i][j] = get_retrieval_precision_indicator(question, context, False)\n",
    "            print(f\"Question {i}, Context {j}: {retrieval_precision_matrix[i][j]}\")\n",
    "\n",
    "    precision_at_1 = np.mean(retrieval_precision_matrix[:, 0])\n",
    "    precision_at_2 = np.mean(np.sum(retrieval_precision_matrix[:, :2], axis=1) / 2)\n",
    "    precision_at_3 = np.mean(np.sum(retrieval_precision_matrix[:, :3], axis=1) / 3)\n",
    "    precision_at_4 = np.mean(np.sum(retrieval_precision_matrix[:, :4], axis=1) / 4)\n",
    "    precision_at_5 = np.mean(np.sum(retrieval_precision_matrix[:, :5], axis=1) / 5)\n",
    "\n",
    "    print(f\"[{precision_at_1}, {precision_at_2}, {precision_at_3}, {precision_at_4}, {precision_at_5}]\")\n",
    "\n",
    "# Create 4 threads to run the function in parallel\n",
    "threads = []\n",
    "for i in range(3):\n",
    "    t = threading.Thread(target=print_retrieval_scores)\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "# [0.6448598130841121, 0.6074766355140186, 0.557632398753894, 0.5537383177570093, 0.5457943925233645]\n",
    "# [0.7009345794392523, 0.6822429906542056, 0.6292834890965732, 0.5887850467289719, 0.5607476635514018]\n",
    "# [0.6635514018691588, 0.6308411214953271, 0.5825545171339565, 0.5514018691588785, 0.5308411214953271]\n",
    "# [0.6635514018691588, 0.6261682242990654, 0.5763239875389408, 0.5630841121495327, 0.5420560747663551]\n",
    "# [0.6635514018691588, 0.5981308411214953, 0.5669781931464173, 0.5373831775700935, 0.5289719626168224]\n",
    "# [0.616822429906542, 0.6121495327102804, 0.5638629283489096, 0.5397196261682243, 0.5233644859813082]\n",
    "# [0.719626168224299, 0.6542056074766355, 0.6105919003115265, 0.5560747663551402, 0.5289719626168223]\n",
    "# [0.6355140186915887, 0.602803738317757, 0.5669781931464175, 0.5327102803738317, 0.5102803738317757]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 1: 0.9252336448598131\n",
      "Precision at 2: 0.9158878504672897\n",
      "Precision at 3: 0.8909657320872275\n",
      "Precision at 4: 0.883177570093458\n",
      "Precision at 5: 0.8766355140186914\n",
      "[0.9252336448598131, 0.9158878504672897, 0.8909657320872275, 0.883177570093458, 0.8766355140186914]\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision at 1,2,3,4 and 5\n",
    "precision_at_1 = np.mean(retrieval_precision_matrix[:, 0])\n",
    "precision_at_2 = np.mean(np.sum(retrieval_precision_matrix[:, :2], axis=1) / 2)\n",
    "precision_at_3 = np.mean(np.sum(retrieval_precision_matrix[:, :3], axis=1) / 3)\n",
    "precision_at_4 = np.mean(np.sum(retrieval_precision_matrix[:, :4], axis=1) / 4)\n",
    "precision_at_5 = np.mean(np.sum(retrieval_precision_matrix[:, :5], axis=1) / 5)\n",
    "\n",
    "print(f\"Precision at 1: {precision_at_1}\")\n",
    "print(f\"Precision at 2: {precision_at_2}\")\n",
    "print(f\"Precision at 3: {precision_at_3}\")\n",
    "print(f\"Precision at 4: {precision_at_4}\")\n",
    "print(f\"Precision at 5: {precision_at_5}\")\n",
    "\n",
    "print(f\"[{precision_at_1}, {precision_at_2}, {precision_at_3}, {precision_at_4}, {precision_at_5}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TokenSplitter set up as ARAGOG paper\n",
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9158878504672897\n",
    "# Precision at 3: 0.8909657320872274\n",
    "# Precision at 4: 0.8878504672897196\n",
    "# Precision at 5: 0.8803738317757009\n",
    "# [0.9252336448598131, 0.9158878504672897, 0.8909657320872274, 0.8878504672897196, 0.8803738317757009]\n",
    "\n",
    "# Precision at 1: 0.9158878504672897\n",
    "# Precision at 2: 0.9158878504672897\n",
    "# Precision at 3: 0.8847352024922119\n",
    "# Precision at 4: 0.8785046728971962\n",
    "# Precision at 5: 0.8710280373831776\n",
    "# [0.9158878504672897, 0.9158878504672897, 0.8847352024922119, 0.8785046728971962, 0.8710280373831776]\n",
    "\n",
    "# Precision at 1: 0.9065420560747663\n",
    "# Precision at 2: 0.8925233644859814\n",
    "# Precision at 3: 0.881619937694704\n",
    "# Precision at 4: 0.8714953271028038\n",
    "# Precision at 5: 0.8710280373831776\n",
    "# [0.9065420560747663, 0.8925233644859814, 0.881619937694704, 0.8714953271028038, 0.8710280373831776]\n",
    "\n",
    "# trial_1 = [0.9252336448598131, 0.9158878504672897, 0.8909657320872274, 0.8878504672897196, 0.8803738317757009]\n",
    "# trial_2 = [0.9158878504672897, 0.9158878504672897, 0.8847352024922119, 0.8785046728971962, 0.8710280373831776]\n",
    "# trial_3 = [0.9065420560747663, 0.8925233644859814, 0.881619937694704, 0.8714953271028038, 0.8710280373831776]\n",
    "# trial_4 = [0.9065420560747663, 0.897196261682243, 0.8847352024922117, 0.8761682242990654, 0.8654205607476636]\n",
    "# trial_5 = [0.9345794392523364, 0.9345794392523364, 0.9127725856697819, 0.8925233644859814, 0.885981308411215]\n",
    "# trial_6 = [0.9065420560747663, 0.8925233644859814, 0.8722741433021808, 0.866822429906542, 0.8616822429906541]\n",
    "# trial_7 = [0.9065420560747663, 0.9065420560747663, 0.8909657320872274, 0.8785046728971962, 0.874766355140187]\n",
    "# trial_8 = [0.9252336448598131, 0.9158878504672897, 0.8909657320872275, 0.883177570093458, 0.8766355140186914]\n",
    "\n",
    "\n",
    "trial_1 = [0.6448598130841121, 0.6074766355140186, 0.557632398753894, 0.5537383177570093, 0.5457943925233645]\n",
    "trial_2 = [0.7009345794392523, 0.6822429906542056, 0.6292834890965732, 0.5887850467289719, 0.5607476635514018]\n",
    "trial_3 = [0.6635514018691588, 0.6308411214953271, 0.5825545171339565, 0.5514018691588785, 0.5308411214953271]\n",
    "trial_4 = [0.6635514018691588, 0.6261682242990654, 0.5763239875389408, 0.5630841121495327, 0.5420560747663551]\n",
    "trial_5 = [0.6635514018691588, 0.5981308411214953, 0.5669781931464173, 0.5373831775700935, 0.5289719626168224]\n",
    "trial_6 = [0.616822429906542, 0.6121495327102804, 0.5638629283489096, 0.5397196261682243, 0.5233644859813082]\n",
    "trial_7 = [0.719626168224299, 0.6542056074766355, 0.6105919003115265, 0.5560747663551402, 0.5289719626168223]\n",
    "trial_8 = [0.6355140186915887, 0.602803738317757, 0.5669781931464175, 0.5327102803738317, 0.5102803738317757]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a list of all trials\n",
    "all_trials = [trial_1, trial_2, trial_3, trial_4, trial_5, trial_6, trial_7, trial_8]\n",
    "\n",
    "# Transpose the list to get precision at each rank\n",
    "precision_at_each_rank = list(map(list, zip(*all_trials)))\n",
    "\n",
    "# Create labels\n",
    "labels = ['P@1', 'P@2', 'P@3', 'P@4', 'P@5']\n",
    "\n",
    "# Create the boxplot\n",
    "plt.boxplot(precision_at_each_rank, vert=False, labels=labels)\n",
    "plt.title('Boxplot of Precision@K for GPT-3.5-Turbo')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Rank')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9065420560747663\n",
    "# Precision at 3: 0.9034267912772584\n",
    "# Precision at 4: 0.8925233644859814\n",
    "# Precision at 5: 0.8766355140186917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Change 1: 2.06%\n",
      "Percentage Change 2: 1.04%\n",
      "Percentage Change 3: 1.75%\n",
      "Percentage Change 4: 2.14%\n",
      "Percentage Change 5: 1.96%\n"
     ]
    }
   ],
   "source": [
    "# TokenSplitter\n",
    "# Precision at 1: 0.9065420560747663\n",
    "# Precision at 2: 0.897196261682243\n",
    "# Precision at 3: 0.8878504672897196\n",
    "# Precision at 4: 0.8738317757009346\n",
    "# Precision at 5: 0.8598130841121495\n",
    "\n",
    "# RecursiveCharacterTextSplitter\n",
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9065420560747663\n",
    "# Precision at 3: 0.9034267912772584\n",
    "# Precision at 4: 0.8925233644859814\n",
    "# Precision at 5: 0.8766355140186917\n",
    "\n",
    "\n",
    "percentage_change_1 = ((0.9252336448598131 - 0.9065420560747663) / 0.9065420560747663) * 100\n",
    "percentage_change_2 = ((0.9065420560747663 - 0.897196261682243) / 0.897196261682243) * 100\n",
    "percentage_change_3 = ((0.9034267912772584 - 0.8878504672897196) / 0.8878504672897196) * 100\n",
    "percentage_change_4 = ((0.8925233644859814 - 0.8738317757009346) / 0.8738317757009346) * 100\n",
    "percentage_change_5 = ((0.8766355140186917 - 0.8598130841121495) / 0.8598130841121495) * 100\n",
    "\n",
    "print(f\"Percentage Change 1: {percentage_change_1:.2f}%\")\n",
    "print(f\"Percentage Change 2: {percentage_change_2:.2f}%\")\n",
    "print(f\"Percentage Change 3: {percentage_change_3:.2f}%\")\n",
    "print(f\"Percentage Change 4: {percentage_change_4:.2f}%\")\n",
    "print(f\"Percentage Change 5: {percentage_change_5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision at 1: 0.9065420560747663\n",
    "Precision at 2: 0.897196261682243\n",
    "Precision at 3: 0.8878504672897196\n",
    "Precision at 4: 0.8738317757009346\n",
    "Precision at 5: 0.8598130841121495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Test\n",
    "Why is there variability? Does this come from documents its uncertain of or is it just hallucinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-large\"\n",
    "            )\n",
    "# The db below has chunks of 512 tokens and overlap of 50. Same as ARAGOG's setup. \n",
    "collection = chroma_client.get_collection(\"chuck_8\", embedding_function=openai_ef)\n",
    "collection.count()\n",
    "\n",
    "import json\n",
    "with open('../eval_questions/eval_data.json') as f:\n",
    "    eval_data = json.load(f)\n",
    "questions = eval_data['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tokens_from_string()\n",
    "results = collection.query(query_texts=questions, n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0, Context 4: 1.0\n",
      "Question 0, Context 4: 1.0\n",
      "Question 0, Context 4: 1.0\n",
      "Question 0, Context 4: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 3, Context 4: 0.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 3, Context 4: 1.0\n",
      "Question 3, Context 4: 0.0\n",
      "Question 4, Context 4: 0.0\n",
      "Question 3, Context 4: 0.0\n",
      "Question 4, Context 4: 1.0\n",
      "Question 4, Context 4: 0.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 4, Context 4: 1.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 6, Context 4: 1.0\n",
      "Question 6, Context 4: 1.0\n",
      "Question 5, Context 4: 0.0\n",
      "Question 6, Context 4: 1.0\n",
      "Question 7, Context 4: 1.0\n",
      "Question 7, Context 4: 1.0\n",
      "Question 6, Context 4: 1.0\n",
      "Question 7, Context 4: 0.0\n",
      "Question 8, Context 4: 0.0\n",
      "Question 8, Context 4: 0.0\n",
      "Question 7, Context 4: 0.0\n",
      "Question 8, Context 4: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 9, Context 4: 1.0\n",
      "Question 8, Context 4: 0.0\n",
      "Question 10, Context 4: 1.0\n",
      "Question 10, Context 4: 0.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 10, Context 4: 1.0\n",
      "Question 11, Context 4: 1.0\n",
      "Question 11, Context 4: 1.0\n",
      "Question 10, Context 4: 1.0\n",
      "Question 11, Context 4: 0.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 11, Context 4: 0.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 13, Context 4: 1.0\n",
      "Question 13, Context 4: 1.0\n",
      "Question 12, Context 4: 0.0\n",
      "Question 13, Context 4: 1.0\n",
      "Question 14, Context 4: 1.0\n",
      "Question 14, Context 4: 1.0\n",
      "Question 13, Context 4: 1.0\n",
      "Question 14, Context 4: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 14, Context 4: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 17, Context 4: 1.0\n",
      "Question 17, Context 4: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 18, Context 4: 0.0\n",
      "Question 18, Context 4: 0.0\n",
      "Question 17, Context 4: 0.0\n",
      "Question 17, Context 4: 0.0\n",
      "Question 19, Context 4: 0.0\n",
      "Question 19, Context 4: 0.0\n",
      "Question 18, Context 4: 0.0\n"
     ]
    }
   ],
   "source": [
    "import backoff\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "@backoff.on_exception(backoff.expo,\n",
    "                      Exception,  # Broadly catching all exceptions\n",
    "                      max_tries=8)\n",
    "def get_retrieval_precision_indicator_backoff(question, context, four=True):\n",
    "    return get_retrieval_precision_indicator(question, context, four)\n",
    "\n",
    "def get_questions_block():\n",
    "    outcomes = np.zeros((len(questions), 5))\n",
    "    # for i, question in enumerate(questions):\n",
    "    for i, question in enumerate(questions):\n",
    "        for j in range(5):\n",
    "            outcomes[i][j] = get_retrieval_precision_indicator_backoff(question, results['documents'][i][0])\n",
    "        print(f\"Question {i}, Context {j}: {outcomes[i][j]}\")\n",
    "    return outcomes\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def execute_threads(num_tasks):\n",
    "    with ThreadPoolExecutor(max_workers=num_tasks) as executor:\n",
    "        futures = [executor.submit(get_questions_block) for _ in range(num_tasks)]\n",
    "        return [future.result() for future in futures]\n",
    "\n",
    "question_blocks = execute_threads(4)\n",
    "final_outcomes = np.concatenate(question_blocks, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outcomes = np.concatenate(question_blocks, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outcomes[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 1.  , 0.05, 1.  , 1.  , 1.  , 1.  , 1.  , 0.2 , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.2 , 1.  , 1.  , 0.65,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.8 , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 0.9 , 1.  , 1.  , 1.  , 1.  , 0.  , 1.  , 1.  , 1.  ,\n",
       "       0.15, 1.  , 1.  , 1.  , 1.  , 0.95, 1.  , 1.  , 0.95, 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 0.9 , 1.  , 1.  , 1.  , 1.  , 1.  , 0.95, 1.  , 1.  ,\n",
       "       1.  , 1.  , 0.45, 0.95, 0.2 , 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outcomes.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "Entropies of each row: [0.99277445 1.         0.93406806 0.93406806 0.97095059]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_entropy(data):\n",
    "    # Ensure data consists only of 0s and 1s\n",
    "    if not np.all((data == 0) | (data == 1)):\n",
    "        raise ValueError(\"Data must consist only of binary values (0 and 1).\")\n",
    "    \n",
    "    p = np.mean(data)  # Probability of 1\n",
    "    if p == 0 or p == 1:\n",
    "        return 0.  # Entropy is zero if all values are the same\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "    return entropy\n",
    "\n",
    "def calculate_entropies(data_matrix):\n",
    "    entropies = np.apply_along_axis(binary_entropy, 1, data_matrix)\n",
    "    return entropies\n",
    "\n",
    "# Example usage:\n",
    "data_matrix = np.random.randint(0, 2, (5, 20))  # Generate a random binary matrix of shape [5, 20]\n",
    "print(data_matrix.dtype)\n",
    "entropies = calculate_entropies(data_matrix)\n",
    "print(\"Entropies of each row:\", entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropies of each row: [0.         0.         0.         0.28639696 0.         0.\n",
      " 0.         0.         0.         0.72192809 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.72192809 0.\n",
      " 0.         0.93406806 0.         0.         0.         0.\n",
      " 0.         0.         0.72192809 0.         0.         0.\n",
      " 0.         0.         0.         0.46899559 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6098403  0.         0.         0.         0.         0.28639696\n",
      " 0.         0.         0.28639696 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.46899559 0.         0.         0.         0.         0.\n",
      " 0.28639696 0.         0.         0.         0.         0.99277445\n",
      " 0.28639696 0.72192809 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "entropies = calculate_entropies(final_outcomes)\n",
    "print(\"Entropies of each row:\", entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy value at index 101: 0.9927744539878083\n",
      "Question: What methodologies were employed in SuperGLUE to establish robust human baseline performances across its tasks?\n",
      "Context:  and COPA, human performance is perfect. On three other tasks, it is\n",
      "in the mid-to-high 90s. On the diagnostics, all models continue to lag signiﬁcantly behind humans.\n",
      "Though all models obtain near perfect gender parity scores on Winogender, this is due to the fact that\n",
      "they are obtaining accuracy near that of random guessing.\n",
      "6 Conclusion\n",
      "We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding\n",
      "systems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU\n",
      "tasks, as measured by the difference between human and machine baselines. The set of eight tasks in\n",
      "our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the\n",
      "tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\n",
      "We evaluate BERT-based baselines and ﬁnd that they still lag behind humans by nearly 20 points.\n",
      "Given the difﬁculty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer,\n",
      "and unsupervised/self-supervised learning techniques will be necessary to approach human-level per-\n",
      "formance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed\n",
      "for work developing new general-purpose machine learning methods for language understanding.\n",
      "7 Acknowledgments\n",
      "We thank the original authors of the included datasets in SuperGLUE for their cooperation in the\n",
      "creation of the benchmark, as well as those who proposed tasks and datasets that we ultimately\n",
      "could not include. This work was made possible in part by a donation to NYU from Eric and Wendy\n",
      "Schmidt made by recommendation of the Schmidt Futures program. We gratefully acknowledge\n",
      "the support of the NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this\n",
      "research, and funding from DeepMind for the hosting of the benchmark platform. AW is supported\n",
      "by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE\n",
      "1342536. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are\n",
      "those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.\n",
      "This project is partly supported by Samsung Advanced Institute of Technology (Next Generation\n",
      "Deep Learning: from Pattern Recognition to AI) and Samsung Electronics (Improving Deep Learning\n",
      "using Latent Structure).\n",
      "9\n",
      "References\n",
      "Stephen H. Bach, Daniel Rodriguez, Yintao\n",
      "Entropy value at index 43: 0.934068055375491\n",
      "Question: How does the performance of RoBERTa compare to BERT and other models on the GLUE benchmark, particularly in terms of state-of-the-art achievements?\n",
      "Context: .0 82.3 56.0 75.1\n",
      "BERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\n",
      "BERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\n",
      "Table 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\n",
      "The number below each task denotes the number of training examples. The “Average” column is slightly different\n",
      "than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\n",
      "model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\n",
      "accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n",
      "We use a batch size of 32 and ﬁne-tune for 3\n",
      "epochs over the data for all GLUE tasks. For each\n",
      "task, we selected the best ﬁne-tuning learning rate\n",
      "(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\n",
      "Additionally, for BERT LARGE we found that ﬁne-\n",
      "tuning was sometimes unstable on small datasets,\n",
      "so we ran several random restarts and selected the\n",
      "best model on the Dev set. With random restarts,\n",
      "we use the same pre-trained checkpoint but per-\n",
      "form different ﬁne-tuning data shufﬂing and clas-\n",
      "siﬁer layer initialization.9\n",
      "Results are presented in Table 1. Both\n",
      "BERT BASE and BERT LARGE outperform all sys-\n",
      "tems on all tasks by a substantial margin, obtaining\n",
      "4.5% and 7.0% respective average accuracy im-\n",
      "provement over the prior state of the art. Note that\n",
      "BERT BASE and OpenAI GPT are nearly identical\n",
      "in terms of model architecture apart from the at-\n",
      "tention masking. For the largest and most widely\n",
      "reported GLUE task, MNLI, BERT obtains a 4.6%\n",
      "absolute accuracy improvement. On the ofﬁcial\n",
      "GLUE leaderboard10, BERT\n",
      "Entropy value at index 40: 0.7219280948873623\n",
      "Question: What specific architectural changes were made to develop DistilBERT from BERT?\n",
      "Context:  needed to do a full pass on the STS-\n",
      "B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\n",
      "DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\n",
      "On device computation We studied whether DistilBERT could be used for on-the-edge applications\n",
      "by building a mobile application for question answering. We compare the average inference time on\n",
      "a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\n",
      "on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\n",
      "model weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n",
      "4.2 Ablation study\n",
      "In this section, we investigate the inﬂuence of various components of the triple loss and the student\n",
      "initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\n",
      "presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\n",
      "impact while the two distillation losses account for a large portion of the performance.\n",
      "5 Related work\n",
      "Task-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\n",
      "tups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\n",
      "Chatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\n",
      "ously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\n",
      "pre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use the original\n",
      "pretraining objective to train smaller student, then ﬁne-tuned via distillation. As shown in the abla-\n",
      "tion study, we found it beneﬁcial to leverage the teacher’s knowledge to pre-train with additional\n",
      "distillation signal.\n",
      "Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using\n",
      "multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation\n",
      "to learn a compact question answering model from a set of large question answering\n",
      "Entropy value at index 50: 0.7219280948873623\n",
      "Question: What findings does RoBERTa reveal about the efficacy of masked language model (MLM) pretraining under its optimized design choices?\n",
      "Context:  for pre-training\n",
      "and ﬁne-tuning large neural language models.\n",
      "\u000fWe conduct a comprehensive evaluation on a\n",
      "wide range of NLP tasks and assess the impact\n",
      "of adversarial training in pre-training from\n",
      "scratch, continual pre-training, task-speciﬁc\n",
      "ﬁne-tuning, and their combinations.\n",
      "\u000fWe obtain signiﬁcant improvements over prior\n",
      "state of the art, including extremely well-\n",
      "trained models such as RoBERTa, in both gen-\n",
      "eralization and robustness.\n",
      "\u000fTo facilitate research, we will release our code\n",
      "and pre-trained models.\n",
      "2 Preliminary\n",
      "In this section, we give a quick overview of lan-\n",
      "guage model pre-training, using BERT (Devlin\n",
      "et al., 2018) as a running example for transformer-\n",
      "based neural language models.\n",
      "2.1 Input Representation\n",
      "We assume that the input consists of text spans\n",
      "(typically sentences) separated by a special to-\n",
      "ken[SEP ]. To address the problem of out-of-\n",
      "vocabulary words, tokens are divided into subwordunits, using Byte-Pair Encoding (BPE) (Sennrich\n",
      "et al., 2015) or its variants (Kudo and Richardson,\n",
      "2018), which generates a ﬁxed-size subword vo-\n",
      "cabulary to compactly represent words in training\n",
      "text corpora.\n",
      "2.2 Model Architecture\n",
      "Following recent pre-training methods (Devlin\n",
      "et al., 2018; Liu et al., 2019c), we use transformer-\n",
      "based models (Vaswani et al., 2017) to lever-\n",
      "age a multi-head attention mechanism, which\n",
      "have demonstrated superiority in parallel computa-\n",
      "tion and modeling long-range dependencies, com-\n",
      "pared to recurrent neural networks such as LSTM\n",
      "(Hochreiter and Schmidhuber, 1997). The input is\n",
      "ﬁrst passed to a lexical encoder, which combines\n",
      "a token embedding, a (token) position embedding\n",
      "and a segment embedding (i.e., which text span the\n",
      "token belongs to) by element-wise summation. The\n",
      "embedding layer is then passed to multiple layers\n",
      "of transformer modules to generate the contextual\n",
      "representation (Vaswani et al., 2017).\n",
      "2.3 Self Supervision\n",
      "A key innovation in BERT (Devlin et al., 2018) is\n",
      "the use of Masked Language Model (MLM) for\n",
      "self-supervised pre-training. Instead of predicting\n",
      "the next token\n",
      "Entropy value at index 103: 0.7219280948873623\n",
      "Question: In the context of PAL, what role do meaningful variable names play in the generated program's effectiveness?\n",
      "Context:  weaker LMs? In all our experi-\n",
      "ments in Section 5, PALused thecode-davinci-002\n",
      "model; but can PALwork with weaker models of code? We\n",
      "compared PALwith COTwhen both prompting approaches\n",
      "use the same weaker base LMs code-cushman-001\n",
      "andcode-davinci-001 . As shown in Figure 7, even\n",
      "though the absolute accuracies of code-cushman-001\n",
      "andcode-davinci-001 are lower, the relative improve-\n",
      "ment of PALover COTremains consistent across models.\n",
      "This shows that PALcan work with weaker models, while\n",
      "its beneﬁt scales elegantly to stronger models as well.\n",
      "Does P AL work with LMs of natural language? We\n",
      "also experimented with PALusing thetext-davinci\n",
      "series. Figure 8 shows the following interesting re-\n",
      "sults: when the base LM’s “code modeling ability” is\n",
      "weak (using text-davinci-001 ),COTperforms better\n",
      "than PAL. However, once the LM’s code modeling abil-\n",
      "ity is sufﬁciently high (using text-davinci-002 and\n",
      "text-davinci-003 ),PALoutperforms COT, and PAL\n",
      "text-davinci-003 performs almost as PALcode-davinci-002 .\n",
      "This shows that PALis not limited to LMs of code, but it\n",
      "can work with LMs that were mainly trained for natural\n",
      "language, if they have a sufﬁciently high coding ability.\n",
      "Is P AL better because of the Python prompt or because\n",
      "of the interpreter? We experimented with generating\n",
      "Python code, while requiring the neural LM to “execute” it\n",
      "as well, without using an interpreter, following Nye et al.\n",
      "(2021); Madaan et al. (2022). We created prompts that are\n",
      "similar to PAL’s, except that they do include the ﬁnal answer.\n",
      "This resulted in a 23.2 solve rate on GSM 8K, much lower\n",
      "than PAL(72.0), and only 4.5 points higher than DIRECT .\n",
      "These results reinforce our hypothesis that the main beneﬁt\n",
      "ofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\n",
      "are provided in Appendix B. For additional discussion on\n",
      "the advantages of code-prompts over textual-prompts, see\n",
      "Appendix G.\n",
      "Do variable names matter? In all\n",
      "Entropy value at index 9: 0.7219280948873623\n",
      "Question: What detailed methodology does LLaMA utilize to ensure the diversity of its pre-training data, particularly in the context of filtering and language identification?\n",
      "Context: topquestionsandanswersfromcommunityforums,suchas\n",
      "Stack Exchange and wikiHow, sampling for quality and diversity. In addition, we manually write 250\n",
      "examplesofpromptsandresponses,whileoptimizingfortaskdiversityandemphasizingauniform\n",
      "responsestyleinthespiritofanAIassistant. Finally,wetrainLIMA,apretrained65B-parameter\n",
      "LLaMa model [Touvron et al., 2023] ﬁne-tuned on this set of 1,000 demonstrations.\n",
      "We compare LIMA to state-of-the-art language models and products across 300 challenging test\n",
      "prompts. Inahumanpreferencestudy,weﬁndthatLIMAoutperformsRLHF-trainedDaVinci003\n",
      "from OpenAI, which was trained with RLHF, as well as a 65B-parameter reproduction of Alpaca\n",
      "[Taori et al., 2023], which was trained on 52,000 examples. While humans typically prefer responses\n",
      "from GPT-4, Claude, and Bard over LIMA, this is not always the case; LIMA produces equal or\n",
      "preferrableresponsesin43%,46%,and58%ofthecases,respectively. Repeatingthehumanpreference\n",
      "annotationswithGPT-4astheannotatorcorroboratesourﬁndings. AnalyzingLIMAresponsesonan\n",
      "absolute scale reveals that 88% meet the prompt requirements, and 50% are considered excellent.\n",
      "Ablationexperimentsrevealvastlydiminishingreturnswhenscalingupdataquantitywithoutalso\n",
      "scaling up prompt diversity, alongside major gains when optimizing data quality. In addition, despite\n",
      "havingzerodialogueexamples,weﬁndthatLIMAcanconductcoherentmulti-turndialogue,and\n",
      "that this ability can be dramatically improved by adding only 30 hand-crafted dialogue chains to the\n",
      "training set. Overall, these remarkable ﬁndings demonstrate the power of pretraining and its relative\n",
      "importance over large-scale instruction tuning and reinforcement learning approaches.\n",
      "2 Alignment Data\n",
      "We deﬁne the Superﬁcial Alignment Hypothesis : A model’s knowledge and capabilities are learnt\n",
      "almostentirelyduringpretraining,whilealignmentteachesitwhichsubdistributionofformatsshould\n",
      "be used when interacting with users. If this hypothesis is correct, and alignment is largely about\n",
      "learning style, then a corollary of the Superﬁcial Alignment Hyp\n",
      "Entropy value at index 66: 0.6098403047164004\n",
      "Question: What record-setting performance did Megatron-LM achieve in terms of parameter count and sustained PetaFLOPs on NVIDIA V100 GPUs?\n",
      "Context:  It consists of two parts: Megatron-LM21(Shoeybi et al.,\n",
      "2019) provides the Transformer implementation, tensor parallelism, and data loading prim-\n",
      "itives, whereas DeepSpeed22(Rasley et al., 2020) provides the ZeRO optimizer, model\n",
      "pipelining, and general distributed training components. This framework allows us to train\n",
      "efficiently with 3D parallelism (Narayanan et al., 2021, shown in Figure 6), a fusion of three\n",
      "complementary approaches to distributed training. These approaches are described below:\n",
      "Figure 6: DP+PP+TP combination leads to 3D parallelism.\n",
      "Data parallelism (DP) replicates the model multiple times, with each replica placed on\n",
      "a different device and fed a slice of the data. The processing is done in parallel and\n",
      "all model replicas are synchronized at the end of each training step.\n",
      "Tensor parallelism (TP) partitions individual layers of the model across multiple de-\n",
      "vices. This way, instead of having the whole activation or gradient tensor reside on\n",
      "a single GPU, we place shards of this tensor on separate GPUs. This technique is\n",
      "sometimes called horizontal parallelism or intra-layer model parallelism.\n",
      "Pipeline parallelism (PP) splits up the model's layers across multiple GPUs, so that\n",
      "only a fraction of the layers of the model are placed on each GPU. This is sometimes\n",
      "called vertical parallelism.\n",
      "Finally, the Zero Redundancy Optimizer (ZeRO; Rajbhandari et al., 2020) allows different\n",
      "processes to only hold a fraction of data (parameters, gradients, and optimizer states)\n",
      "20.github.com/bigscience-workshop/Megatron-DeepSpeed\n",
      "21.github.com/NVIDIA/Megatron-LM\n",
      "22.github.com/microsoft/DeepSpeed\n",
      "19\n",
      "BigScience Workshop\n",
      "required for a training step. We used ZeRO stage 1, meaning that only the optimizer states\n",
      "are sharded in this manner.\n",
      "Thefourcomponentsdescribedabovearecombinedtogethertoallowscalingtohundreds\n",
      "of GPUs with extremely high GPU utilization. We were able to achieve 156 TFLOPs in\n",
      "our fastest configuration with A100 GPUs, attaining our objective of half of the theoretical\n",
      "peak performance of 312 TFLOPs (in float32 orbfloat16 ).\n",
      "3.4.3 Floating Point Format\n",
      "In earlier experiments with 104B-parameter models on NVIDIA V100 GPUs, we observed\n",
      "numerical instabilities\n",
      "Entropy value at index 90: 0.4689955935892811\n",
      "Question: How does Megatron-LM's implementation ensure training stability for extremely large transformer models?\n",
      "Context: \n",
      "checkpointing (Chen et al., 2016) after every transformer\n",
      "layer.\n",
      "For GPT-2 models, all training is performed with sequences\n",
      "of 1024 subword units at a batch size of 512 for 300k itera-\n",
      "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n",
      "tions. Our learning rate of 1.5e-4 utilizes a warmup period\n",
      "of 3k iterations before following a single cycle cosine decay\n",
      "over the remaining 297k iterations. We stop the decay at a\n",
      "minimum learning rate of 1e-5.\n",
      "For BERT models, we largely follow the training process\n",
      "described in (Lan et al., 2019). We use the original BERT\n",
      "dictionary with vocab size of 30,522. In addition, we re-\n",
      "place the next sentence prediction head with sentence order\n",
      "prediction as suggested by (Lan et al., 2019) and use whole\n",
      "word n-gram masking of (Joshi et al., 2019). For all cases,\n",
      "we set the batch size to 1024 and use a learning rate of 1.0e-\n",
      "4 warmed up over 10,000 iterations and decayed linearly\n",
      "over 2 million iterations. Other training parameters are kept\n",
      "the same as (Devlin et al., 2018).\n",
      "5. Experiments\n",
      "All of our experiments use up to 32 DGX-2H servers (a total\n",
      "of 512 Tesla V100 SXM3 32GB GPUs). Our infrastruc-\n",
      "ture is optimized for multi-node deep learning applications,\n",
      "with 300 GB/sec bandwidth between GPUs inside a server\n",
      "via NVSwitch and 100 GB/sec of interconnect bandwidth\n",
      "between servers using 8 InﬁniBand adapters per server.\n",
      "5.1. Scaling Analysis\n",
      "To test the scalability of our implementation, we consider\n",
      "GPT-2 models with four sets of parameters detailed in Table\n",
      "1. To have consistent GEMM sizes in the self attention layer,\n",
      "the hidden size per attention head is kept constant at 96\n",
      "while the number of heads and layers are varied to obtain\n",
      "conﬁgurations ranging from 1 billion to 8 billion parameters.\n",
      "The conﬁguration with 1.2 billion parameters ﬁts on a single\n",
      "GPU whereas the 8 billion parameter model requires 8-way\n",
      "model parallelism (8 GPUs). The original vocabulary size\n",
      "was\n",
      "Entropy value at index 57: 0.4689955935892811\n",
      "Question: What is the impact of removing the NSP loss on RoBERTa's performance across various benchmarks compared to BERT?\n",
      "Context:  in training the original BERT model.\n",
      "Devlin et al. (2019 ) observe that removing NSP\n",
      "hurts performance, with signiﬁcant performance\n",
      "degradation on QNLI, MNLI, and SQuAD 1.1.\n",
      "However, some recent work has questioned the\n",
      "necessity of the NSP loss ( Lample and Conneau ,\n",
      "2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\n",
      "To better understand this discrepancy, we com-\n",
      "pare several alternative training formats:\n",
      "•SEGMENT -PAIR +NSP: This follows the original\n",
      "input format used in BERT ( Devlin et al. ,2019 ),\n",
      "with the NSP loss. Each input has a pair of seg-\n",
      "ments, which can each contain multiple natural\n",
      "sentences, but the total combined length must\n",
      "be less than 512 tokens.\n",
      "Model SQuAD 1.1/2.0 MNLI-m SST-2 RACE\n",
      "Our reimplementation (with NSP loss):\n",
      "SEGMENT -PAIR 90.4/78.7 84.0 92.9 64.2\n",
      "SENTENCE -PAIR 88.7/76.2 82.9 92.1 63.0\n",
      "Our reimplementation (without NSP loss):\n",
      "FULL -SENTENCES 90.4/79.1 84.7 92.5 64.8\n",
      "DOC-SENTENCES 90.6/79.7 84.7 92.7 65.6\n",
      "BERT BASE 88.5/76.3 84.3 92.8 64.3\n",
      "XLNet BASE (K = 7) –/81.3 85.8 92.7 66.1\n",
      "XLNet BASE (K = 6) –/81.0 85.6 93.4 66.7\n",
      "Table 2: Development set results for base models pretrained over B OOK CORPUS and W IKIPEDIA . All models are\n",
      "trained for 1M steps with a batch size of 256 sequences. We rep ort F1 for SQuAD and accuracy for MNLI-m,\n",
      "SST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERT BASEand\n",
      "XLNet BASEare from Yang et al. (2019 ).\n",
      "•SENTENCE -PAIR +NSP:\n",
      "Entropy value at index 71: 0.2863969571159563\n",
      "Question: How does SuperGLUE's scoring system work, and what does it aim to achieve?\n",
      "Context: . (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n",
      "(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\n",
      "remains substantial scope for improvement towards GLUE’s high-level goals, the original version of\n",
      "the benchmark is no longer a suitable metric for quantifying such progress.\n",
      "In response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\n",
      "language understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\n",
      "simple, hard-to-game measure of progress toward general-purpose language understanding technolo-\n",
      "gies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\n",
      "innovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\n",
      "multitask, and unsupervised or self-supervised learning.\n",
      "SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\n",
      "eight language understanding tasks, drawing on existing data, accompanied by a single-number\n",
      "performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\n",
      "More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\n",
      "were identiﬁed from those submitted to an open call for task proposals and were selected based on\n",
      "difﬁculty for current NLP approaches.\n",
      "More diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\n",
      "classiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\n",
      "and question answering (QA).\n",
      "Comprehensive human baselines: We include human performance estimates for all benchmark\n",
      "tasks, which verify that substantial headroom exists between a strong BERT-based baseline and\n",
      "human performance.\n",
      "Improved code support: SuperGLUE is distributed with a new, modular toolkit for work on\n",
      "pretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\n",
      "PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\n",
      "Reﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\n",
      "revamped to ensure fair competition, an informative leaderboard, and full credit assignment to data\n",
      "Entropy value at index 74: 0.2863969571159563\n",
      "Question: Describe the computational approach to obtaining Task2Vec embeddings using a probe network.\n",
      "Context: \u000b(\u000b= 0:15when using a ResNet-34 pretrained\n",
      "on ImageNet as the probe network) is robust to the choice\n",
      "of meta-tasks.\n",
      "4.MODEL 2VEC: task/model co-embedding\n",
      "By construction, the TASK 2VEC distance ignores details\n",
      "of the model and only relies on the task. If we know what\n",
      "task a model was trained on, we can represent the model by\n",
      "the embedding of that task. However, in general we may\n",
      "not have such information ( e.g., black-box models or hand-\n",
      "constructed feature extractors). We may also have multiplemodels trained on the same task with different performance\n",
      "characteristics. To model the joint interaction between task\n",
      "and model ( i.e., architecture and training algorithm), we aim\n",
      "to learn a joint embedding of the two.\n",
      "We consider for concreteness the problem of learning\n",
      "a joint embedding for model selection. In order to em-\n",
      "bed models in the task space so that those near a task\n",
      "are likely to perform well on that task, we formulate the\n",
      "following meta-learning problem: Given kmodels, their\n",
      "MODEL 2VEC embedding are the vectors mi=Fi+bi,\n",
      "whereFiis the task embedding of the task used to train\n",
      "modelmi(if available, else we set it to zero), and biis a\n",
      "learned “model bias” that perturbs the task embedding to\n",
      "account for particularities of the model. We learn biby opti-\n",
      "mizing ak-way cross entropy loss to predict the best model\n",
      "given the task distance (see Supplementary Material):\n",
      "L=E[\u0000logp(mjdasym(t;m0);:::;d asym(t;mk))]:\n",
      "After training, given a novel query task t, we can then pre-\n",
      "dict the best model for it as the arg maxidasym(t;mi), that\n",
      "is, the model miembedded closest to the query task.\n",
      "5. Experiments\n",
      "We test TASK 2VEC on a large collection of tasks and\n",
      "models, related to different degrees. Our experiments aim to\n",
      "test both qualitative properties of the embedding and its per-\n",
      "formance on meta-learning tasks. We use an off-the-shelf\n",
      "ResNet-34 pretrained on ImageNet as our probe network,\n",
      "which we found to give the best overall performance (see\n",
      "Sect. 5.2). The collection of tasks is generated starting\n",
      "from the following four main datasets. iNaturalist [36]:\n",
      "Each task extracted corresponds to species class\n",
      "Entropy value at index 96: 0.2863969571159563\n",
      "Question: In what ways does GLM-130B's bilingual capability extend its application compared to monolingual models?\n",
      "Context: 32G) 0.31s 6.97s 0.67s 28.1s\n",
      "4\u0002RTX 3090 (24G) 0.37s 8.16s 1.30s 32.3s\n",
      "8\u0002RTX 2080 Ti (11G) 0.39s 6.77s 1.04s 27.3s\n",
      "5 T HERESULTS\n",
      "We follow the common settings in LLMs such as GPT-3 and PaLM to evaluate GLM-130B for\n",
      "English1. As a bilingual LLM with Chinese, GLM-130B is also evaluated on Chinese benchmarks.\n",
      "Discussion on the Scope of Zero-Shot Learning in GLM-130B. Since GLM-130B has been\n",
      "trained with MIP, here we clarify its scope of zero-shot evaluation. In fact, “zero-shot” seems to\n",
      "have controversial interpretations without a consensus in the community. We follow one of the in-\n",
      "ﬂuential related surveys (Xian et al., 2018), which says “At test time, in zero-shot learning setting,\n",
      "the aim is to assign a test image to an unseen class label” where involving unseen class labels is a\n",
      "key. Therefore, we derive our criterion to pick GLM-130B’s zero-shot (and few-shot) datasets as:\n",
      "•English : 1) For tasks with ﬁxed labels (e.g., natural language inference ): no datasets in such tasks\n",
      "should be evaluated on; 2) For tasks without ﬁxed labels (e.g., (multiple-choice) QA, topic classi-\n",
      "ﬁcation ): only datasets with an obvious domain transfer from those in MIP should be considered.\n",
      "•Chinese : All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.\n",
      "Filtering Test Datasets. Following prior practices (Brown et al., 2020; Rae et al., 2021) and our\n",
      "criterion mentioned above, we ﬁlter and refrain to report potentially contaminated datasets’ evalua-\n",
      "tion results. For LAMBADA and CLUE, we ﬁnd minimal overlap under the 13-gram setting. Pile,\n",
      "MMLU, and BIG-bench are either held-out or released later than the crawling of corpora.\n",
      "5.1 L ANGUAGE MODELING\n",
      "LAMBADA. LAMBADA (Paperno et al., \n",
      "Entropy value at index 102: 0.2863969571159563\n",
      "Question: How do the selected tasks in SuperGLUE reflect the benchmark's goals for advancing language understanding technologies?\n",
      "Context: SuperGLUE: A Stickier Benchmark for\n",
      "General-Purpose Language Understanding Systems\n",
      "Alex Wang\u0003\n",
      "New York UniversityYada Pruksachatkun\u0003\n",
      "New York UniversityNikita Nangia\u0003\n",
      "New York University\n",
      "Amanpreet Singh\u0003\n",
      "Facebook AI ResearchJulian Michael\n",
      "University of WashingtonFelix Hill\n",
      "DeepMindOmer Levy\n",
      "Facebook AI Research\n",
      "Samuel R. Bowman\n",
      "New York University\n",
      "Abstract\n",
      "In the last year, new models and methods for pretraining and transfer learning have\n",
      "driven striking performance improvements across a range of language understand-\n",
      "ing tasks. The GLUE benchmark, introduced a little over one year ago, offers\n",
      "a single-number metric that summarizes progress on a diverse set of such tasks,\n",
      "but performance on the benchmark has recently surpassed the level of non-expert\n",
      "humans, suggesting limited headroom for further research. In this paper we present\n",
      "SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\n",
      "cult language understanding tasks, a software toolkit, and a public leaderboard.\n",
      "SuperGLUE is available at super.gluebenchmark.com .\n",
      "1 Introduction\n",
      "Recently there has been notable progress across many natural language processing (NLP) tasks, led\n",
      "by methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n",
      "(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\n",
      "from massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\n",
      "The tasks that have proven amenable to this general approach include question answering, textual\n",
      "entailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\n",
      "In this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\n",
      "framework for research towards general-purpose language understanding technologies. GLUE is\n",
      "a collection of nine language understanding tasks built on existing public datasets, together with\n",
      "private test data, an evaluation server, a single-number target metric, and an accompanying expert-\n",
      "constructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\n",
      "understanding that covers a range of training data volumes, task genres, and task formulations. We\n",
      "believe it was these aspects that made GLUE particularly appropriate for exhibiting the transfer\n",
      "Entropy value at index 3: 0.28639695711595625\n",
      "Question: Can you describe the modifications LLaMA makes to the transformer architecture for improved performance?\n",
      "Context:  Transformer models\n",
      "More recently, the transformer architecture was developed (Vaswani et al., 2017). Transformers are a class\n",
      "of architectures that use a series of so-called transformer blocks comprising a self-attention layer followed by\n",
      "a feedforward layer, linked together with residual connections. The self-attention layer helps the model to\n",
      "considerneighbouringwordsintheinputasitprocessesaspeciﬁcword. Originally,thetransformerarchitecture\n",
      "was proposed for the task of machine translation (Vaswani et al., 2017). (Radford et al., 2018b) use a modiﬁed\n",
      "version applied to the task of language modeling (predicting the next word in a sentence). Subsequent work on\n",
      "LMs (Brown et al., 2020; Radford et al., 2018a) uses a similar architecture. An accessible visual introduction\n",
      "to the transformer architecture can be found in (Alammar, 2018). Recent language models built on the\n",
      "transformer architecture have been ﬁne-tuned directly, without the need for task-speciﬁc architectures (Devlin\n",
      "et al., 2019; Howard and Ruder, 2018; Radford et al., 2018a).\n",
      "1.2.3. “Large” Language Models\n",
      "TherecentupwindinLMresearchisrootedinthecapacitytoincreaseLMsizeintermsofnumberofparameters\n",
      "andsizeoftrainingdata(Benderetal.,2021). TrainingmodelsonextremelylargedatasetssuchastheColossal\n",
      "Clean Crawl Corpus (C4) (Raﬀel et al., 2020) and WebText (Radford et al., 2018b) resulted in sequence\n",
      "prediction systems with much more general applicability compared to the prior state-of-the-art (Brown et al.,\n",
      "2020; Fedus et al., 2021; Rosset, 2020). These models also displayed greater few-shot and zero-shot learning\n",
      "capabilities compared to smaller LMs (Brown et al., 2020). These properties were found to greatly simplify the\n",
      "development of task-speciﬁc LAs by reducing the adaptation process to prompt design (Zhang et al., 2021b).\n",
      "The insight that powerful sequence prediction systems could be created by scaling up the size of LMs and\n",
      "training corpora motivated an upsurge in interest and investment in LM research by several\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the entropies sorted in descending order\n",
    "indices_sorted_by_entropy = np.argsort(entropies)[::-1]\n",
    "# print(\"Indices of rows sorted by entropy:\", indices_sorted_by_entropy)\n",
    "\n",
    "for index in indices_sorted_by_entropy:\n",
    "    entropy_value = entropies[index]\n",
    "    if entropy_value == 0:\n",
    "        break\n",
    "    print(f\"Entropy value at index {index}: {entropy_value}\")\n",
    "    print(f\"Question: {questions[index]}\")\n",
    "    print(f\"Context: {results['documents'][index][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Conclusion\n",
    "Yes, uncertainty arises from specific queries and their contexts. These queries can either be difficult to understand or, even if clear, it may be challenging to determine whether the surrounding context aids in answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def call_gpt(question, context, four=True):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\" if four else \"gpt-3.5-turbo\",\n",
    "        # messages=[\n",
    "        #     {\"role\": \"system\", \"content\": get_retrieval_precision_prompt(question, context)},\n",
    "        #     {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "        # ]\n",
    "                messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond using markdown.\"},\n",
    "            {\"role\": \"user\", \"content\": get_retrieval_precision_prompt(question, context)}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content.lower().strip() == 'true'\n",
    "\n",
    "def get_retrieval_precision_indicator(question, context, four=True):\n",
    "    result1 = call_gpt(question, context, four)\n",
    "    if not result1:\n",
    "        return 0\n",
    "    result2 = call_gpt(question, context, four)\n",
    "    return 1 if result2 and result1 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-large\"\n",
    "            )\n",
    "\n",
    "collection = chroma_client.get_collection(\"chuck_8\", embedding_function=openai_ef)\n",
    "collection.count()\n",
    "\n",
    "import json\n",
    "with open('../eval_questions/eval_data.json') as f:\n",
    "    eval_data = json.load(f)\n",
    "questions = eval_data['questions']\n",
    "\n",
    "results = collection.query(query_texts=questions, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0\n",
      "Question 0\n",
      "Question 0\n",
      "Question 1\n",
      "Question 1\n",
      "Question 2\n",
      "Question 2\n",
      "Question 1\n",
      "Question 3\n",
      "Question 3\n",
      "Question 4\n",
      "Question 2\n",
      "Question 4\n",
      "Question 5\n",
      "Question 5\n",
      "Question 3\n",
      "Question 6\n",
      "Question 4\n",
      "Question 6\n",
      "Question 7\n",
      "Question 7\n",
      "Question 8\n",
      "Question 8\n",
      "Question 9\n",
      "Question 5\n",
      "Question 10\n",
      "Question 9\n",
      "Question 6\n",
      "Question 11\n",
      "Question 10\n",
      "Question 7\n",
      "Question 12\n",
      "Question 13\n",
      "Question 8\n",
      "Question 14\n",
      "Question 11\n",
      "Question 9\n",
      "Question 12\n",
      "Question 15\n",
      "Question 13\n",
      "Question 10\n",
      "Question 14\n",
      "Question 11\n",
      "Question 16\n",
      "Question 15\n",
      "Question 12\n",
      "Question 17\n",
      "Question 16\n",
      "Question 18\n",
      "Question 13\n",
      "Question 17\n",
      "Question 19\n",
      "Question 18\n",
      "Question 14\n",
      "Question 20\n",
      "Question 19\n",
      "Question 15\n",
      "Question 21\n",
      "Question 20\n",
      "Question 22\n",
      "Question 21\n",
      "Question 16\n",
      "Question 22\n",
      "Question 17\n",
      "Question 23\n",
      "Question 23\n",
      "Question 18\n",
      "Question 24\n",
      "Question 19\n",
      "Question 24\n",
      "Question 25\n",
      "Question 26\n",
      "Question 20\n",
      "Question 25\n",
      "Question 27\n",
      "Question 21\n",
      "Question 26\n",
      "Question 28\n",
      "Question 29\n",
      "Question 22\n",
      "Question 30\n",
      "Question 27\n",
      "Question 31\n",
      "Question 28\n",
      "Question 23\n",
      "Question 32\n",
      "Question 29\n",
      "Question 24\n",
      "Question 33\n",
      "Question 30\n",
      "Question 25\n",
      "Question 34\n",
      "Question 31\n",
      "Question 26\n",
      "Question 35\n",
      "Question 36\n",
      "Question 32\n",
      "Question 27\n",
      "Question 33\n",
      "Question 37\n",
      "Question 34\n",
      "Question 28\n",
      "Question 38\n",
      "Question 35\n",
      "Question 39\n",
      "Question 29\n",
      "Question 36\n",
      "Question 40\n",
      "Question 30\n",
      "Question 37\n",
      "Question 41\n",
      "Question 42\n",
      "Question 31\n",
      "Question 38\n",
      "Question 32\n",
      "Question 43\n",
      "Question 44\n",
      "Question 39\n",
      "Question 33\n",
      "Question 40\n",
      "Question 45\n",
      "Question 41\n",
      "Question 34\n",
      "Question 46\n",
      "Question 42\n",
      "Question 47\n",
      "Question 35\n",
      "Question 48\n",
      "Question 43\n",
      "Question 44\n",
      "Question 36\n",
      "Question 49\n",
      "Question 45\n",
      "Question 37\n",
      "Question 50\n",
      "Question 51\n",
      "Question 46\n",
      "Question 38\n",
      "Question 52\n",
      "Question 47\n",
      "Question 39\n",
      "Question 53\n",
      "Question 48\n",
      "Question 49\n",
      "Question 54\n",
      "Question 40\n",
      "Question 50\n",
      "Question 41\n",
      "Question 51\n",
      "Question 42\n",
      "Question 52\n",
      "Question 43\n",
      "Question 53\n",
      "Question 44\n",
      "Question 45\n",
      "Question 46Question 54\n",
      "\n",
      "Question 47\n",
      "Question 55\n",
      "Question 48\n",
      "Question 56\n",
      "Question 49\n",
      "Question 57\n",
      "Question 50\n",
      "Question 58\n",
      "Question 51\n",
      "Question 59\n",
      "Question 52\n",
      "Question 53\n",
      "Question 60\n",
      "Question 54\n",
      "Question 61\n",
      "Question 55\n",
      "Question 62\n",
      "Question 56\n",
      "Question 57\n",
      "Question 63\n",
      "Question 58\n",
      "Question 64\n",
      "Question 55\n",
      "Question 65\n",
      "Question 59\n",
      "Question 56\n",
      "Question 57\n",
      "Question 66\n",
      "Question 60\n",
      "Question 67\n",
      "Question 58\n",
      "Question 61\n",
      "Question 68\n",
      "Question 59\n",
      "Question 62\n",
      "Question 69\n",
      "Question 63\n",
      "Question 60\n",
      "Question 61\n",
      "Question 70\n",
      "Question 71\n",
      "Question 64\n",
      "Question 62\n",
      "Question 65\n",
      "Question 72\n",
      "Question 63\n",
      "Question 66\n",
      "Question 64\n",
      "Question 73\n",
      "Question 67\n",
      "Question 65\n",
      "Question 68\n",
      "Question 66\n",
      "Question 69\n",
      "Question 67\n",
      "Question 70\n",
      "Question 74\n",
      "Question 68\n",
      "Question 71\n",
      "Question 69\n",
      "Question 75\n",
      "Question 70\n",
      "Question 76\n",
      "Question 71\n",
      "Question 72\n",
      "Question 77\n",
      "Question 73\n",
      "Question 78\n",
      "Question 72\n",
      "Question 73\n",
      "Question 74\n",
      "Question 74\n",
      "Question 79\n",
      "Question 75\n",
      "Question 75\n",
      "Question 76\n",
      "Question 80\n",
      "Question 76\n",
      "Question 77\n",
      "Question 81\n",
      "Question 77\n",
      "Question 78\n",
      "Question 82\n",
      "Question 78\n",
      "Question 79\n",
      "Question 83\n",
      "Question 80\n",
      "Question 79\n",
      "Question 80\n",
      "Question 84\n",
      "Question 81\n",
      "Question 81\n",
      "Question 85\n",
      "Question 82\n",
      "Question 82\n",
      "Question 86\n",
      "Question 83\n",
      "Question 87\n",
      "Question 83\n",
      "Question 84\n",
      "Question 88\n",
      "Question 85\n",
      "Question 84\n",
      "Question 89\n",
      "Question 85\n",
      "Question 86\n",
      "Question 90\n",
      "Question 87\n",
      "Question 86\n",
      "Question 91\n",
      "Question 88\n",
      "Question 87\n",
      "Question 92\n",
      "Question 93\n",
      "Question 89\n",
      "Question 88\n",
      "Question 90\n",
      "Question 94\n",
      "Question 89\n",
      "Question 95\n",
      "Question 91\n",
      "Question 92Question 90\n",
      "\n",
      "Question 96\n",
      "Question 93\n",
      "Question 91\n",
      "Question 97\n",
      "Question 94\n",
      "Question 92\n",
      "Question 98\n",
      "Question 99\n",
      "Question 93\n",
      "Question 100\n",
      "Question 95\n",
      "Question 101\n",
      "Question 94\n",
      "Question 102\n",
      "Question 96\n",
      "Question 97\n",
      "Question 103\n",
      "Question 95\n",
      "Question 98\n",
      "Question 104\n",
      "Question 96\n",
      "Question 99\n",
      "Question 105\n",
      "Question 97\n",
      "Question 98\n",
      "Question 100\n",
      "Question 106\n",
      "Question 101\n",
      "Question 99\n",
      "Question 100\n",
      "Question 102\n",
      "Question 103\n",
      "Question 101\n",
      "Question 102\n",
      "Question 104\n",
      "Question 105\n",
      "Question 103\n",
      "Question 104\n",
      "Question 106\n",
      "Question 105\n",
      "Question 106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_score_column(column):\n",
    "    scores = []\n",
    "    for i, question in enumerate(questions):\n",
    "        context = results['documents'][i][column]\n",
    "        scores.append(get_retrieval_precision_indicator(question, context, False))\n",
    "        print(f\"Question {i}\")\n",
    "    return scores\n",
    "\n",
    "def get_scores_for_columns(columns):\n",
    "    with ThreadPoolExecutor(max_workers=len(columns)) as executor:\n",
    "        futures = [executor.submit(get_score_column, column) for column in columns]\n",
    "        return np.array([future.result() for future in futures]).T\n",
    "\n",
    "scores = get_scores_for_columns([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4143302180685358\n",
      "[0.5046729  0.39252336 0.34579439]\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean())\n",
    "print(scores.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 54\n",
      "Question 27\n",
      "Question 0\n",
      "Question 81\n",
      "Question 55\n",
      "Question 1\n",
      "Question 82\n",
      "Question 28\n",
      "Question 56\n",
      "Question 83\n",
      "Question 2\n",
      "Question 3\n",
      "Question 29\n",
      "Question 57\n",
      "Question 4\n",
      "Question 84\n",
      "Question 30\n",
      "Question 58\n",
      "Question 85\n",
      "Question 5\n",
      "Question 31\n",
      "Question 59\n",
      "Question 32\n",
      "Question 6\n",
      "Question 60\n",
      "Question 7\n",
      "Question 33\n",
      "Question 86\n",
      "Question 61\n",
      "Question 8\n",
      "Question 34\n",
      "Question 87\n",
      "Question 62\n",
      "Question 9\n",
      "Question 63\n",
      "Question 88\n",
      "Question 35\n",
      "Question 10\n",
      "Question 36\n",
      "Question 64\n",
      "Question 89\n",
      "Question 11\n",
      "Question 37\n",
      "Question 90\n",
      "Question 12\n",
      "Question 65\n",
      "Question 38\n",
      "Question 91\n",
      "Question 66\n",
      "Question 13\n",
      "Question 92\n",
      "Question 39\n",
      "Question 14\n",
      "Question 67\n",
      "Question 93\n",
      "Question 15\n",
      "Question 94\n",
      "Question 16\n",
      "Question 40\n",
      "Question 17\n",
      "Question 95\n",
      "Question 68\n",
      "Question 41\n",
      "Question 18\n",
      "Question 96\n",
      "Question 69\n",
      "Question 42\n",
      "Question 97\n",
      "Question 19\n",
      "Question 43\n",
      "Question 70\n",
      "Question 20\n",
      "Question 71\n",
      "Question 44\n",
      "Question 98\n",
      "Question 21\n",
      "Question 72\n",
      "Question 45\n",
      "Question 22\n",
      "Question 99\n",
      "Question 73\n",
      "Question 46\n",
      "Question 23\n",
      "Question 74\n",
      "Question 47\n",
      "Question 100\n",
      "Question 24\n",
      "Question 75\n",
      "Question 48\n",
      "Question 101\n",
      "Question 25\n",
      "Question 76\n",
      "Question 49\n",
      "Question 102\n",
      "Question 77\n",
      "Question 26\n",
      "Question 50\n",
      "Question 103\n",
      "Question 51\n",
      "Question 78\n",
      "Question 104\n",
      "Question 79\n",
      "Question 105\n",
      "Question 52\n",
      "Question 80\n",
      "Question 106\n",
      "Question 53\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_score(question_group):\n",
    "    scores = []\n",
    "    for i, question in question_group:\n",
    "        context = results['documents'][int(i)][0]\n",
    "        scores.append(get_retrieval_precision_indicator(question, context))\n",
    "        print(f\"Question {i}\")\n",
    "    return scores\n",
    "\n",
    "def get_scores(num_tasks):\n",
    "    # Split questions into groups for each thread\n",
    "    question_groups = np.array_split(list(enumerate(questions)), num_tasks)\n",
    "    # print(question_groups)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_tasks) as executor:\n",
    "        futures = [executor.submit(get_score, group) for group in question_groups]\n",
    "        # Flatten the list of scores from each thread\n",
    "        return np.concatenate([future.result() for future in futures])\n",
    "\n",
    "scores = get_scores(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065420560747663"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean() #0.916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does Claude Score Similarly to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='false', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_CHROMA_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "\n",
    "def call_gpt(question, context, four=True):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\" if four else \"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": get_retrieval_precision_prompt(question, context)},\n",
    "            {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content.lower().strip() == 'true'\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    # temperature=0.0,\n",
    "    system=get_retrieval_precision_prompt(question, context),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
