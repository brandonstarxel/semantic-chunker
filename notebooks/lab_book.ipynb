{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "The objectives of this week are to develop and test benchmarks. This will give us a baseline to compare our methods to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Find example pdf / text data.\n",
    "- Setup LangChain Recursive Text Splitter.\n",
    "- Setup fixed length token splitter.\n",
    "- Setup Chroma DB.\n",
    "- Create pipeline of: Text + Chunker -> Chroma Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Text Data\n",
    "To start off simple, I copied a recent news article from BBC about Effective Accelerationist, Grimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coachella: Grimes apologises for technical difficulties\n",
      "\n",
      "Mon 15 Apr\n",
      "\n",
      "BBC NEWS\n",
      "\n",
      "Grimes has apologised for \"major technical difficulties\" during her Coachella DJ set.\n",
      "\n",
      "Fans watched the singer scream in ...\n"
     ]
    }
   ],
   "source": [
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Test the function with the news.txt file\n",
    "news_data = read_txt_file('../data/news.txt')\n",
    "print(news_data[:200]+'...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain Recursive Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the Recursive Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "# Split the news_data using the splitter\n",
    "split_text = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain fixed length splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "\n",
    "texts = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"chuck_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Precision:\n",
    "In the ARAGOG paper they used Tonic Validate for this. If have taken Tonic's prompt so our implementation is identical (except we have the power to use models beyond GPT-3.5).\n",
    "Ref: https://github.com/TonicAI/tonic_validate/blob/main/tonic_validate/utils/llm_calls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_precision_prompt(question, context):\n",
    "    main_message = (\"Considering the following question and context, determine whether the context \"\n",
    "                    \"is relevant for answering the question. If the context is relevant for \"\n",
    "                    \"answering the question, respond with true. If the context is not relevant for \"\n",
    "                    \"answering the question, respond with false. Respond with either true or false \"\n",
    "                    \"and no additional text.\")\n",
    "\n",
    "    main_message += f\"\\nQUESTION: {question}\\n\"\n",
    "    main_message += f\"CONTEXT: {context}\\n\"\n",
    "\n",
    "    return main_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\n",
      "QUESTION: What is the capital of France?\n",
      "CONTEXT: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the function get_retrieval_precision_prompt\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\"\n",
    "\n",
    "print(get_retrieval_precision_prompt(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the realm of code, a method elegant and divine,\\nRecursion weaves a dance in loops, a concept quite refined.\\nAn echo of itself, the function calls its own embrace,\\nEach iteration a reflection, a cycle of poetic grace.\\n\\nLike a mirror reflecting endless images, recursive paths unfold,\\nEach repetition delving deeper, a story yet untold.\\nA self-referential loop, it spirals through the code's domain,\\nEmbracing complexity with a rhythm, a mesmerizing refrain.\\n\\nThrough recursive magic, problems are tackled with finesse,\\nBreaking them into smaller parts, a puzzle to address.\\nIn the dance of function calls, a symphony of logic rings,\\nSolving complex algorithms with recursive wings.\\n\\nSo heed the call of recursion, let it guide your coding quest,\\nIn its looping, self-referential beauty, let your code be blessed.\\nFor in the world of programming, where complexity may reign,\\nRecursion offers elegance, an eternal looping chain.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='*clears throat and speaks in a croaky voice* Hmm, well I am today, young Padawan. The Force, strong in me it flows. Yes, hmmm.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_CHROMA_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='true', type='text')]\n"
     ]
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=get_retrieval_precision_prompt(question, context),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the json file and read it\n",
    "with open('../eval_questions/eval_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data to verify it's been read correctly\n",
    "print(len(data['questions']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts:\n",
    "\n",
    "Currently using:\n",
    "ARAGOGs - Dataset (small, they added all other papers, arXiv papers)\n",
    "ARAGOGs - Questions (107 questions)\n",
    "Tonic Validate - Prompt for Retrieval Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Text to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../papers_for_questions/bert.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the Recursive Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1024,\n",
    "    # chunk_overlap=256,\n",
    ")\n",
    "\n",
    "# Split the news_data using the splitter\n",
    "split_text = splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_text[:5]\n",
    "\n",
    "# Count the number of tokens in each page_content\n",
    "import tiktoken\n",
    "\n",
    "# Count the number of tokens in each page_content\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# for page in split_text:\n",
    "#     print(num_tokens_from_string(page.page_content))\n",
    "    # print(page.page_content[:200]+'...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../papers_for_questions/bert.pdf', 'page': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).These approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_text[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "# collection = chroma_client.get_or_create_collection(name=\"chuck_1\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [chunk.page_content for chunk in split_text]\n",
    "metadatas = [chunk.metadata for chunk in split_text]\n",
    "ids = [str(i) for i in range(len(split_text))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['3', '20', '0', '6', '4']],\n",
       " 'distances': [[0.7218702708002468,\n",
       "   0.7597797400597308,\n",
       "   0.7711573382687963,\n",
       "   0.7771485522310896,\n",
       "   0.8290271472604436]],\n",
       " 'metadatas': [[{'page': 2, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 13, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 0, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 4, 'source': '../papers_for_questions/bert.pdf'},\n",
       "   {'page': 3, 'source': '../papers_for_questions/bert.pdf'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['BERT BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nQuestion Paragraph Start/End Span \\nBERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nMasked Sentence A Masked Sentence B \\nPre-training Fine-Tuning NSP Mask LM Mask LM \\nUnlabeled Sentence A and B Pair SQuAD \\nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning . Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\\n4We note that in the literature the bidirectional Trans-',\n",
       "   '•Learning rate (Adam) : 5e-5, 3e-5, 2e-5\\n•Number of epochs : 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are ﬁne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ( [SEP] ) and\\nclassiﬁer token ( [CLS] ) which are only in-\\ntroduced at ﬁne-tuning time; BERT learns\\n[SEP] ,[CLS] and sentence A/Bembed-\\ndings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n• GPT used the same learning rate of 5e-5 for\\nall ﬁne-tuning experiments; BERT chooses a\\ntask-speciﬁc ﬁne-tuning learning rate which\\nperforms the best on the development set.To isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of ﬁne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciﬁc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe ﬁgure,Erepresents the input embedding, Ti\\nrepresents the contextual representation of token i,\\n[CLS] is the special symbol for classiﬁcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised .\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiﬁ-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment ,contradiction , or\\nneutral with respect to the ﬁrst one.\\nQQP Quora Question Pairs is a binary classiﬁ-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiﬁcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct',\n",
       "   'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout }@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based andﬁne-tuning . The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning allpre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the maskedarXiv:1810.04805v2  [cs.CL]  24 May 2019',\n",
       "   '[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input \\nE[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token \\nEmbeddings \\nEA EB EB EB EB EB EA EA EA EA EASegment \\nEmbeddings \\nE0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position \\nEmbeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufﬂed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks—\\nwhether they involve single text or text pairs—by\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciﬁc inputs and outputs into BERT and ﬁne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence Aand sentence Bfrom pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and(4) a degenerate text- ∅pair in text classiﬁcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiﬁcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, ﬁne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7We de-\\nscribe the task-speciﬁc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT ﬁne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo ﬁne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the ﬁnal hid-\\nden vectorC∈RHcorresponding to the ﬁrst\\ninput token ( [CLS] ) as the aggregate representa-\\ntion. The only new parameters introduced during\\nﬁne-tuning are classiﬁcation layer weights W∈\\nRK×H, whereKis the number of labels. We com-\\npute a standard classiﬁcation loss with CandW,\\ni.e.,log(softmax( CWT)).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq .',\n",
       "   'Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g.,⟨Question, Answer⟩) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ( [SEP] ). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence Aor sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token asC∈RH,\\nand the ﬁnal hidden vector for the ithinput token\\nasTi∈RH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right orright-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.In order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nﬁne-tuning, since the [MASK] token does not ap-\\npear during ﬁne-tuning. To mitigate this, we do\\nnot always replace “masked” words with the ac-\\ntual[MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthei-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTiwill be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(query_texts=[\"What are the two main tasks BERT is pre-trained on?\"], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Issue:\n",
    "\n",
    "Retrieval Precision expects a COMPLETE RAG system and wants to measure the TOTAL number of returned contexts divided by relavent context. \n",
    "\n",
    "The issue with this is we'd ideally just retrieve N contexts always. The metric should not punish if the model returns all relavent context but there isn't much relavent context. Nor should it be rewarded if it only returns one relavent context but there's lots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Current tokens: 19559\n",
      "1 Current tokens: 12387\n",
      "2 Current tokens: 15654\n",
      "3 Current tokens: 21248\n",
      "4 Current tokens: 30842\n",
      "5 Current tokens: 17573\n",
      "6 Current tokens: 9560\n",
      "7 Current tokens: 24615\n",
      "8 Current tokens: 25285\n",
      "9 Current tokens: 15259\n",
      "10 Current tokens: 9842\n",
      "11 Current tokens: 58265\n",
      "12 Current tokens: 12804\n",
      "13 Current tokens: 58632\n",
      "14 Current tokens: 10976\n",
      "15 Current tokens: 14916\n",
      "16 Current tokens: 18802\n",
      "17 Current tokens: 17639\n",
      "18 Current tokens: 10244\n",
      "19 Current tokens: 11171\n",
      "20 Current tokens: 35282\n",
      "21 Current tokens: 30161\n",
      "22 Current tokens: 10952\n",
      "23 Current tokens: 173899\n",
      "24 Current tokens: 22685\n",
      "25 Current tokens: 46057\n",
      "26 Current tokens: 24993\n",
      "27 Current tokens: 25551\n",
      "28 Current tokens: 19021\n",
      "29 Current tokens: 24021\n",
      "30 Current tokens: 20091\n",
      "31 Current tokens: 17079\n",
      "32 Current tokens: 29179\n",
      "33 Current tokens: 21489\n",
      "34 Current tokens: 55100\n",
      "35 Current tokens: 9865\n",
      "36 Current tokens: 6815\n",
      "37 Current tokens: 5797\n",
      "38 Current tokens: 12755\n",
      "39 Current tokens: 10308\n",
      "40 Current tokens: 25661\n",
      "41 Current tokens: 10264\n",
      "42 Current tokens: 17921\n",
      "43 Current tokens: 97020\n",
      "44 Current tokens: 33483\n",
      "45 Current tokens: 12554\n",
      "46 Current tokens: 44544\n",
      "47 Current tokens: 11206\n",
      "48 Current tokens: 12914\n",
      "49 Current tokens: 17923\n",
      "50 Current tokens: 15530\n",
      "51 Current tokens: 23606\n",
      "52 Current tokens: 20202\n",
      "53 Current tokens: 21771\n",
      "54 Current tokens: 11626\n",
      "55 Current tokens: 205882\n",
      "56 Current tokens: 11273\n",
      "57 Current tokens: 25207\n",
      "58 Current tokens: 0\n",
      "59 Current tokens: 30975\n",
      "60 Current tokens: 11902\n",
      "61 Current tokens: 82374\n",
      "62 Current tokens: 40863\n",
      "63 Current tokens: 20994\n",
      "64 Current tokens: 45672\n",
      "65 Current tokens: 8763\n",
      "66 Current tokens: 30854\n",
      "67 Current tokens: 49161\n",
      "68 Current tokens: 37766\n",
      "69 Current tokens: 36633\n",
      "70 Current tokens: 35388\n",
      "71 Current tokens: 15215\n",
      "72 Current tokens: 14645\n",
      "73 Current tokens: 14714\n",
      "74 Current tokens: 31848\n",
      "75 Current tokens: 21528\n",
      "76 Current tokens: 36029\n",
      "77 Current tokens: 12767\n",
      "78 Current tokens: 18595\n",
      "79 Current tokens: 9874\n",
      "80 Current tokens: 16008\n",
      "81 Current tokens: 20360\n",
      "82 Current tokens: 39401\n",
      "83 Current tokens: 13187\n",
      "84 Current tokens: 18761\n",
      "85 Current tokens: 22880\n",
      "86 Current tokens: 16055\n",
      "87 Current tokens: 36818\n",
      "88 Current tokens: 7580\n",
      "89 Current tokens: 13265\n",
      "90 Current tokens: 41673\n",
      "91 Current tokens: 14288\n",
      "92 Current tokens: 50795\n",
      "93 Current tokens: 12738\n",
      "94 Current tokens: 20988\n",
      "95 Current tokens: 23167\n",
      "96 Current tokens: 36312\n",
      "97 Current tokens: 27498\n",
      "98 Current tokens: 16408\n",
      "99 Current tokens: 9686\n",
      "100 Current tokens: 36501\n",
      "101 Current tokens: 22854\n",
      "102 Current tokens: 15346\n",
      "103 Current tokens: 5975\n",
      "104 Current tokens: 19854\n",
      "105 Current tokens: 14591\n",
      "106 Current tokens: 21092\n",
      "107 Current tokens: 28209\n",
      "108 Current tokens: 16562\n",
      "109 Current tokens: 17947\n",
      "110 Current tokens: 11385\n",
      "111 Current tokens: 32191\n",
      "112 Current tokens: 2536\n",
      "113 Current tokens: 5759\n",
      "114 Current tokens: 36676\n",
      "115 Current tokens: 11865\n",
      "116 Current tokens: 30388\n",
      "117 Current tokens: 18665\n",
      "118 Current tokens: 20551\n",
      "119 Current tokens: 27988\n",
      "120 Current tokens: 24393\n",
      "121 Current tokens: 11049\n",
      "122 Current tokens: 11140\n",
      "123 Current tokens: 7205\n",
      "124 Current tokens: 40763\n",
      "125 Current tokens: 118945\n",
      "126 Current tokens: 16519\n",
      "127 Current tokens: 24115\n",
      "128 Current tokens: 18167\n",
      "129 Current tokens: 37570\n",
      "130 Current tokens: 4221\n",
      "131 Current tokens: 8861\n",
      "132 Current tokens: 11823\n",
      "133 Current tokens: 30163\n",
      "134 Current tokens: 64225\n",
      "135 Current tokens: 24013\n",
      "136 Current tokens: 14032\n",
      "137 Current tokens: 24899\n",
      "138 Current tokens: 0\n",
      "139 Current tokens: 8468\n",
      "140 Current tokens: 11306\n",
      "141 Current tokens: 11361\n",
      "142 Current tokens: 31339\n",
      "143 Current tokens: 31394\n",
      "144 Current tokens: 7394\n",
      "145 Current tokens: 5711\n",
      "146 Current tokens: 26640\n",
      "147 Current tokens: 9185\n",
      "148 Current tokens: 18042\n",
      "149 Current tokens: 11959\n",
      "150 Current tokens: 9315\n",
      "151 Current tokens: 46255\n",
      "152 Current tokens: 31166\n",
      "153 Current tokens: 13383\n",
      "154 Current tokens: 10905\n",
      "155 Current tokens: 25827\n",
      "156 Current tokens: 20461\n",
      "157 Current tokens: 9653\n",
      "158 Current tokens: 12013\n",
      "159 Current tokens: 19471\n",
      "160 Current tokens: 24288\n",
      "161 Current tokens: 15731\n",
      "162 Current tokens: 28463\n",
      "163 Current tokens: 19446\n",
      "164 Current tokens: 5204\n",
      "165 Current tokens: 26922\n",
      "166 Current tokens: 16155\n",
      "167 Current tokens: 21507\n",
      "168 Current tokens: 14651\n",
      "169 Current tokens: 52181\n",
      "170 Current tokens: 4997\n",
      "171 Current tokens: 26709\n",
      "172 Current tokens: 18754\n",
      "173 Current tokens: 10332\n",
      "174 Current tokens: 18887\n",
      "175 Current tokens: 17111\n",
      "176 Current tokens: 13681\n",
      "177 Current tokens: 9578\n",
      "178 Current tokens: 9761\n",
      "179 Current tokens: 14957\n",
      "180 Current tokens: 17241\n",
      "181 Current tokens: 17886\n",
      "182 Current tokens: 10056\n",
      "183 Current tokens: 30103\n",
      "184 Current tokens: 16117\n",
      "185 Current tokens: 37346\n",
      "186 Current tokens: 14333\n",
      "187 Current tokens: 9580\n",
      "188 Current tokens: 15702\n",
      "189 Current tokens: 53020\n",
      "190 Current tokens: 10845\n",
      "191 Current tokens: 14781\n",
      "192 Current tokens: 43349\n",
      "193 Current tokens: 16942\n",
      "194 Current tokens: 20406\n",
      "195 Current tokens: 25532\n",
      "196 Current tokens: 10531\n",
      "197 Current tokens: 12886\n",
      "198 Current tokens: 15042\n",
      "199 Current tokens: 13199\n",
      "200 Current tokens: 21720\n",
      "201 Current tokens: 22600\n",
      "202 Current tokens: 22495\n",
      "203 Current tokens: 27107\n",
      "204 Current tokens: 11625\n",
      "205 Current tokens: 9905\n",
      "206 Current tokens: 25987\n",
      "207 Current tokens: 10627\n",
      "208 Current tokens: 14306\n",
      "209 Current tokens: 0\n",
      "210 Current tokens: 11025\n",
      "211 Current tokens: 4768\n",
      "212 Current tokens: 31787\n",
      "213 Current tokens: 15936\n",
      "214 Current tokens: 15751\n",
      "215 Current tokens: 7373\n",
      "216 Current tokens: 0\n",
      "217 Current tokens: 10422\n",
      "218 Current tokens: 17622\n",
      "219 Current tokens: 17788\n",
      "220 Current tokens: 27068\n",
      "221 Current tokens: 26407\n",
      "222 Current tokens: 35508\n",
      "223 Current tokens: 36666\n",
      "224 Current tokens: 61251\n",
      "225 Current tokens: 97258\n",
      "226 Current tokens: 21838\n",
      "227 Current tokens: 23623\n",
      "228 Current tokens: 29806\n",
      "229 Current tokens: 7292\n",
      "230 Current tokens: 31581\n",
      "231 Current tokens: 43434\n",
      "232 Current tokens: 33885\n",
      "233 Current tokens: 16090\n",
      "234 Current tokens: 59016\n",
      "235 Current tokens: 22574\n",
      "236 Current tokens: 15510\n",
      "237 Current tokens: 36973\n",
      "238 Current tokens: 20642\n",
      "239 Current tokens: 25200\n",
      "240 Current tokens: 13372\n",
      "241 Current tokens: 18497\n",
      "242 Current tokens: 19220\n",
      "243 Current tokens: 20306\n",
      "244 Current tokens: 24581\n",
      "245 Current tokens: 21663\n",
      "246 Current tokens: 39737\n",
      "247 Current tokens: 11574\n",
      "248 Current tokens: 20446\n",
      "249 Current tokens: 53154\n",
      "250 Current tokens: 17907\n",
      "251 Current tokens: 12234\n",
      "252 Current tokens: 13865\n",
      "253 Current tokens: 15579\n",
      "254 Current tokens: 8005\n",
      "255 Current tokens: 15754\n",
      "256 Current tokens: 37309\n",
      "257 Current tokens: 34823\n",
      "258 Current tokens: 23963\n",
      "259 Current tokens: 32256\n",
      "260 Current tokens: 8722\n",
      "261 Current tokens: 17860\n",
      "262 Current tokens: 14453\n",
      "263 Current tokens: 6043\n",
      "264 Current tokens: 20254\n",
      "265 Current tokens: 18671\n",
      "266 Current tokens: 24148\n",
      "267 Current tokens: 4205\n",
      "268 Current tokens: 31489\n",
      "269 Current tokens: 17232\n",
      "270 Current tokens: 10693\n",
      "271 Current tokens: 20993\n",
      "272 Current tokens: 13065\n",
      "273 Current tokens: 26663\n",
      "274 Current tokens: 10490\n",
      "275 Current tokens: 11995\n",
      "276 Current tokens: 19222\n",
      "277 Current tokens: 27483\n",
      "278 Current tokens: 58408\n",
      "279 Current tokens: 6329\n",
      "280 Current tokens: 6075\n",
      "281 Current tokens: 14438\n",
      "282 Current tokens: 66267\n",
      "283 Current tokens: 20527\n",
      "284 Current tokens: 25150\n",
      "285 Current tokens: 12586\n",
      "286 Current tokens: 11538\n",
      "287 Current tokens: 20998\n",
      "288 Current tokens: 58467\n",
      "289 Current tokens: 18310\n",
      "290 Current tokens: 12882\n",
      "291 Current tokens: 17905\n",
      "292 Current tokens: 29235\n",
      "293 Current tokens: 26076\n",
      "294 Current tokens: 19740\n",
      "295 Current tokens: 10713\n",
      "296 Current tokens: 5032\n",
      "297 Current tokens: 20513\n",
      "298 Current tokens: 19572\n",
      "299 Current tokens: 17966\n",
      "300 Current tokens: 12873\n",
      "301 Current tokens: 51757\n",
      "302 Current tokens: 12367\n",
      "303 Current tokens: 12631\n",
      "304 Current tokens: 14357\n",
      "305 Current tokens: 6743\n",
      "306 Current tokens: 8030\n",
      "307 Current tokens: 19234\n",
      "308 Current tokens: 8776\n",
      "309 Current tokens: 17927\n",
      "310 Current tokens: 9253\n",
      "311 Current tokens: 6839\n",
      "312 Current tokens: 9623\n",
      "313 Current tokens: 17984\n",
      "314 Current tokens: 11724\n",
      "315 Current tokens: 5397\n",
      "316 Current tokens: 4899\n",
      "317 Current tokens: 15684\n",
      "318 Current tokens: 10339\n",
      "319 Current tokens: 29964\n",
      "320 Current tokens: 27633\n",
      "321 Current tokens: 14855\n",
      "322 Current tokens: 18973\n",
      "323 Current tokens: 7882\n",
      "324 Current tokens: 9783\n",
      "325 Current tokens: 17455\n",
      "326 Current tokens: 9911\n",
      "327 Current tokens: 38612\n",
      "328 Current tokens: 16583\n",
      "329 Current tokens: 41052\n",
      "330 Current tokens: 20366\n",
      "331 Current tokens: 25752\n",
      "332 Current tokens: 11057\n",
      "333 Current tokens: 12749\n",
      "334 Current tokens: 84829\n",
      "335 Current tokens: 10632\n",
      "336 Current tokens: 20173\n",
      "337 Current tokens: 12747\n",
      "338 Current tokens: 26900\n",
      "339 Current tokens: 10282\n",
      "340 Current tokens: 12527\n",
      "341 Current tokens: 6150\n",
      "342 Current tokens: 15593\n",
      "343 Current tokens: 39079\n",
      "344 Current tokens: 11917\n",
      "345 Current tokens: 10180\n",
      "346 Current tokens: 33256\n",
      "347 Current tokens: 16702\n",
      "348 Current tokens: 20322\n",
      "349 Current tokens: 11747\n",
      "350 Current tokens: 17868\n",
      "351 Current tokens: 31020\n",
      "352 Current tokens: 13191\n",
      "353 Current tokens: 11172\n",
      "354 Current tokens: 15469\n",
      "355 Current tokens: 19457\n",
      "356 Current tokens: 18071\n",
      "357 Current tokens: 21320\n",
      "358 Current tokens: 29212\n",
      "359 Current tokens: 10152\n",
      "360 Current tokens: 6513\n",
      "361 Current tokens: 7147\n",
      "362 Current tokens: 26581\n",
      "363 Current tokens: 39023\n",
      "364 Current tokens: 14562\n",
      "365 Current tokens: 29952\n",
      "366 Current tokens: 22094\n",
      "367 Current tokens: 23671\n",
      "368 Current tokens: 716981\n",
      "369 Current tokens: 11140\n",
      "370 Current tokens: 12374\n",
      "371 Current tokens: 10941\n",
      "372 Current tokens: 18893\n",
      "373 Current tokens: 10754\n",
      "374 Current tokens: 7283\n",
      "375 Current tokens: 22884\n",
      "376 Current tokens: 12471\n",
      "377 Current tokens: 23061\n",
      "378 Current tokens: 11224\n",
      "379 Current tokens: 19973\n",
      "380 Current tokens: 6207\n",
      "381 Current tokens: 12924\n",
      "382 Current tokens: 25958\n",
      "383 Current tokens: 13691\n",
      "384 Current tokens: 16300\n",
      "385 Current tokens: 28190\n",
      "386 Current tokens: 41741\n",
      "387 Current tokens: 34003\n",
      "388 Current tokens: 36028\n",
      "389 Current tokens: 13895\n",
      "390 Current tokens: 22789\n",
      "391 Current tokens: 40899\n",
      "392 Current tokens: 10608\n",
      "393 Current tokens: 22649\n",
      "394 Current tokens: 24549\n",
      "395 Current tokens: 10246\n",
      "396 Current tokens: 12704\n",
      "397 Current tokens: 6330\n",
      "398 Current tokens: 59817\n",
      "399 Current tokens: 26951\n",
      "400 Current tokens: 0\n",
      "401 Current tokens: 24343\n",
      "402 Current tokens: 7008\n",
      "403 Current tokens: 15232\n",
      "404 Current tokens: 49879\n",
      "405 Current tokens: 6912\n",
      "406 Current tokens: 10195\n",
      "407 Current tokens: 14479\n",
      "408 Current tokens: 9615\n",
      "409 Current tokens: 23162\n",
      "410 Current tokens: 20262\n",
      "411 Current tokens: 10194\n",
      "412 Current tokens: 11911\n",
      "413 Current tokens: 62458\n",
      "414 Current tokens: 10378\n",
      "415 Current tokens: 13747\n",
      "416 Current tokens: 37260\n",
      "417 Current tokens: 63868\n",
      "418 Current tokens: 41922\n",
      "419 Current tokens: 8234\n",
      "420 Current tokens: 78414\n",
      "421 Current tokens: 58980\n",
      "422 Current tokens: 12528\n",
      "[58, 138, 209, 216, 400]\n",
      "Total tokens: 10155019\n",
      "Price: $0.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the file\n",
    "with open('../data/train.jsonl', 'r') as file:\n",
    "    # Iterate over each line\n",
    "    # i = 0\n",
    "    total_tokens = 0\n",
    "    fails = []\n",
    "    for index, line in enumerate(file):\n",
    "        # Load the JSON data from each line\n",
    "        data = json.loads(line)\n",
    "        # Now you can use the data as a normal Python dictionary\n",
    "        # print(data)\n",
    "        # dict_keys(['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'content', 'references'])\n",
    "        # print(data.keys())\n",
    "        try:\n",
    "            current_tokens = num_tokens_from_string(data['content'])\n",
    "        except:\n",
    "            fails.append(index)\n",
    "            current_tokens = 0\n",
    "\n",
    "        # print(data['content'][-500:])\n",
    "        \n",
    "        total_tokens += current_tokens\n",
    "        print(f\"{index} Current tokens: {current_tokens}\")\n",
    "\n",
    "        # i += 1\n",
    "        # if i > 5:\n",
    "        #     break\n",
    "    print(fails)\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Price: ${round((total_tokens/1000000) * 0.02, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose Collection.\n",
    "2. Iterate through pdfs. Keep track in case of error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_algorithm():\n",
    "    pass\n",
    "\n",
    "\n",
    "# This class is designed to rapidly create new collections for various chunking methods.\n",
    "# It's done via a class so that it holds state in the case of an error mid-way through the process.\n",
    "class CollectionWriter:\n",
    "    def __init__(self, path=\"../data/chroma_db\"):\n",
    "        self.chroma_client = chromadb.PersistentClient(path=path)\n",
    "        self.openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "        self.collection = None\n",
    "        self.index = 0\n",
    "\n",
    "    def trial_run(self, collection_name, chunking_function):\n",
    "        self.write_to_collection(\"trial_run\", chunking_function, 0, trial_run=True)\n",
    "\n",
    "    def write_to_collection(self, collection_name, chunking_function, start_index=0, trial_run=False):\n",
    "        if not trial_run:\n",
    "            self.collection = chroma_client.get_or_create_collection(name=collection_name, embedding_function=self.openai_ef)\n",
    "        \n",
    "        total_tokens = 0\n",
    "\n",
    "        with open('../data/train.jsonl', 'r') as file:\n",
    "            for index, line in enumerate(file):\n",
    "                if index < start_index:\n",
    "                    continue\n",
    "                if index == 368:\n",
    "                    continue\n",
    "                self.index = index\n",
    "                data = json.loads(line)\n",
    "                try:\n",
    "                    documents = chunking_function(data['content'])\n",
    "                except:\n",
    "                    documents = []\n",
    "                metadatas = [{\"id\": data['id'], \"title\": data['title']} for _ in range(len(documents))]\n",
    "                ids = [data['id']+\":\"+str(i) for i in range(len(documents))]\n",
    "                if not trial_run:\n",
    "                    try:\n",
    "                        self.collection.add(\n",
    "                            documents=documents,\n",
    "                            metadatas=metadatas,\n",
    "                            ids=ids\n",
    "                        )\n",
    "                    except:\n",
    "                        print(f\"Failed at index {index}\")\n",
    "                # print(ids[:5])\n",
    "                # print(metadatas[:5])\n",
    "                # print(documents[0])\n",
    "                try:\n",
    "                    num_tokens = sum([num_tokens_from_string(doc) for doc in documents])\n",
    "                except:\n",
    "                    num_tokens = 0\n",
    "                print(f\"{index} Added {len(documents)} documents. Current tokens: {num_tokens}\")\n",
    "                # print(f\"{index} Added {len(documents)} documents.\")\n",
    "                total_tokens += num_tokens\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Price: ${round((total_tokens/1000000) * 0.13, 2)}\")\n",
    "        # print(f\"Price: ${round((total_tokens/1000000) * 0.02, 2)}\")\n",
    "        return self.collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Added 52 documents. Current tokens: 19523\n",
      "1 Added 33 documents. Current tokens: 12363\n",
      "2 Added 42 documents. Current tokens: 15620\n",
      "3 Added 57 documents. Current tokens: 21219\n",
      "4 Added 82 documents. Current tokens: 30797\n",
      "5 Added 47 documents. Current tokens: 17547\n",
      "6 Added 26 documents. Current tokens: 9545\n",
      "7 Added 65 documents. Current tokens: 24580\n",
      "8 Added 67 documents. Current tokens: 25242\n",
      "9 Added 41 documents. Current tokens: 15236\n",
      "10 Added 26 documents. Current tokens: 9832\n",
      "11 Added 154 documents. Current tokens: 58182\n",
      "12 Added 34 documents. Current tokens: 12786\n",
      "13 Added 154 documents. Current tokens: 58543\n",
      "14 Added 29 documents. Current tokens: 10962\n",
      "15 Added 40 documents. Current tokens: 14892\n",
      "16 Added 63 documents. Current tokens: 18731\n",
      "17 Added 47 documents. Current tokens: 17612\n",
      "18 Added 27 documents. Current tokens: 10237\n",
      "19 Added 30 documents. Current tokens: 11152\n",
      "20 Added 93 documents. Current tokens: 35219\n",
      "21 Added 79 documents. Current tokens: 30109\n",
      "22 Added 29 documents. Current tokens: 10935\n",
      "23 Added 488 documents. Current tokens: 173644\n",
      "24 Added 60 documents. Current tokens: 22646\n",
      "25 Added 122 documents. Current tokens: 46002\n",
      "26 Added 66 documents. Current tokens: 24946\n",
      "27 Added 68 documents. Current tokens: 25508\n",
      "28 Added 51 documents. Current tokens: 18990\n",
      "29 Added 63 documents. Current tokens: 23980\n",
      "30 Added 53 documents. Current tokens: 20062\n",
      "31 Added 45 documents. Current tokens: 17046\n",
      "32 Added 77 documents. Current tokens: 29133\n",
      "33 Added 57 documents. Current tokens: 21461\n",
      "34 Added 145 documents. Current tokens: 55006\n",
      "35 Added 27 documents. Current tokens: 9854\n",
      "36 Added 18 documents. Current tokens: 6801\n",
      "37 Added 16 documents. Current tokens: 5788\n",
      "38 Added 34 documents. Current tokens: 12737\n",
      "39 Added 28 documents. Current tokens: 10297\n",
      "40 Added 68 documents. Current tokens: 25618\n",
      "41 Added 27 documents. Current tokens: 10248\n",
      "42 Added 48 documents. Current tokens: 17900\n",
      "43 Added 273 documents. Current tokens: 96843\n",
      "44 Added 98 documents. Current tokens: 33430\n",
      "45 Added 34 documents. Current tokens: 12534\n",
      "46 Added 118 documents. Current tokens: 44476\n",
      "47 Added 30 documents. Current tokens: 11191\n",
      "48 Added 34 documents. Current tokens: 12890\n",
      "49 Added 47 documents. Current tokens: 17893\n",
      "50 Added 42 documents. Current tokens: 15511\n",
      "51 Added 63 documents. Current tokens: 23569\n",
      "52 Added 54 documents. Current tokens: 20175\n",
      "53 Added 58 documents. Current tokens: 21741\n",
      "54 Added 31 documents. Current tokens: 11607\n",
      "55 Added 540 documents. Current tokens: 205560\n",
      "56 Added 30 documents. Current tokens: 11258\n",
      "57 Added 67 documents. Current tokens: 25176\n",
      "58 Added 26 documents. Current tokens: 9885\n",
      "59 Added 82 documents. Current tokens: 30922\n",
      "60 Added 32 documents. Current tokens: 11882\n",
      "61 Added 218 documents. Current tokens: 82225\n",
      "62 Added 108 documents. Current tokens: 40793\n",
      "63 Added 56 documents. Current tokens: 20968\n",
      "64 Added 121 documents. Current tokens: 45574\n",
      "65 Added 23 documents. Current tokens: 8749\n",
      "66 Added 82 documents. Current tokens: 30807\n",
      "67 Added 130 documents. Current tokens: 49077\n",
      "68 Added 103 documents. Current tokens: 37703\n",
      "69 Added 96 documents. Current tokens: 36570\n",
      "70 Added 94 documents. Current tokens: 35340\n",
      "71 Added 41 documents. Current tokens: 15195\n",
      "72 Added 39 documents. Current tokens: 14629\n",
      "73 Added 39 documents. Current tokens: 14693\n",
      "74 Added 84 documents. Current tokens: 31790\n",
      "75 Added 57 documents. Current tokens: 21490\n",
      "76 Added 94 documents. Current tokens: 35960\n",
      "77 Added 34 documents. Current tokens: 12757\n",
      "78 Added 49 documents. Current tokens: 18569\n",
      "79 Added 27 documents. Current tokens: 9862\n",
      "80 Added 42 documents. Current tokens: 15985\n",
      "81 Added 54 documents. Current tokens: 20325\n",
      "82 Added 106 documents. Current tokens: 39344\n",
      "83 Added 35 documents. Current tokens: 13171\n",
      "84 Added 50 documents. Current tokens: 18729\n",
      "85 Added 61 documents. Current tokens: 22841\n",
      "86 Added 43 documents. Current tokens: 16026\n",
      "87 Added 97 documents. Current tokens: 36763\n",
      "88 Added 20 documents. Current tokens: 7571\n",
      "89 Added 42 documents. Current tokens: 13239\n",
      "90 Added 110 documents. Current tokens: 41604\n",
      "91 Added 38 documents. Current tokens: 14264\n",
      "92 Added 134 documents. Current tokens: 50720\n",
      "93 Added 34 documents. Current tokens: 12714\n",
      "94 Added 58 documents. Current tokens: 20949\n",
      "95 Added 61 documents. Current tokens: 23126\n",
      "96 Added 97 documents. Current tokens: 36258\n",
      "97 Added 73 documents. Current tokens: 27439\n",
      "98 Added 43 documents. Current tokens: 16379\n",
      "99 Added 26 documents. Current tokens: 9673\n",
      "100 Added 97 documents. Current tokens: 36442\n",
      "101 Added 60 documents. Current tokens: 22813\n",
      "102 Added 41 documents. Current tokens: 15333\n",
      "103 Added 16 documents. Current tokens: 5962\n",
      "104 Added 53 documents. Current tokens: 19823\n",
      "105 Added 41 documents. Current tokens: 14571\n",
      "106 Added 56 documents. Current tokens: 21066\n",
      "107 Added 74 documents. Current tokens: 28155\n",
      "108 Added 44 documents. Current tokens: 16531\n",
      "109 Added 48 documents. Current tokens: 17914\n",
      "110 Added 31 documents. Current tokens: 11371\n",
      "111 Added 85 documents. Current tokens: 32140\n",
      "112 Added 7 documents. Current tokens: 2531\n",
      "113 Added 16 documents. Current tokens: 5752\n",
      "114 Added 97 documents. Current tokens: 36627\n",
      "115 Added 32 documents. Current tokens: 11852\n",
      "116 Added 80 documents. Current tokens: 30339\n",
      "117 Added 49 documents. Current tokens: 18634\n",
      "118 Added 63 documents. Current tokens: 20535\n",
      "119 Added 74 documents. Current tokens: 27947\n",
      "120 Added 65 documents. Current tokens: 24354\n",
      "121 Added 29 documents. Current tokens: 11031\n",
      "122 Added 30 documents. Current tokens: 11124\n",
      "123 Added 19 documents. Current tokens: 7194\n",
      "124 Added 108 documents. Current tokens: 40689\n",
      "125 Added 316 documents. Current tokens: 118787\n",
      "126 Added 44 documents. Current tokens: 16493\n",
      "127 Added 64 documents. Current tokens: 24074\n",
      "128 Added 48 documents. Current tokens: 18133\n",
      "129 Added 99 documents. Current tokens: 37498\n",
      "130 Added 12 documents. Current tokens: 4212\n",
      "131 Added 24 documents. Current tokens: 8847\n",
      "132 Added 32 documents. Current tokens: 11804\n",
      "133 Added 80 documents. Current tokens: 30124\n",
      "134 Added 169 documents. Current tokens: 64112\n",
      "135 Added 64 documents. Current tokens: 23979\n",
      "136 Added 37 documents. Current tokens: 14011\n",
      "137 Added 67 documents. Current tokens: 24858\n",
      "138 Added 110 documents. Current tokens: 40901\n",
      "139 Added 23 documents. Current tokens: 8455\n",
      "140 Added 30 documents. Current tokens: 11287\n",
      "141 Added 30 documents. Current tokens: 11346\n",
      "142 Added 86 documents. Current tokens: 31285\n",
      "143 Added 84 documents. Current tokens: 31356\n",
      "144 Added 20 documents. Current tokens: 7386\n",
      "145 Added 15 documents. Current tokens: 5701\n",
      "146 Added 70 documents. Current tokens: 26590\n",
      "147 Added 25 documents. Current tokens: 9177\n",
      "148 Added 48 documents. Current tokens: 18011\n",
      "149 Added 32 documents. Current tokens: 11940\n",
      "150 Added 26 documents. Current tokens: 9301\n",
      "151 Added 124 documents. Current tokens: 46141\n",
      "152 Added 82 documents. Current tokens: 31109\n",
      "153 Added 36 documents. Current tokens: 13360\n",
      "154 Added 29 documents. Current tokens: 10888\n",
      "155 Added 69 documents. Current tokens: 25791\n",
      "156 Added 55 documents. Current tokens: 20422\n",
      "157 Added 26 documents. Current tokens: 9633\n",
      "158 Added 32 documents. Current tokens: 11992\n",
      "159 Added 52 documents. Current tokens: 19443\n",
      "160 Added 65 documents. Current tokens: 24251\n",
      "161 Added 42 documents. Current tokens: 15705\n",
      "162 Added 75 documents. Current tokens: 28415\n",
      "163 Added 51 documents. Current tokens: 19419\n",
      "164 Added 14 documents. Current tokens: 5194\n",
      "165 Added 72 documents. Current tokens: 26883\n",
      "166 Added 43 documents. Current tokens: 16131\n",
      "167 Added 57 documents. Current tokens: 21481\n",
      "168 Added 39 documents. Current tokens: 14628\n",
      "169 Added 142 documents. Current tokens: 52052\n",
      "170 Added 13 documents. Current tokens: 4987\n",
      "171 Added 72 documents. Current tokens: 26671\n",
      "172 Added 50 documents. Current tokens: 18727\n",
      "173 Added 27 documents. Current tokens: 10312\n",
      "174 Added 50 documents. Current tokens: 18847\n",
      "175 Added 45 documents. Current tokens: 17085\n",
      "176 Added 37 documents. Current tokens: 13665\n",
      "177 Added 26 documents. Current tokens: 9557\n",
      "178 Added 26 documents. Current tokens: 9742\n",
      "179 Added 40 documents. Current tokens: 14935\n",
      "180 Added 46 documents. Current tokens: 17210\n",
      "181 Added 53 documents. Current tokens: 17857\n",
      "182 Added 27 documents. Current tokens: 10034\n",
      "183 Added 79 documents. Current tokens: 30052\n",
      "184 Added 43 documents. Current tokens: 16085\n",
      "185 Added 98 documents. Current tokens: 37280\n",
      "186 Added 38 documents. Current tokens: 14312\n",
      "187 Added 26 documents. Current tokens: 9569\n",
      "188 Added 42 documents. Current tokens: 15679\n",
      "189 Added 141 documents. Current tokens: 52925\n",
      "190 Added 29 documents. Current tokens: 10828\n",
      "191 Added 39 documents. Current tokens: 14756\n",
      "192 Added 121 documents. Current tokens: 43286\n",
      "193 Added 45 documents. Current tokens: 16916\n",
      "194 Added 58 documents. Current tokens: 20370\n",
      "195 Added 68 documents. Current tokens: 25498\n",
      "196 Added 28 documents. Current tokens: 10512\n",
      "197 Added 34 documents. Current tokens: 12864\n",
      "198 Added 40 documents. Current tokens: 15021\n",
      "199 Added 36 documents. Current tokens: 13178\n",
      "200 Added 69 documents. Current tokens: 21689\n",
      "201 Added 60 documents. Current tokens: 22569\n",
      "202 Added 61 documents. Current tokens: 22464\n",
      "203 Added 72 documents. Current tokens: 27063\n",
      "204 Added 31 documents. Current tokens: 11602\n",
      "205 Added 27 documents. Current tokens: 9888\n",
      "206 Added 68 documents. Current tokens: 25938\n",
      "207 Added 28 documents. Current tokens: 10613\n",
      "208 Added 38 documents. Current tokens: 14285\n",
      "209 Added 65 documents. Current tokens: 24627\n",
      "210 Added 30 documents. Current tokens: 11008\n",
      "211 Added 13 documents. Current tokens: 4764\n",
      "212 Added 84 documents. Current tokens: 31730\n",
      "213 Added 51 documents. Current tokens: 15919\n",
      "214 Added 41 documents. Current tokens: 15726\n",
      "215 Added 20 documents. Current tokens: 7364\n",
      "216 Added 75 documents. Current tokens: 28409\n",
      "217 Added 28 documents. Current tokens: 10403\n",
      "218 Added 46 documents. Current tokens: 17591\n",
      "219 Added 47 documents. Current tokens: 17758\n",
      "220 Added 82 documents. Current tokens: 27029\n",
      "221 Added 70 documents. Current tokens: 26359\n",
      "222 Added 94 documents. Current tokens: 35451\n",
      "223 Added 96 documents. Current tokens: 36587\n",
      "224 Added 161 documents. Current tokens: 61142\n",
      "225 Added 256 documents. Current tokens: 97089\n",
      "226 Added 59 documents. Current tokens: 21814\n",
      "227 Added 62 documents. Current tokens: 23574\n",
      "228 Added 79 documents. Current tokens: 29758\n",
      "229 Added 20 documents. Current tokens: 7281\n",
      "230 Added 84 documents. Current tokens: 31538\n",
      "231 Added 115 documents. Current tokens: 43380\n",
      "232 Added 96 documents. Current tokens: 33824\n",
      "233 Added 42 documents. Current tokens: 16059\n",
      "234 Added 159 documents. Current tokens: 58890\n",
      "235 Added 60 documents. Current tokens: 22533\n",
      "236 Added 41 documents. Current tokens: 15486\n",
      "237 Added 97 documents. Current tokens: 36917\n",
      "238 Added 55 documents. Current tokens: 20611\n",
      "239 Added 67 documents. Current tokens: 25155\n",
      "240 Added 36 documents. Current tokens: 13345\n",
      "241 Added 49 documents. Current tokens: 18465\n",
      "242 Added 51 documents. Current tokens: 19198\n",
      "243 Added 54 documents. Current tokens: 20270\n",
      "244 Added 65 documents. Current tokens: 24541\n",
      "245 Added 57 documents. Current tokens: 21623\n",
      "246 Added 105 documents. Current tokens: 39683\n",
      "247 Added 31 documents. Current tokens: 11555\n",
      "248 Added 54 documents. Current tokens: 20416\n",
      "249 Added 140 documents. Current tokens: 53066\n",
      "250 Added 48 documents. Current tokens: 17879\n",
      "251 Added 33 documents. Current tokens: 12214\n",
      "252 Added 37 documents. Current tokens: 13849\n",
      "253 Added 42 documents. Current tokens: 15550\n",
      "254 Added 22 documents. Current tokens: 7993\n",
      "255 Added 42 documents. Current tokens: 15735\n",
      "256 Added 99 documents. Current tokens: 37245\n",
      "257 Added 91 documents. Current tokens: 34763\n",
      "258 Added 68 documents. Current tokens: 23915\n",
      "259 Added 85 documents. Current tokens: 32191\n",
      "260 Added 24 documents. Current tokens: 8713\n",
      "261 Added 47 documents. Current tokens: 17832\n",
      "262 Added 39 documents. Current tokens: 14426\n",
      "263 Added 16 documents. Current tokens: 6036\n",
      "264 Added 54 documents. Current tokens: 20213\n",
      "265 Added 50 documents. Current tokens: 18652\n",
      "266 Added 63 documents. Current tokens: 24083\n",
      "267 Added 12 documents. Current tokens: 4199\n",
      "268 Added 83 documents. Current tokens: 31432\n",
      "269 Added 46 documents. Current tokens: 17210\n",
      "270 Added 29 documents. Current tokens: 10679\n",
      "271 Added 55 documents. Current tokens: 20956\n",
      "272 Added 35 documents. Current tokens: 13049\n",
      "273 Added 70 documents. Current tokens: 26615\n",
      "274 Added 28 documents. Current tokens: 10473\n",
      "275 Added 32 documents. Current tokens: 11974\n",
      "276 Added 51 documents. Current tokens: 19193\n",
      "277 Added 73 documents. Current tokens: 27439\n",
      "278 Added 153 documents. Current tokens: 58308\n",
      "279 Added 17 documents. Current tokens: 6320\n",
      "280 Added 17 documents. Current tokens: 6066\n",
      "281 Added 39 documents. Current tokens: 14411\n",
      "282 Added 175 documents. Current tokens: 66158\n",
      "283 Added 54 documents. Current tokens: 20486\n",
      "284 Added 67 documents. Current tokens: 25107\n",
      "285 Added 34 documents. Current tokens: 12564\n",
      "286 Added 31 documents. Current tokens: 11524\n",
      "287 Added 58 documents. Current tokens: 20973\n",
      "288 Added 161 documents. Current tokens: 58341\n",
      "289 Added 49 documents. Current tokens: 18283\n",
      "290 Added 34 documents. Current tokens: 12857\n",
      "291 Added 48 documents. Current tokens: 17880\n",
      "292 Added 77 documents. Current tokens: 29185\n",
      "293 Added 69 documents. Current tokens: 26040\n",
      "294 Added 53 documents. Current tokens: 19703\n",
      "295 Added 29 documents. Current tokens: 10696\n",
      "296 Added 14 documents. Current tokens: 5026\n",
      "297 Added 56 documents. Current tokens: 20471\n",
      "298 Added 52 documents. Current tokens: 19538\n",
      "299 Added 48 documents. Current tokens: 17927\n",
      "300 Added 34 documents. Current tokens: 12853\n",
      "301 Added 137 documents. Current tokens: 51683\n",
      "302 Added 33 documents. Current tokens: 12348\n",
      "303 Added 34 documents. Current tokens: 12616\n",
      "304 Added 38 documents. Current tokens: 14339\n",
      "305 Added 18 documents. Current tokens: 6732\n",
      "306 Added 22 documents. Current tokens: 8017\n",
      "307 Added 51 documents. Current tokens: 19200\n",
      "308 Added 24 documents. Current tokens: 8762\n",
      "309 Added 47 documents. Current tokens: 17892\n",
      "310 Added 25 documents. Current tokens: 9237\n",
      "311 Added 19 documents. Current tokens: 6830\n",
      "312 Added 26 documents. Current tokens: 9610\n",
      "313 Added 47 documents. Current tokens: 17955\n",
      "314 Added 31 documents. Current tokens: 11707\n",
      "315 Added 15 documents. Current tokens: 5394\n",
      "316 Added 13 documents. Current tokens: 4894\n",
      "317 Added 42 documents. Current tokens: 15663\n",
      "318 Added 28 documents. Current tokens: 10325\n",
      "319 Added 80 documents. Current tokens: 29919\n",
      "320 Added 74 documents. Current tokens: 27586\n",
      "321 Added 40 documents. Current tokens: 14838\n",
      "322 Added 50 documents. Current tokens: 18940\n",
      "323 Added 21 documents. Current tokens: 7871\n",
      "324 Added 26 documents. Current tokens: 9766\n",
      "325 Added 48 documents. Current tokens: 17429\n",
      "326 Added 27 documents. Current tokens: 9894\n",
      "327 Added 102 documents. Current tokens: 38549\n",
      "328 Added 44 documents. Current tokens: 16551\n",
      "329 Added 108 documents. Current tokens: 40981\n",
      "330 Added 54 documents. Current tokens: 20322\n",
      "331 Added 68 documents. Current tokens: 25706\n",
      "332 Added 29 documents. Current tokens: 11040\n",
      "333 Added 34 documents. Current tokens: 12730\n",
      "334 Added 231 documents. Current tokens: 84692\n",
      "335 Added 28 documents. Current tokens: 10618\n",
      "336 Added 53 documents. Current tokens: 20141\n",
      "337 Added 34 documents. Current tokens: 12729\n",
      "338 Added 71 documents. Current tokens: 26869\n",
      "339 Added 28 documents. Current tokens: 10269\n",
      "340 Added 33 documents. Current tokens: 12509\n",
      "341 Added 17 documents. Current tokens: 6141\n",
      "342 Added 41 documents. Current tokens: 15571\n",
      "343 Added 103 documents. Current tokens: 39005\n",
      "344 Added 32 documents. Current tokens: 11895\n",
      "345 Added 27 documents. Current tokens: 10159\n",
      "346 Added 90 documents. Current tokens: 33162\n",
      "347 Added 45 documents. Current tokens: 16671\n",
      "348 Added 54 documents. Current tokens: 20290\n",
      "349 Added 32 documents. Current tokens: 11734\n",
      "350 Added 48 documents. Current tokens: 17834\n",
      "351 Added 82 documents. Current tokens: 30961\n",
      "352 Added 36 documents. Current tokens: 13164\n",
      "353 Added 30 documents. Current tokens: 11156\n",
      "354 Added 42 documents. Current tokens: 15454\n",
      "355 Added 51 documents. Current tokens: 19419\n",
      "356 Added 49 documents. Current tokens: 18035\n",
      "357 Added 56 documents. Current tokens: 21286\n",
      "358 Added 79 documents. Current tokens: 29170\n",
      "359 Added 27 documents. Current tokens: 10138\n",
      "360 Added 17 documents. Current tokens: 6500\n",
      "361 Added 19 documents. Current tokens: 7138\n",
      "362 Added 71 documents. Current tokens: 26542\n",
      "363 Added 103 documents. Current tokens: 38968\n",
      "364 Added 42 documents. Current tokens: 14535\n",
      "365 Added 79 documents. Current tokens: 29901\n",
      "366 Added 59 documents. Current tokens: 22065\n",
      "367 Added 63 documents. Current tokens: 23638\n",
      "369 Added 30 documents. Current tokens: 11124\n",
      "370 Added 33 documents. Current tokens: 12360\n",
      "371 Added 29 documents. Current tokens: 10928\n",
      "372 Added 50 documents. Current tokens: 18869\n",
      "373 Added 29 documents. Current tokens: 10736\n",
      "374 Added 20 documents. Current tokens: 7274\n",
      "375 Added 61 documents. Current tokens: 22849\n",
      "376 Added 33 documents. Current tokens: 12447\n",
      "377 Added 61 documents. Current tokens: 23015\n",
      "378 Added 30 documents. Current tokens: 11207\n",
      "379 Added 53 documents. Current tokens: 19944\n",
      "380 Added 17 documents. Current tokens: 6198\n",
      "381 Added 35 documents. Current tokens: 12905\n",
      "382 Added 69 documents. Current tokens: 25922\n",
      "383 Added 37 documents. Current tokens: 13672\n",
      "384 Added 43 documents. Current tokens: 16274\n",
      "385 Added 76 documents. Current tokens: 28155\n",
      "386 Added 142 documents. Current tokens: 41671\n",
      "387 Added 89 documents. Current tokens: 33946\n",
      "388 Added 96 documents. Current tokens: 35979\n",
      "389 Added 38 documents. Current tokens: 13875\n",
      "390 Added 60 documents. Current tokens: 22757\n",
      "391 Added 109 documents. Current tokens: 40830\n",
      "392 Added 30 documents. Current tokens: 10588\n",
      "393 Added 60 documents. Current tokens: 22604\n",
      "394 Added 65 documents. Current tokens: 24512\n",
      "395 Added 27 documents. Current tokens: 10228\n",
      "396 Added 34 documents. Current tokens: 12688\n",
      "397 Added 17 documents. Current tokens: 6320\n",
      "398 Added 158 documents. Current tokens: 59713\n",
      "399 Added 72 documents. Current tokens: 26912\n",
      "400 Added 346 documents. Current tokens: 130840\n",
      "401 Added 64 documents. Current tokens: 24296\n",
      "402 Added 19 documents. Current tokens: 6995\n",
      "403 Added 60 documents. Current tokens: 15039\n",
      "404 Added 132 documents. Current tokens: 49808\n",
      "405 Added 19 documents. Current tokens: 6904\n",
      "406 Added 27 documents. Current tokens: 10183\n",
      "407 Added 39 documents. Current tokens: 14462\n",
      "408 Added 26 documents. Current tokens: 9595\n",
      "409 Added 61 documents. Current tokens: 23126\n",
      "410 Added 54 documents. Current tokens: 20224\n",
      "411 Added 27 documents. Current tokens: 10179\n",
      "412 Added 32 documents. Current tokens: 11897\n",
      "413 Added 166 documents. Current tokens: 62377\n",
      "414 Added 28 documents. Current tokens: 10361\n",
      "415 Added 37 documents. Current tokens: 13730\n",
      "416 Added 99 documents. Current tokens: 37203\n",
      "417 Added 171 documents. Current tokens: 63735\n",
      "418 Added 112 documents. Current tokens: 41847\n",
      "419 Added 22 documents. Current tokens: 8219\n",
      "420 Added 207 documents. Current tokens: 78281\n",
      "421 Added 156 documents. Current tokens: 58887\n",
      "422 Added 33 documents. Current tokens: 12506\n",
      "Total tokens: 9657196\n",
      "Price: $1.26\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "import json\n",
    "# Initialize the Recursive Text Splitter\n",
    "#   chunk_size: int = 4000, DEFAULT_CHUNK_SIZE\n",
    "#   chunk_overlap: int = 200, DEFAULT_CHUNK_OVERLAP\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1024,\n",
    "    # chunk_overlap=256,\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    "    length_function=num_tokens_from_string\n",
    ")\n",
    "\n",
    "# splitter = TokenTextSplitter(\n",
    "#     encoding_name=\"cl100k_base\",\n",
    "#     chunk_size=400,\n",
    "#     chunk_overlap=0,\n",
    "# )\n",
    "\n",
    "collection_writer = CollectionWriter()\n",
    "\n",
    "def chunking_function(content):\n",
    "    return splitter.split_text(content)\n",
    "\n",
    "# chunked_collection = collection_writer.trial_run(\"chuck_1\", chunking_function)\n",
    "chunked_collection = collection_writer.write_to_collection(\"chuck_4\", chunking_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "split_text = splitter.split_text(\"This is a test sentence. This is another test sentence. This is a third test sentence.\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=chuck_4),\n",
       " Collection(name=chuck_1),\n",
       " Collection(name=chuck_3),\n",
       " Collection(name=chuck_2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()\n",
    "# chroma_client.delete_collection(\"chuck_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20588"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-large\"\n",
    "            )\n",
    "\n",
    "collection = chroma_client.get_collection(\"chuck_8\", embedding_function=openai_ef)\n",
    "\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[\"What are the two main tasks BERT is pre-trained on?\"], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and next sentence pre-\\ndiction.\\nMasked Language Model (MLM) A random\\nsample of the tokens in the input sequence is\\nselected and replaced with the special token\\n[MASK]. The MLM objective is a cross-entropy\\nloss on predicting the masked tokens. BERT uni-\\nformly selects 15% of the input tokens for possi-\\nble replacement. Of the selected tokens, 80% are\\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\\ncabulary token.\\nIn the original implementation, random mask-\\ning and replacement is performed once in the be-\\nginning and saved for the duration of training, al-\\nthough in practice, data is duplicated so the mask\\nis not always the same for every training sentence\\n(see Section 4.1).\\nNext Sentence Prediction (NSP) NSP is a bi-\\nnary classiﬁcation loss for predicting whether two\\nsegments follow each other in the original text.\\nPositive examples are created by taking consecu-\\ntive sentences from the text corpus. Negative ex-\\namples are created by pairing segments from dif-\\nferent documents. Positive and negative examples\\nare sampled with equal probability.\\nThe NSP objective was designed to improve\\nperformance on downstream tasks, such as Natural\\nLanguage Inference ( Bowman et al. ,2015 ), which\\nrequire reasoning about the relationships between\\npairs of sentences.\\n2.4 Optimization\\nBERT is optimized with Adam ( Kingma and Ba ,\\n2015 ) using the following parameters: β1= 0.9,\\nβ2= 0.999,ǫ=1e-6 and L2weight de-\\ncay of0.01. The learning rate is warmed up\\nover the ﬁrst 10,000 steps to a peak value of\\n1e-4, and then linearly decayed. BERT trains\\nwith a dropout of 0.1 on all layers and at-\\ntention weights, and a GELU activation func-\\ntion ( Hendrycks and Gimpel ,2016 ). Models are\\npretrained for S=1,000,000 updates, with mini-\\nbatches containing B=256 sequences of maxi-\\nmum length T=512 tokens.\\n2.5 Data\\nBERT is trained on a combination of B OOK COR-\\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\\nwhich totals 16GB of'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../eval_questions/eval_data.json') as f:\n",
    "    eval_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=eval_data['questions'], n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthetensor2tensor library.1Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERT BASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERT LARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERT BASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every',\n",
       " 'we show that our model also works well with an\\nELMo module on NER and constituency parsing.\\nThe concurrently introduced BERT model (De-\\nvlin et al., 2018) is a transformer encoder model\\nthat captures left and right context. There is sig-\\nniﬁcant overlap between their work and ours but\\nthere are also signiﬁcant differences: our model is\\na bi-directional transformer language model that\\npredicts every single token in a sequence. BERT\\nis also a transformer encoder that has access to the\\nentire input which makes it bi-directional but this\\nchoice requires a special training regime. In par-\\nticular, they multi-task between predicting a sub-\\nset of masked input tokens, similar to a denoising\\nautoencoder, and a next sentence prediction task.\\nIn comparison, we optimize a single loss function\\nthat requires the model to predict each token of an\\ninput sentence given all surrounding tokens. Weuse all tokens as training targets and therefore ex-\\ntract learning signal from every single token in the\\nsentence and not just a subset.\\nBERT tailors pretraining to capture dependen-\\ncies between sentences via a next sentence predic-\\ntion task as well as by constructing training exam-\\nples of sentence-pairs with input markers that dis-\\ntinguish between tokens of the two sentences. Our\\nmodel is trained similarly to a classical language\\nmodel since we do not adapt the training exam-\\nples to resemble the end task data and we do not\\nsolve a denoising task during training.\\nFinally, BERT as well as Radford et al. (2018)\\nconsider only a single data source to pretrain\\ntheir models, either BooksCorpus (Radford et al.,\\n2018), or BooksCorpus and additional Wikipedia',\n",
       " 'BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\nfjacobdevlin,mingweichang,kentonl,kristout g@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT , which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,',\n",
       " 'trained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level andtoken-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert .\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and',\n",
       " 'StructBERT builds upon the BERT architecture, which uses a multi-layer bidirectional Transformer network [ 26]. Given\\na single text sentence or a pair of text sentences, BERT packs them in one token sequence and learns a contextualized\\nvector representation for each token. Every input token is represented based on the word, the position, and the text\\nsegment it belongs to. Next, the input vectors are fed into a stack of multi-layer bidirectional Transformer blocks, which\\nuses self-attention to compute the text representations by considering the entire input sequence.\\nThe original BERT introduces two unsupervised prediction tasks to pre-train the model: i.e., a masked LM task and\\na next sentence prediction task. Different from original BERT, our StructBERT ampliﬁes the ability of the masked\\nLM task by shufﬂing certain number of tokens after word masking and predicting the right order. Moreover, to better\\nunderstand the relationship between sentences, StructBERT randomly swaps the sentence order and predicts the next\\nsentence and the previous sentence as a new sentence prediction task. In this way, the new model not only explicitly\\ncaptures the ﬁne-grained word structure in every sentence, but also properly models the inter-sentence structure in\\na bidirectional manner. Once the StructBERT language model is pre-trained with these two auxiliary tasks, we can\\nﬁne-tune it on task-speciﬁc data for a wide range of downstream tasks.\\n2.1 Input Representation\\nEvery inputxis a sequence of word tokens, which can be either a single sentence or a pair of sentences packed together.\\nThe input representation follows that used in BERT [ 6]. For each input token ti, its vector representation xiis computed\\nby summing the corresponding token embedding, positional embedding, and segment embedding. We always add a']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = eval_data['questions'][0]\n",
    "context = results['documents'][0][0]\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_retrieval_precision_indicator(question, context):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": get_retrieval_precision_prompt(question, context)},\n",
    "            {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "        ]\n",
    "        )\n",
    "    \n",
    "    return 1 if completion.choices[0].message.content.lower().strip() == 'true' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "retrieval_precision_matrix = np.zeros((len(eval_data['questions']), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0, Context 0: 1.0\n",
      "Question 0, Context 1: 1.0\n",
      "Question 0, Context 2: 1.0\n",
      "Question 0, Context 3: 1.0\n",
      "Question 0, Context 4: 0.0\n",
      "Question 1, Context 0: 1.0\n",
      "Question 1, Context 1: 1.0\n",
      "Question 1, Context 2: 1.0\n",
      "Question 1, Context 3: 1.0\n",
      "Question 1, Context 4: 1.0\n",
      "Question 2, Context 0: 1.0\n",
      "Question 2, Context 1: 1.0\n",
      "Question 2, Context 2: 1.0\n",
      "Question 2, Context 3: 1.0\n",
      "Question 2, Context 4: 1.0\n",
      "Question 3, Context 0: 1.0\n",
      "Question 3, Context 1: 1.0\n",
      "Question 3, Context 2: 1.0\n",
      "Question 3, Context 3: 1.0\n",
      "Question 3, Context 4: 1.0\n",
      "Question 4, Context 0: 1.0\n",
      "Question 4, Context 1: 1.0\n",
      "Question 4, Context 2: 1.0\n",
      "Question 4, Context 3: 1.0\n",
      "Question 4, Context 4: 1.0\n",
      "Question 5, Context 0: 1.0\n",
      "Question 5, Context 1: 1.0\n",
      "Question 5, Context 2: 1.0\n",
      "Question 5, Context 3: 1.0\n",
      "Question 5, Context 4: 1.0\n",
      "Question 6, Context 0: 1.0\n",
      "Question 6, Context 1: 1.0\n",
      "Question 6, Context 2: 1.0\n",
      "Question 6, Context 3: 0.0\n",
      "Question 6, Context 4: 0.0\n",
      "Question 7, Context 0: 1.0\n",
      "Question 7, Context 1: 1.0\n",
      "Question 7, Context 2: 1.0\n",
      "Question 7, Context 3: 1.0\n",
      "Question 7, Context 4: 1.0\n",
      "Question 8, Context 0: 1.0\n",
      "Question 8, Context 1: 0.0\n",
      "Question 8, Context 2: 1.0\n",
      "Question 8, Context 3: 1.0\n",
      "Question 8, Context 4: 1.0\n",
      "Question 9, Context 0: 1.0\n",
      "Question 9, Context 1: 1.0\n",
      "Question 9, Context 2: 0.0\n",
      "Question 9, Context 3: 1.0\n",
      "Question 9, Context 4: 0.0\n",
      "Question 10, Context 0: 1.0\n",
      "Question 10, Context 1: 1.0\n",
      "Question 10, Context 2: 1.0\n",
      "Question 10, Context 3: 1.0\n",
      "Question 10, Context 4: 1.0\n",
      "Question 11, Context 0: 1.0\n",
      "Question 11, Context 1: 1.0\n",
      "Question 11, Context 2: 1.0\n",
      "Question 11, Context 3: 1.0\n",
      "Question 11, Context 4: 0.0\n",
      "Question 12, Context 0: 1.0\n",
      "Question 12, Context 1: 1.0\n",
      "Question 12, Context 2: 1.0\n",
      "Question 12, Context 3: 1.0\n",
      "Question 12, Context 4: 1.0\n",
      "Question 13, Context 0: 1.0\n",
      "Question 13, Context 1: 1.0\n",
      "Question 13, Context 2: 1.0\n",
      "Question 13, Context 3: 1.0\n",
      "Question 13, Context 4: 1.0\n",
      "Question 14, Context 0: 1.0\n",
      "Question 14, Context 1: 1.0\n",
      "Question 14, Context 2: 0.0\n",
      "Question 14, Context 3: 1.0\n",
      "Question 14, Context 4: 0.0\n",
      "Question 15, Context 0: 1.0\n",
      "Question 15, Context 1: 1.0\n",
      "Question 15, Context 2: 1.0\n",
      "Question 15, Context 3: 1.0\n",
      "Question 15, Context 4: 1.0\n",
      "Question 16, Context 0: 1.0\n",
      "Question 16, Context 1: 1.0\n",
      "Question 16, Context 2: 1.0\n",
      "Question 16, Context 3: 1.0\n",
      "Question 16, Context 4: 1.0\n",
      "Question 17, Context 0: 1.0\n",
      "Question 17, Context 1: 1.0\n",
      "Question 17, Context 2: 1.0\n",
      "Question 17, Context 3: 1.0\n",
      "Question 17, Context 4: 1.0\n",
      "Question 18, Context 0: 1.0\n",
      "Question 18, Context 1: 1.0\n",
      "Question 18, Context 2: 1.0\n",
      "Question 18, Context 3: 1.0\n",
      "Question 18, Context 4: 1.0\n",
      "Question 19, Context 0: 1.0\n",
      "Question 19, Context 1: 1.0\n",
      "Question 19, Context 2: 1.0\n",
      "Question 19, Context 3: 0.0\n",
      "Question 19, Context 4: 1.0\n",
      "Question 20, Context 0: 1.0\n",
      "Question 20, Context 1: 1.0\n",
      "Question 20, Context 2: 1.0\n",
      "Question 20, Context 3: 1.0\n",
      "Question 20, Context 4: 1.0\n",
      "Question 21, Context 0: 1.0\n",
      "Question 21, Context 1: 1.0\n",
      "Question 21, Context 2: 1.0\n",
      "Question 21, Context 3: 1.0\n",
      "Question 21, Context 4: 1.0\n",
      "Question 22, Context 0: 1.0\n",
      "Question 22, Context 1: 1.0\n",
      "Question 22, Context 2: 0.0\n",
      "Question 22, Context 3: 1.0\n",
      "Question 22, Context 4: 1.0\n",
      "Question 23, Context 0: 1.0\n",
      "Question 23, Context 1: 1.0\n",
      "Question 23, Context 2: 1.0\n",
      "Question 23, Context 3: 1.0\n",
      "Question 23, Context 4: 0.0\n",
      "Question 24, Context 0: 1.0\n",
      "Question 24, Context 1: 1.0\n",
      "Question 24, Context 2: 0.0\n",
      "Question 24, Context 3: 1.0\n",
      "Question 24, Context 4: 1.0\n",
      "Question 25, Context 0: 1.0\n",
      "Question 25, Context 1: 1.0\n",
      "Question 25, Context 2: 1.0\n",
      "Question 25, Context 3: 1.0\n",
      "Question 25, Context 4: 1.0\n",
      "Question 26, Context 0: 1.0\n",
      "Question 26, Context 1: 1.0\n",
      "Question 26, Context 2: 1.0\n",
      "Question 26, Context 3: 1.0\n",
      "Question 26, Context 4: 1.0\n",
      "Question 27, Context 0: 1.0\n",
      "Question 27, Context 1: 1.0\n",
      "Question 27, Context 2: 1.0\n",
      "Question 27, Context 3: 1.0\n",
      "Question 27, Context 4: 0.0\n",
      "Question 28, Context 0: 1.0\n",
      "Question 28, Context 1: 1.0\n",
      "Question 28, Context 2: 1.0\n",
      "Question 28, Context 3: 1.0\n",
      "Question 28, Context 4: 1.0\n",
      "Question 29, Context 0: 1.0\n",
      "Question 29, Context 1: 1.0\n",
      "Question 29, Context 2: 1.0\n",
      "Question 29, Context 3: 0.0\n",
      "Question 29, Context 4: 1.0\n",
      "Question 30, Context 0: 1.0\n",
      "Question 30, Context 1: 1.0\n",
      "Question 30, Context 2: 0.0\n",
      "Question 30, Context 3: 1.0\n",
      "Question 30, Context 4: 0.0\n",
      "Question 31, Context 0: 1.0\n",
      "Question 31, Context 1: 1.0\n",
      "Question 31, Context 2: 0.0\n",
      "Question 31, Context 3: 1.0\n",
      "Question 31, Context 4: 0.0\n",
      "Question 32, Context 0: 1.0\n",
      "Question 32, Context 1: 1.0\n",
      "Question 32, Context 2: 1.0\n",
      "Question 32, Context 3: 1.0\n",
      "Question 32, Context 4: 1.0\n",
      "Question 33, Context 0: 1.0\n",
      "Question 33, Context 1: 1.0\n",
      "Question 33, Context 2: 1.0\n",
      "Question 33, Context 3: 1.0\n",
      "Question 33, Context 4: 1.0\n",
      "Question 34, Context 0: 1.0\n",
      "Question 34, Context 1: 0.0\n",
      "Question 34, Context 2: 1.0\n",
      "Question 34, Context 3: 1.0\n",
      "Question 34, Context 4: 1.0\n",
      "Question 35, Context 0: 1.0\n",
      "Question 35, Context 1: 1.0\n",
      "Question 35, Context 2: 1.0\n",
      "Question 35, Context 3: 1.0\n",
      "Question 35, Context 4: 1.0\n",
      "Question 36, Context 0: 1.0\n",
      "Question 36, Context 1: 0.0\n",
      "Question 36, Context 2: 1.0\n",
      "Question 36, Context 3: 1.0\n",
      "Question 36, Context 4: 1.0\n",
      "Question 37, Context 0: 1.0\n",
      "Question 37, Context 1: 1.0\n",
      "Question 37, Context 2: 1.0\n",
      "Question 37, Context 3: 1.0\n",
      "Question 37, Context 4: 0.0\n",
      "Question 38, Context 0: 1.0\n",
      "Question 38, Context 1: 1.0\n",
      "Question 38, Context 2: 1.0\n",
      "Question 38, Context 3: 1.0\n",
      "Question 38, Context 4: 1.0\n",
      "Question 39, Context 0: 1.0\n",
      "Question 39, Context 1: 1.0\n",
      "Question 39, Context 2: 1.0\n",
      "Question 39, Context 3: 1.0\n",
      "Question 39, Context 4: 1.0\n",
      "Question 40, Context 0: 0.0\n",
      "Question 40, Context 1: 1.0\n",
      "Question 40, Context 2: 0.0\n",
      "Question 40, Context 3: 1.0\n",
      "Question 40, Context 4: 1.0\n",
      "Question 41, Context 0: 1.0\n",
      "Question 41, Context 1: 1.0\n",
      "Question 41, Context 2: 1.0\n",
      "Question 41, Context 3: 1.0\n",
      "Question 41, Context 4: 1.0\n",
      "Question 42, Context 0: 1.0\n",
      "Question 42, Context 1: 0.0\n",
      "Question 42, Context 2: 1.0\n",
      "Question 42, Context 3: 0.0\n",
      "Question 42, Context 4: 1.0\n",
      "Question 43, Context 0: 1.0\n",
      "Question 43, Context 1: 1.0\n",
      "Question 43, Context 2: 1.0\n",
      "Question 43, Context 3: 1.0\n",
      "Question 43, Context 4: 1.0\n",
      "Question 44, Context 0: 1.0\n",
      "Question 44, Context 1: 1.0\n",
      "Question 44, Context 2: 1.0\n",
      "Question 44, Context 3: 1.0\n",
      "Question 44, Context 4: 1.0\n",
      "Question 45, Context 0: 1.0\n",
      "Question 45, Context 1: 1.0\n",
      "Question 45, Context 2: 1.0\n",
      "Question 45, Context 3: 1.0\n",
      "Question 45, Context 4: 1.0\n",
      "Question 46, Context 0: 1.0\n",
      "Question 46, Context 1: 1.0\n",
      "Question 46, Context 2: 0.0\n",
      "Question 46, Context 3: 1.0\n",
      "Question 46, Context 4: 1.0\n",
      "Question 47, Context 0: 1.0\n",
      "Question 47, Context 1: 0.0\n",
      "Question 47, Context 2: 1.0\n",
      "Question 47, Context 3: 1.0\n",
      "Question 47, Context 4: 1.0\n",
      "Question 48, Context 0: 1.0\n",
      "Question 48, Context 1: 1.0\n",
      "Question 48, Context 2: 1.0\n",
      "Question 48, Context 3: 1.0\n",
      "Question 48, Context 4: 1.0\n",
      "Question 49, Context 0: 1.0\n",
      "Question 49, Context 1: 1.0\n",
      "Question 49, Context 2: 1.0\n",
      "Question 49, Context 3: 0.0\n",
      "Question 49, Context 4: 1.0\n",
      "Question 50, Context 0: 1.0\n",
      "Question 50, Context 1: 1.0\n",
      "Question 50, Context 2: 1.0\n",
      "Question 50, Context 3: 1.0\n",
      "Question 50, Context 4: 1.0\n",
      "Question 51, Context 0: 1.0\n",
      "Question 51, Context 1: 1.0\n",
      "Question 51, Context 2: 0.0\n",
      "Question 51, Context 3: 0.0\n",
      "Question 51, Context 4: 1.0\n",
      "Question 52, Context 0: 1.0\n",
      "Question 52, Context 1: 1.0\n",
      "Question 52, Context 2: 1.0\n",
      "Question 52, Context 3: 0.0\n",
      "Question 52, Context 4: 1.0\n",
      "Question 53, Context 0: 1.0\n",
      "Question 53, Context 1: 1.0\n",
      "Question 53, Context 2: 0.0\n",
      "Question 53, Context 3: 1.0\n",
      "Question 53, Context 4: 1.0\n",
      "Question 54, Context 0: 1.0\n",
      "Question 54, Context 1: 1.0\n",
      "Question 54, Context 2: 1.0\n",
      "Question 54, Context 3: 1.0\n",
      "Question 54, Context 4: 1.0\n",
      "Question 55, Context 0: 1.0\n",
      "Question 55, Context 1: 1.0\n",
      "Question 55, Context 2: 1.0\n",
      "Question 55, Context 3: 1.0\n",
      "Question 55, Context 4: 1.0\n",
      "Question 56, Context 0: 1.0\n",
      "Question 56, Context 1: 1.0\n",
      "Question 56, Context 2: 1.0\n",
      "Question 56, Context 3: 1.0\n",
      "Question 56, Context 4: 1.0\n",
      "Question 57, Context 0: 1.0\n",
      "Question 57, Context 1: 1.0\n",
      "Question 57, Context 2: 1.0\n",
      "Question 57, Context 3: 1.0\n",
      "Question 57, Context 4: 1.0\n",
      "Question 58, Context 0: 1.0\n",
      "Question 58, Context 1: 1.0\n",
      "Question 58, Context 2: 1.0\n",
      "Question 58, Context 3: 0.0\n",
      "Question 58, Context 4: 1.0\n",
      "Question 59, Context 0: 1.0\n",
      "Question 59, Context 1: 1.0\n",
      "Question 59, Context 2: 0.0\n",
      "Question 59, Context 3: 1.0\n",
      "Question 59, Context 4: 1.0\n",
      "Question 60, Context 0: 1.0\n",
      "Question 60, Context 1: 1.0\n",
      "Question 60, Context 2: 1.0\n",
      "Question 60, Context 3: 1.0\n",
      "Question 60, Context 4: 1.0\n",
      "Question 61, Context 0: 1.0\n",
      "Question 61, Context 1: 1.0\n",
      "Question 61, Context 2: 1.0\n",
      "Question 61, Context 3: 1.0\n",
      "Question 61, Context 4: 1.0\n",
      "Question 62, Context 0: 0.0\n",
      "Question 62, Context 1: 1.0\n",
      "Question 62, Context 2: 1.0\n",
      "Question 62, Context 3: 1.0\n",
      "Question 62, Context 4: 1.0\n",
      "Question 63, Context 0: 1.0\n",
      "Question 63, Context 1: 1.0\n",
      "Question 63, Context 2: 1.0\n",
      "Question 63, Context 3: 1.0\n",
      "Question 63, Context 4: 1.0\n",
      "Question 64, Context 0: 1.0\n",
      "Question 64, Context 1: 1.0\n",
      "Question 64, Context 2: 1.0\n",
      "Question 64, Context 3: 1.0\n",
      "Question 64, Context 4: 1.0\n",
      "Question 65, Context 0: 1.0\n",
      "Question 65, Context 1: 1.0\n",
      "Question 65, Context 2: 1.0\n",
      "Question 65, Context 3: 1.0\n",
      "Question 65, Context 4: 1.0\n",
      "Question 66, Context 0: 1.0\n",
      "Question 66, Context 1: 0.0\n",
      "Question 66, Context 2: 1.0\n",
      "Question 66, Context 3: 1.0\n",
      "Question 66, Context 4: 1.0\n",
      "Question 67, Context 0: 1.0\n",
      "Question 67, Context 1: 1.0\n",
      "Question 67, Context 2: 1.0\n",
      "Question 67, Context 3: 1.0\n",
      "Question 67, Context 4: 1.0\n",
      "Question 68, Context 0: 1.0\n",
      "Question 68, Context 1: 1.0\n",
      "Question 68, Context 2: 1.0\n",
      "Question 68, Context 3: 1.0\n",
      "Question 68, Context 4: 1.0\n",
      "Question 69, Context 0: 1.0\n",
      "Question 69, Context 1: 1.0\n",
      "Question 69, Context 2: 1.0\n",
      "Question 69, Context 3: 0.0\n",
      "Question 69, Context 4: 1.0\n",
      "Question 70, Context 0: 1.0\n",
      "Question 70, Context 1: 1.0\n",
      "Question 70, Context 2: 1.0\n",
      "Question 70, Context 3: 1.0\n",
      "Question 70, Context 4: 1.0\n",
      "Question 71, Context 0: 0.0\n",
      "Question 71, Context 1: 1.0\n",
      "Question 71, Context 2: 0.0\n",
      "Question 71, Context 3: 1.0\n",
      "Question 71, Context 4: 1.0\n",
      "Question 72, Context 0: 1.0\n",
      "Question 72, Context 1: 1.0\n",
      "Question 72, Context 2: 1.0\n",
      "Question 72, Context 3: 1.0\n",
      "Question 72, Context 4: 1.0\n",
      "Question 73, Context 0: 0.0\n",
      "Question 73, Context 1: 1.0\n",
      "Question 73, Context 2: 0.0\n",
      "Question 73, Context 3: 1.0\n",
      "Question 73, Context 4: 1.0\n",
      "Question 74, Context 0: 1.0\n",
      "Question 74, Context 1: 1.0\n",
      "Question 74, Context 2: 1.0\n",
      "Question 74, Context 3: 1.0\n",
      "Question 74, Context 4: 1.0\n",
      "Question 75, Context 0: 1.0\n",
      "Question 75, Context 1: 1.0\n",
      "Question 75, Context 2: 1.0\n",
      "Question 75, Context 3: 1.0\n",
      "Question 75, Context 4: 0.0\n",
      "Question 76, Context 0: 1.0\n",
      "Question 76, Context 1: 1.0\n",
      "Question 76, Context 2: 1.0\n",
      "Question 76, Context 3: 1.0\n",
      "Question 76, Context 4: 1.0\n",
      "Question 77, Context 0: 1.0\n",
      "Question 77, Context 1: 0.0\n",
      "Question 77, Context 2: 1.0\n",
      "Question 77, Context 3: 1.0\n",
      "Question 77, Context 4: 1.0\n",
      "Question 78, Context 0: 1.0\n",
      "Question 78, Context 1: 1.0\n",
      "Question 78, Context 2: 1.0\n",
      "Question 78, Context 3: 1.0\n",
      "Question 78, Context 4: 1.0\n",
      "Question 79, Context 0: 1.0\n",
      "Question 79, Context 1: 1.0\n",
      "Question 79, Context 2: 1.0\n",
      "Question 79, Context 3: 1.0\n",
      "Question 79, Context 4: 1.0\n",
      "Question 80, Context 0: 0.0\n",
      "Question 80, Context 1: 1.0\n",
      "Question 80, Context 2: 1.0\n",
      "Question 80, Context 3: 1.0\n",
      "Question 80, Context 4: 1.0\n",
      "Question 81, Context 0: 1.0\n",
      "Question 81, Context 1: 1.0\n",
      "Question 81, Context 2: 1.0\n",
      "Question 81, Context 3: 1.0\n",
      "Question 81, Context 4: 1.0\n",
      "Question 82, Context 0: 1.0\n",
      "Question 82, Context 1: 1.0\n",
      "Question 82, Context 2: 1.0\n",
      "Question 82, Context 3: 1.0\n",
      "Question 82, Context 4: 1.0\n",
      "Question 83, Context 0: 1.0\n",
      "Question 83, Context 1: 1.0\n",
      "Question 83, Context 2: 1.0\n",
      "Question 83, Context 3: 1.0\n",
      "Question 83, Context 4: 1.0\n",
      "Question 84, Context 0: 1.0\n",
      "Question 84, Context 1: 1.0\n",
      "Question 84, Context 2: 1.0\n",
      "Question 84, Context 3: 1.0\n",
      "Question 84, Context 4: 1.0\n",
      "Question 85, Context 0: 0.0\n",
      "Question 85, Context 1: 0.0\n",
      "Question 85, Context 2: 0.0\n",
      "Question 85, Context 3: 0.0\n",
      "Question 85, Context 4: 0.0\n",
      "Question 86, Context 0: 1.0\n",
      "Question 86, Context 1: 1.0\n",
      "Question 86, Context 2: 1.0\n",
      "Question 86, Context 3: 1.0\n",
      "Question 86, Context 4: 1.0\n",
      "Question 87, Context 0: 1.0\n",
      "Question 87, Context 1: 1.0\n",
      "Question 87, Context 2: 1.0\n",
      "Question 87, Context 3: 1.0\n",
      "Question 87, Context 4: 1.0\n",
      "Question 88, Context 0: 1.0\n",
      "Question 88, Context 1: 1.0\n",
      "Question 88, Context 2: 0.0\n",
      "Question 88, Context 3: 1.0\n",
      "Question 88, Context 4: 1.0\n",
      "Question 89, Context 0: 1.0\n",
      "Question 89, Context 1: 1.0\n",
      "Question 89, Context 2: 0.0\n",
      "Question 89, Context 3: 0.0\n",
      "Question 89, Context 4: 1.0\n",
      "Question 90, Context 0: 1.0\n",
      "Question 90, Context 1: 1.0\n",
      "Question 90, Context 2: 1.0\n",
      "Question 90, Context 3: 1.0\n",
      "Question 90, Context 4: 0.0\n",
      "Question 91, Context 0: 0.0\n",
      "Question 91, Context 1: 1.0\n",
      "Question 91, Context 2: 1.0\n",
      "Question 91, Context 3: 1.0\n",
      "Question 91, Context 4: 1.0\n",
      "Question 92, Context 0: 1.0\n",
      "Question 92, Context 1: 1.0\n",
      "Question 92, Context 2: 1.0\n",
      "Question 92, Context 3: 1.0\n",
      "Question 92, Context 4: 1.0\n",
      "Question 93, Context 0: 1.0\n",
      "Question 93, Context 1: 1.0\n",
      "Question 93, Context 2: 1.0\n",
      "Question 93, Context 3: 1.0\n",
      "Question 93, Context 4: 1.0\n",
      "Question 94, Context 0: 1.0\n",
      "Question 94, Context 1: 1.0\n",
      "Question 94, Context 2: 1.0\n",
      "Question 94, Context 3: 1.0\n",
      "Question 94, Context 4: 0.0\n",
      "Question 95, Context 0: 1.0\n",
      "Question 95, Context 1: 1.0\n",
      "Question 95, Context 2: 1.0\n",
      "Question 95, Context 3: 1.0\n",
      "Question 95, Context 4: 1.0\n",
      "Question 96, Context 0: 1.0\n",
      "Question 96, Context 1: 1.0\n",
      "Question 96, Context 2: 1.0\n",
      "Question 96, Context 3: 1.0\n",
      "Question 96, Context 4: 1.0\n",
      "Question 97, Context 0: 0.0\n",
      "Question 97, Context 1: 0.0\n",
      "Question 97, Context 2: 1.0\n",
      "Question 97, Context 3: 1.0\n",
      "Question 97, Context 4: 1.0\n",
      "Question 98, Context 0: 1.0\n",
      "Question 98, Context 1: 1.0\n",
      "Question 98, Context 2: 1.0\n",
      "Question 98, Context 3: 0.0\n",
      "Question 98, Context 4: 1.0\n",
      "Question 99, Context 0: 1.0\n",
      "Question 99, Context 1: 1.0\n",
      "Question 99, Context 2: 1.0\n",
      "Question 99, Context 3: 1.0\n",
      "Question 99, Context 4: 1.0\n",
      "Question 100, Context 0: 1.0\n",
      "Question 100, Context 1: 1.0\n",
      "Question 100, Context 2: 1.0\n",
      "Question 100, Context 3: 1.0\n",
      "Question 100, Context 4: 1.0\n",
      "Question 101, Context 0: 1.0\n",
      "Question 101, Context 1: 0.0\n",
      "Question 101, Context 2: 0.0\n",
      "Question 101, Context 3: 0.0\n",
      "Question 101, Context 4: 1.0\n",
      "Question 102, Context 0: 1.0\n",
      "Question 102, Context 1: 1.0\n",
      "Question 102, Context 2: 1.0\n",
      "Question 102, Context 3: 1.0\n",
      "Question 102, Context 4: 1.0\n",
      "Question 103, Context 0: 1.0\n",
      "Question 103, Context 1: 1.0\n",
      "Question 103, Context 2: 1.0\n",
      "Question 103, Context 3: 0.0\n",
      "Question 103, Context 4: 0.0\n",
      "Question 104, Context 0: 1.0\n",
      "Question 104, Context 1: 1.0\n",
      "Question 104, Context 2: 1.0\n",
      "Question 104, Context 3: 1.0\n",
      "Question 104, Context 4: 1.0\n",
      "Question 105, Context 0: 1.0\n",
      "Question 105, Context 1: 1.0\n",
      "Question 105, Context 2: 1.0\n",
      "Question 105, Context 3: 1.0\n",
      "Question 105, Context 4: 1.0\n",
      "Question 106, Context 0: 1.0\n",
      "Question 106, Context 1: 1.0\n",
      "Question 106, Context 2: 1.0\n",
      "Question 106, Context 3: 0.0\n",
      "Question 106, Context 4: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(eval_data['questions']):\n",
    "    for j, context in enumerate(results['documents'][i]):\n",
    "        retrieval_precision_matrix[i][j] = get_retrieval_precision_indicator(question, context)\n",
    "        print(f\"Question {i}, Context {j}: {retrieval_precision_matrix[i][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 1: 0.9252336448598131\n",
      "Precision at 2: 0.9158878504672897\n",
      "Precision at 3: 0.8909657320872275\n",
      "Precision at 4: 0.883177570093458\n",
      "Precision at 5: 0.8766355140186914\n",
      "[0.9252336448598131, 0.9158878504672897, 0.8909657320872275, 0.883177570093458, 0.8766355140186914]\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision at 1,2,3,4 and 5\n",
    "precision_at_1 = np.mean(retrieval_precision_matrix[:, 0])\n",
    "precision_at_2 = np.mean(np.sum(retrieval_precision_matrix[:, :2], axis=1) / 2)\n",
    "precision_at_3 = np.mean(np.sum(retrieval_precision_matrix[:, :3], axis=1) / 3)\n",
    "precision_at_4 = np.mean(np.sum(retrieval_precision_matrix[:, :4], axis=1) / 4)\n",
    "precision_at_5 = np.mean(np.sum(retrieval_precision_matrix[:, :5], axis=1) / 5)\n",
    "\n",
    "print(f\"Precision at 1: {precision_at_1}\")\n",
    "print(f\"Precision at 2: {precision_at_2}\")\n",
    "print(f\"Precision at 3: {precision_at_3}\")\n",
    "print(f\"Precision at 4: {precision_at_4}\")\n",
    "print(f\"Precision at 5: {precision_at_5}\")\n",
    "\n",
    "print(f\"[{precision_at_1}, {precision_at_2}, {precision_at_3}, {precision_at_4}, {precision_at_5}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TokenSplitter set up as ARAGOG paper\n",
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9158878504672897\n",
    "# Precision at 3: 0.8909657320872274\n",
    "# Precision at 4: 0.8878504672897196\n",
    "# Precision at 5: 0.8803738317757009\n",
    "# [0.9252336448598131, 0.9158878504672897, 0.8909657320872274, 0.8878504672897196, 0.8803738317757009]\n",
    "\n",
    "# Precision at 1: 0.9158878504672897\n",
    "# Precision at 2: 0.9158878504672897\n",
    "# Precision at 3: 0.8847352024922119\n",
    "# Precision at 4: 0.8785046728971962\n",
    "# Precision at 5: 0.8710280373831776\n",
    "# [0.9158878504672897, 0.9158878504672897, 0.8847352024922119, 0.8785046728971962, 0.8710280373831776]\n",
    "\n",
    "# Precision at 1: 0.9065420560747663\n",
    "# Precision at 2: 0.8925233644859814\n",
    "# Precision at 3: 0.881619937694704\n",
    "# Precision at 4: 0.8714953271028038\n",
    "# Precision at 5: 0.8710280373831776\n",
    "# [0.9065420560747663, 0.8925233644859814, 0.881619937694704, 0.8714953271028038, 0.8710280373831776]\n",
    "\n",
    "trial_1 = [0.9252336448598131, 0.9158878504672897, 0.8909657320872274, 0.8878504672897196, 0.8803738317757009]\n",
    "trial_2 = [0.9158878504672897, 0.9158878504672897, 0.8847352024922119, 0.8785046728971962, 0.8710280373831776]\n",
    "trial_3 = [0.9065420560747663, 0.8925233644859814, 0.881619937694704, 0.8714953271028038, 0.8710280373831776]\n",
    "trial_4 = [0.9065420560747663, 0.897196261682243, 0.8847352024922117, 0.8761682242990654, 0.8654205607476636]\n",
    "trial_5 = [0.9345794392523364, 0.9345794392523364, 0.9127725856697819, 0.8925233644859814, 0.885981308411215]\n",
    "trial_6 = [0.9065420560747663, 0.8925233644859814, 0.8722741433021808, 0.866822429906542, 0.8616822429906541]\n",
    "trial_7 = [0.9065420560747663, 0.9065420560747663, 0.8909657320872274, 0.8785046728971962, 0.874766355140187]\n",
    "trial_8 = [0.9252336448598131, 0.9158878504672897, 0.8909657320872275, 0.883177570093458, 0.8766355140186914]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a list of all trials\n",
    "all_trials = [trial_1, trial_2, trial_3, trial_4, trial_5, trial_6, trial_7, trial_8]\n",
    "\n",
    "# Transpose the list to get precision at each rank\n",
    "precision_at_each_rank = list(map(list, zip(*all_trials)))\n",
    "\n",
    "# Create labels\n",
    "labels = ['P@1', 'P@2', 'P@3', 'P@4', 'P@5']\n",
    "\n",
    "# Create the boxplot\n",
    "plt.boxplot(precision_at_each_rank, vert=False, labels=labels)\n",
    "plt.title('Boxplot of Precision@K for GPT-3.5-Turbo')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Rank')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9065420560747663\n",
    "# Precision at 3: 0.9034267912772584\n",
    "# Precision at 4: 0.8925233644859814\n",
    "# Precision at 5: 0.8766355140186917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Change 1: 2.06%\n",
      "Percentage Change 2: 1.04%\n",
      "Percentage Change 3: 1.75%\n",
      "Percentage Change 4: 2.14%\n",
      "Percentage Change 5: 1.96%\n"
     ]
    }
   ],
   "source": [
    "# TokenSplitter\n",
    "# Precision at 1: 0.9065420560747663\n",
    "# Precision at 2: 0.897196261682243\n",
    "# Precision at 3: 0.8878504672897196\n",
    "# Precision at 4: 0.8738317757009346\n",
    "# Precision at 5: 0.8598130841121495\n",
    "\n",
    "# RecursiveCharacterTextSplitter\n",
    "# Precision at 1: 0.9252336448598131\n",
    "# Precision at 2: 0.9065420560747663\n",
    "# Precision at 3: 0.9034267912772584\n",
    "# Precision at 4: 0.8925233644859814\n",
    "# Precision at 5: 0.8766355140186917\n",
    "\n",
    "\n",
    "percentage_change_1 = ((0.9252336448598131 - 0.9065420560747663) / 0.9065420560747663) * 100\n",
    "percentage_change_2 = ((0.9065420560747663 - 0.897196261682243) / 0.897196261682243) * 100\n",
    "percentage_change_3 = ((0.9034267912772584 - 0.8878504672897196) / 0.8878504672897196) * 100\n",
    "percentage_change_4 = ((0.8925233644859814 - 0.8738317757009346) / 0.8738317757009346) * 100\n",
    "percentage_change_5 = ((0.8766355140186917 - 0.8598130841121495) / 0.8598130841121495) * 100\n",
    "\n",
    "print(f\"Percentage Change 1: {percentage_change_1:.2f}%\")\n",
    "print(f\"Percentage Change 2: {percentage_change_2:.2f}%\")\n",
    "print(f\"Percentage Change 3: {percentage_change_3:.2f}%\")\n",
    "print(f\"Percentage Change 4: {percentage_change_4:.2f}%\")\n",
    "print(f\"Percentage Change 5: {percentage_change_5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision at 1: 0.9065420560747663\n",
    "Precision at 2: 0.897196261682243\n",
    "Precision at 3: 0.8878504672897196\n",
    "Precision at 4: 0.8738317757009346\n",
    "Precision at 5: 0.8598130841121495"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
