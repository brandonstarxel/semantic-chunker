{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "The objectives of this week are to develop and test benchmarks. This will give us a baseline to compare our methods to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Find example pdf / text data.\n",
    "- Setup LangChain Recursive Text Splitter.\n",
    "- Setup fixed length token splitter.\n",
    "- Setup Chroma DB.\n",
    "- Create pipeline of: Text + Chunker -> Chroma Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Text Data\n",
    "To start off simple, I copied a recent news article from BBC about Effective Accelerationist, Grimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coachella: Grimes apologises for technical difficulties\n",
      "\n",
      "Mon 15 Apr\n",
      "\n",
      "BBC NEWS\n",
      "\n",
      "Grimes has apologised for \"major technical difficulties\" during her Coachella DJ set.\n",
      "\n",
      "Fans watched the singer scream in ...\n"
     ]
    }
   ],
   "source": [
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Test the function with the news.txt file\n",
    "news_data = read_txt_file('../data/news.txt')\n",
    "print(news_data[:200]+'...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain Recursive Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the Recursive Text Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "# Split the news_data using the splitter\n",
    "split_text = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangChain fixed length splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coachella: Grimes apologises for technical difficulties\\n\\nMon 15 Apr\\n\\nBBC NEWS', 'BBC NEWS\\n\\nGrimes has apologised for \"major technical difficulties\" during her Coachella DJ set.', 'Fans watched the singer scream in frustration after a string of problems - such as songs playing at', 'as songs playing at double-speed - marred the second half of her festival slot.', 'Posting on X, the singer said it was \"one of the first times\" she had \"outsourced essential']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "\n",
    "texts = splitter.split_text(news_data)\n",
    "\n",
    "# Print the first 5 splits\n",
    "print(split_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"chuck_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Precision:\n",
    "In the ARAGOG paper they used Tonic Validate for this. If have taken Tonic's prompt so our implementation is identical (except we have the power to use models beyond GPT-3.5).\n",
    "Ref: https://github.com/TonicAI/tonic_validate/blob/main/tonic_validate/utils/llm_calls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_precision_prompt(question, context):\n",
    "    main_message = (\"Considering the following question and context, determine whether the context \"\n",
    "                    \"is relevant for answering the question. If the context is relevant for \"\n",
    "                    \"answering the question, respond with true. If the context is not relevant for \"\n",
    "                    \"answering the question, respond with false. Respond with either true or false \"\n",
    "                    \"and no additional text.\")\n",
    "\n",
    "    main_message += f\"\\nQUESTION: {question}\\n\"\n",
    "    main_message += f\"CONTEXT: {context}\\n\"\n",
    "\n",
    "    return main_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\n",
      "QUESTION: What is the capital of France?\n",
      "CONTEXT: France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the function get_retrieval_precision_prompt\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.\"\n",
    "\n",
    "print(get_retrieval_precision_prompt(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_CHROMA_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='*clears throat and speaks in a croaky voice* Hmm, well I am today, young Padawan. The Force, strong in me it flows. Yes, hmmm.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import anthropic\n",
    "\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_CHROMA_API_KEY')\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How are you today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='true', type='text')]\n"
     ]
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=get_retrieval_precision_prompt(question, context),\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Is this CONTEXT relavent?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the json file and read it\n",
    "with open('../eval_questions/eval_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data to verify it's been read correctly\n",
    "print(len(data['questions']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
