{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals:\n",
    "- More corpus and questions. \n",
    "- Ideally we get different style corpuses.\n",
    "1. Research:\n",
    "Start with WikiText then use this to look for other text datasets.\n",
    "2. Compose a list of all different text datasets.\n",
    "3. Get a sample from each type.\n",
    "4. Generate questions for each dataset. \n",
    "\n",
    "\n",
    "Tokens extracted from good wiki articles\n",
    "https://huggingface.co/datasets/wikitext\n",
    "\n",
    "JSON Conversations between a user and a chat bot. \n",
    "https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k?row=0\n",
    "\n",
    "Finance:\n",
    "https://huggingface.co/datasets/AdaptLLM/finance-tasks?row=12\n",
    "\n",
    "Finance SEC fillings:\n",
    "https://huggingface.co/datasets/JanosAudran/financial-reports-sec\n",
    "\n",
    "PubMed Open Access:\n",
    "https://huggingface.co/datasets/pmc/open_access\n",
    "\n",
    "Legal:\n",
    "https://huggingface.co/datasets/pile-of-law/pile-of-law\n",
    "\n",
    "\n",
    "\n",
    "Next goal?\n",
    "\n",
    "We have a lot of weird data. Now what. \n",
    "\n",
    "For each dataset, attempt to get large texts out of it. These texts will then be our 'main docs'. Our corpus dataset will be all these docs for which we run chunking over each individual, then union of chunks. Questions generated just be looking at single doc. \n",
    "\n",
    "Current goal:\n",
    "Get 'docs' from datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_huggingface_data(params):\n",
    "    # URL for the API endpoint\n",
    "    url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the JSON response\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35458\n",
      " = Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = fetch_huggingface_data({\n",
    "    'dataset': 'wikitext',\n",
    "    'config': 'wikitext-103-v1',\n",
    "    'split': 'train',\n",
    "    'offset': 0,\n",
    "    'length': 100\n",
    "})\n",
    "\n",
    "total_doc = ''\n",
    "\n",
    "for row in data['rows']:\n",
    "    total_doc += row['row']['text']\n",
    "\n",
    "print(len(total_doc))\n",
    "print(total_doc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
